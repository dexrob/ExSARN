{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "'''\n",
    "sample command: python T4_BT19_ae.py -k 0 -c 0 -r 1 --data_dir /home/ruihan/data\n",
    "Individual training for BioTac data (full/partial data)\n",
    "if -r=1, train with full data\n",
    "if -r=2, train with half data\n",
    "loss = classification loss + recon loss \n",
    "'''\n",
    "\n",
    "# Import\n",
    "import os,sys\n",
    "import pickle\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from vrae.vrae import VRAEC_v2\n",
    "from preprocess_data import get_TrainValTestLoader, get_TrainValTestDataset, get_TrainValTestData, property_label\n",
    "from vrae.visual import plot_grad_flow, plot_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parse argument\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-i\", \"--rep\", type=int, default=0, help='index of running repetition')\n",
    "# parser.add_argument('--data_dir', type=str, default='data', help=\"DIR set in 'gh_download.sh' to store compiled_data\")\n",
    "# parser.add_argument(\"-k\", \"--kfold\", type=int, default=0, help=\"kfold_number for loading data\")\n",
    "# parser.add_argument(\"-r\", \"--reduction\", type=int, default=1, help=\"data reduction ratio for partial training\")\n",
    "# parser.add_argument(\"-c\", \"--cuda\", default=0, help=\"index of cuda gpu to use\")\n",
    "# parser.add_argument(\"--r_on\", default=0.1, help=\"ratio_on for phased lstm\")\n",
    "# parser.add_argument(\"--p_max\", default=200, help=\"period_init_max for phased lstm\")\n",
    "# parser.add_argument(\"--w_r\", default=0.01, type=float, help=\"weight of recon loss\")\n",
    "# parser.add_argument(\"--h_s\", default=90, type=int, help=\"hidden size of rnn layers\")\n",
    "# parser.add_argument(\"--dataset\", default='c20', type=str, help=\"name of dataset\")\n",
    "# parser.add_argument(\"--thickness_lat\", default=10, type=int, help=\"size for the thickness latent space\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# dummy class to replace argparser, if running jupyter notebook\n",
    "class Args:\n",
    "    rep = 0\n",
    "    data_dir = 'data'\n",
    "    kfold = 0\n",
    "    cuda = '0'\n",
    "    reduction = 1\n",
    "    r_on = 0.1\n",
    "    p_max = 200\n",
    "    w_r = 0.01\n",
    "    h_s = 90\n",
    "    dataset = 'c50'\n",
    "    thickness_lat = 10\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 0 kfold number, train with full data, put to device: cpu\n",
      "chop org data of length 400 into 1 segments, each of which is has length 400\n",
      "chop org data of length 400 into 1 segments, each of which is has length 400\n",
      "chop org data of length 400 into 1 segments, each of which is has length 400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VRAE(n_epochs=100,batch_size=32,cuda=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set hyper params\n",
    "args_data_dir = args.data_dir\n",
    "kfold_number = args.kfold\n",
    "data_reduction_ratio = args.reduction\n",
    "shuffle = True # set to False for partial training\n",
    "sequence_length = 400\n",
    "number_of_features = 19\n",
    "\n",
    "hidden_size = args.h_s\n",
    "hidden_layer_depth = 1\n",
    "latent_length = 40\n",
    "batch_size = 32\n",
    "learning_rate = 0.001 # 0.0005\n",
    "n_epochs = 100\n",
    "dropout_rate = 0.2\n",
    "cuda = True # options: True, False\n",
    "header = None\n",
    "dataset = args.dataset\n",
    "if dataset == 'c50':\n",
    "    num_class_texture = 50\n",
    "else:\n",
    "    num_class_texture = 20\n",
    "\n",
    "thickness_latent_length = args.thickness_lat\n",
    "\n",
    "# loss weightage\n",
    "w_r = args.w_r\n",
    "w_c = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Load data\n",
    "# data_dir = os.path.join(args_data_dir, \"compiled_data/\")\n",
    "logDir = 'models_and_stats/'\n",
    "if_plot = False\n",
    "\n",
    "# RNN block\n",
    "block = \"LSTM\" # LSTM, GRU, phased_LSTM\n",
    "\n",
    "model_name = 'B_block_{}_data_{}_wrI_{}_wC_{}_hidden_{}_latent_{}_r_on_{}_p_max_{}'.format(block, dataset, w_r, w_c, str(hidden_size), str(latent_length), str(args.r_on), str(args.p_max))\n",
    "\n",
    "if torch.cuda.is_available() and cuda:\n",
    "    device = torch.device(\"cuda:{}\".format(args.cuda))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "if args.reduction != 1:\n",
    "    print(\"load {} kfold number, reduce data to {} folds, put to device: {}\".format(args.kfold, args.reduction, device))\n",
    "else:\n",
    "    print(\"load {} kfold number, train with full data, put to device: {}\".format(args.kfold, device))\n",
    "\n",
    "prefix = \"\"\n",
    "dataset_dir = os.path.join(args_data_dir, dataset+\"/\") # TODO\n",
    "train_set, val_set, test_set = get_TrainValTestDataset(dataset_dir, k=0, prefix=prefix, seq_len=sequence_length)\n",
    "train_loader, val_loader, test_loader = get_TrainValTestLoader(dataset_dir, k=0, batch_size=batch_size,shuffle=shuffle, prefix=prefix,seq_len=sequence_length)\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = get_TrainValTestData(dataset_dir, k=0, prefix=prefix,seq_len=sequence_length)\n",
    "# Initialize models\n",
    "model = VRAEC_v2(num_class_texture=num_class_texture,\n",
    "            block=block,\n",
    "            sequence_length=sequence_length, # TODO\n",
    "            number_of_features = number_of_features,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            thickness_latent_length = thickness_latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate,\n",
    "            cuda = cuda,\n",
    "            model_name=model_name,\n",
    "            header=header,\n",
    "            device = device,\n",
    "            ratio_on=args.r_on, period_init_max=args.p_max)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.fc.weight False\n",
      "encoder.fc.bias False\n",
      "encoder.model.weight_ih_l0 False\n",
      "encoder.model.weight_hh_l0 False\n",
      "encoder.model.bias_ih_l0 False\n",
      "encoder.model.bias_hh_l0 False\n",
      "lmbd.hidden_to_mean.weight False\n",
      "lmbd.hidden_to_mean.bias False\n",
      "classifier_texture.0.weight False\n",
      "classifier_texture.0.bias False\n",
      "lat_to_lat.hidden_to_mean.weight True\n",
      "lat_to_lat.hidden_to_mean.bias True\n",
      "classifier_thickness.0.weight False\n",
      "classifier_thickness.0.bias False\n",
      "decoder.latent_to_hidden.weight False\n",
      "decoder.latent_to_hidden.bias False\n",
      "decoder.model.weight_ih_l0 False\n",
      "decoder.model.weight_hh_l0 False\n",
      "decoder.model.bias_ih_l0 False\n",
      "decoder.model.bias_hh_l0 False\n",
      "decoder.hidden_to_output.weight False\n",
      "decoder.hidden_to_output.bias False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name[:3] != 'lat':\n",
    "        param.requires_grad = False\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl_loss: tensor(0.0283) cl_loss_thickness: tensor(1.7619, grad_fn=<NllLossBackward>) recon_loss: tensor(116.9083)\n",
      "train f1 score: 0.38888888888888895\n",
      "val f1 score: 0.7843137254901961\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 0 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.942. Thickness Accuracy: train: 0.488, valid 0.706\n",
      "cl_loss: tensor(0.0617) cl_loss_thickness: tensor(0.4602, grad_fn=<NllLossBackward>) recon_loss: tensor(96.1730)\n",
      "train f1 score: 0.8363636363636364\n",
      "val f1 score: 0.9473684210526316\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 1 Loss: train 0.029, valid 0.031. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.839, valid 0.906\n",
      "cl_loss: tensor(0.0236) cl_loss_thickness: tensor(0.2480, grad_fn=<NllLossBackward>) recon_loss: tensor(51.6363)\n",
      "train f1 score: 0.9824561403508771\n",
      "val f1 score: 0.9508196721311476\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 2 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.944. Thickness Accuracy: train: 0.914, valid 0.917\n",
      "cl_loss: tensor(0.0294) cl_loss_thickness: tensor(0.2025, grad_fn=<NllLossBackward>) recon_loss: tensor(73.1606)\n",
      "train f1 score: 0.9666666666666666\n",
      "val f1 score: 0.9491525423728815\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 3 Loss: train 0.028, valid 0.030. Accuracy: train: 0.989, valid 0.940. Thickness Accuracy: train: 0.928, valid 0.917\n",
      "cl_loss: tensor(0.0691) cl_loss_thickness: tensor(0.3434, grad_fn=<NllLossBackward>) recon_loss: tensor(93.5048)\n",
      "train f1 score: 0.9491525423728813\n",
      "val f1 score: 0.8679245283018867\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 4 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.944. Thickness Accuracy: train: 0.927, valid 0.931\n",
      "cl_loss: tensor(0.0474) cl_loss_thickness: tensor(0.1958, grad_fn=<NllLossBackward>) recon_loss: tensor(95.1363)\n",
      "train f1 score: 0.9824561403508771\n",
      "val f1 score: 0.9836065573770492\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 5 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.931, valid 0.931\n",
      "cl_loss: tensor(0.0302) cl_loss_thickness: tensor(0.2040, grad_fn=<NllLossBackward>) recon_loss: tensor(95.4397)\n",
      "train f1 score: 0.9666666666666666\n",
      "val f1 score: 0.9473684210526316\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 6 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.936, valid 0.931\n",
      "cl_loss: tensor(0.0537) cl_loss_thickness: tensor(0.2173, grad_fn=<NllLossBackward>) recon_loss: tensor(109.1677)\n",
      "train f1 score: 0.9655172413793104\n",
      "val f1 score: 0.9666666666666666\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 7 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.934, valid 0.935\n",
      "cl_loss: tensor(0.0832) cl_loss_thickness: tensor(0.2123, grad_fn=<NllLossBackward>) recon_loss: tensor(45.9240)\n",
      "train f1 score: 0.9666666666666666\n",
      "val f1 score: 0.9655172413793104\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 8 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.935, valid 0.938\n",
      "cl_loss: tensor(0.0498) cl_loss_thickness: tensor(0.2083, grad_fn=<NllLossBackward>) recon_loss: tensor(47.4449)\n",
      "train f1 score: 0.9491525423728813\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 9 Loss: train 0.029, valid 0.029. Accuracy: train: 0.988, valid 0.942. Thickness Accuracy: train: 0.932, valid 0.933\n",
      "cl_loss: tensor(0.1469) cl_loss_thickness: tensor(0.2333, grad_fn=<NllLossBackward>) recon_loss: tensor(47.3082)\n",
      "train f1 score: 0.9655172413793104\n",
      "val f1 score: 0.967741935483871\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 10 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.944. Thickness Accuracy: train: 0.933, valid 0.938\n",
      "cl_loss: tensor(0.0488) cl_loss_thickness: tensor(0.2244, grad_fn=<NllLossBackward>) recon_loss: tensor(54.2061)\n",
      "train f1 score: 0.9508196721311476\n",
      "val f1 score: 0.9836065573770492\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 11 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.937, valid 0.940\n",
      "cl_loss: tensor(0.1110) cl_loss_thickness: tensor(0.1582, grad_fn=<NllLossBackward>) recon_loss: tensor(64.8321)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9666666666666666\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 12 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.935, valid 0.940\n",
      "cl_loss: tensor(0.0316) cl_loss_thickness: tensor(0.2538, grad_fn=<NllLossBackward>) recon_loss: tensor(93.2782)\n",
      "train f1 score: 0.9454545454545454\n",
      "val f1 score: 0.9310344827586207\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 13 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.940, valid 0.933\n",
      "cl_loss: tensor(0.0591) cl_loss_thickness: tensor(0.1865, grad_fn=<NllLossBackward>) recon_loss: tensor(62.3338)\n",
      "train f1 score: 0.9655172413793104\n",
      "val f1 score: 0.9433962264150945\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 14 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.942. Thickness Accuracy: train: 0.935, valid 0.938\n",
      "cl_loss: tensor(0.0385) cl_loss_thickness: tensor(0.2011, grad_fn=<NllLossBackward>) recon_loss: tensor(91.4162)\n",
      "train f1 score: 0.9491525423728813\n",
      "val f1 score: 0.9824561403508771\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 15 Loss: train 0.029, valid 0.031. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.938, valid 0.933\n",
      "cl_loss: tensor(0.0315) cl_loss_thickness: tensor(0.0767, grad_fn=<NllLossBackward>) recon_loss: tensor(229.3556)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9655172413793104\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 16 Loss: train 0.029, valid 0.030. Accuracy: train: 0.990, valid 0.938. Thickness Accuracy: train: 0.933, valid 0.933\n",
      "cl_loss: tensor(0.0275) cl_loss_thickness: tensor(0.1970, grad_fn=<NllLossBackward>) recon_loss: tensor(104.9030)\n",
      "train f1 score: 0.9259259259259259\n",
      "val f1 score: 0.9056603773584904\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 17 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.940. Thickness Accuracy: train: 0.939, valid 0.933\n",
      "cl_loss: tensor(0.1300) cl_loss_thickness: tensor(0.2484, grad_fn=<NllLossBackward>) recon_loss: tensor(71.4493)\n",
      "train f1 score: 0.9508196721311475\n",
      "val f1 score: 0.9836065573770492\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 18 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.944. Thickness Accuracy: train: 0.938, valid 0.940\n",
      "cl_loss: tensor(0.0893) cl_loss_thickness: tensor(0.0597, grad_fn=<NllLossBackward>) recon_loss: tensor(162.1501)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 19 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.937, valid 0.935\n",
      "cl_loss: tensor(0.0530) cl_loss_thickness: tensor(0.1178, grad_fn=<NllLossBackward>) recon_loss: tensor(49.3466)\n",
      "train f1 score: 0.9818181818181818\n",
      "val f1 score: 0.9642857142857143\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 20 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.935, valid 0.935\n",
      "cl_loss: tensor(0.0729) cl_loss_thickness: tensor(0.1893, grad_fn=<NllLossBackward>) recon_loss: tensor(73.1529)\n",
      "train f1 score: 0.9259259259259259\n",
      "val f1 score: 0.9824561403508771\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 21 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.942. Thickness Accuracy: train: 0.937, valid 0.935\n",
      "cl_loss: tensor(0.0282) cl_loss_thickness: tensor(0.1712, grad_fn=<NllLossBackward>) recon_loss: tensor(89.2916)\n",
      "train f1 score: 0.9666666666666666\n",
      "val f1 score: 0.9642857142857143\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 22 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.939, valid 0.938\n",
      "cl_loss: tensor(0.1186) cl_loss_thickness: tensor(0.1620, grad_fn=<NllLossBackward>) recon_loss: tensor(91.4776)\n",
      "train f1 score: 0.9824561403508771\n",
      "val f1 score: 0.9824561403508771\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 23 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.942. Thickness Accuracy: train: 0.940, valid 0.942\n",
      "cl_loss: tensor(0.0410) cl_loss_thickness: tensor(0.2000, grad_fn=<NllLossBackward>) recon_loss: tensor(68.3029)\n",
      "train f1 score: 0.9655172413793104\n",
      "val f1 score: 0.9811320754716981\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 24 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.943, valid 0.933\n",
      "cl_loss: tensor(0.0191) cl_loss_thickness: tensor(0.2256, grad_fn=<NllLossBackward>) recon_loss: tensor(64.7847)\n",
      "train f1 score: 0.9056603773584904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val f1 score: 0.9824561403508771\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 25 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.943, valid 0.940\n",
      "cl_loss: tensor(0.0298) cl_loss_thickness: tensor(0.0737, grad_fn=<NllLossBackward>) recon_loss: tensor(79.5054)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9642857142857143\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 26 Loss: train 0.029, valid 0.029. Accuracy: train: 0.989, valid 0.944. Thickness Accuracy: train: 0.939, valid 0.944\n",
      "cl_loss: tensor(0.0916) cl_loss_thickness: tensor(0.0783, grad_fn=<NllLossBackward>) recon_loss: tensor(76.2005)\n",
      "train f1 score: 0.9836065573770492\n",
      "val f1 score: 0.9803921568627451\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 27 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.944, valid 0.942\n",
      "cl_loss: tensor(0.0428) cl_loss_thickness: tensor(0.2149, grad_fn=<NllLossBackward>) recon_loss: tensor(63.1984)\n",
      "train f1 score: 0.9454545454545454\n",
      "val f1 score: 0.9642857142857143\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 28 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.938. Thickness Accuracy: train: 0.940, valid 0.946\n",
      "cl_loss: tensor(0.0695) cl_loss_thickness: tensor(0.2280, grad_fn=<NllLossBackward>) recon_loss: tensor(80.8586)\n",
      "train f1 score: 0.9454545454545454\n",
      "val f1 score: 0.9411764705882353\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 29 Loss: train 0.029, valid 0.031. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.946, valid 0.946\n",
      "cl_loss: tensor(0.0334) cl_loss_thickness: tensor(0.0711, grad_fn=<NllLossBackward>) recon_loss: tensor(81.7322)\n",
      "train f1 score: 0.983050847457627\n",
      "val f1 score: 0.9818181818181818\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 30 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.942. Thickness Accuracy: train: 0.948, valid 0.942\n",
      "cl_loss: tensor(0.0291) cl_loss_thickness: tensor(0.1157, grad_fn=<NllLossBackward>) recon_loss: tensor(60.7080)\n",
      "train f1 score: 0.9655172413793104\n",
      "val f1 score: 0.9824561403508771\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 31 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.940. Thickness Accuracy: train: 0.946, valid 0.944\n",
      "cl_loss: tensor(0.0532) cl_loss_thickness: tensor(0.2936, grad_fn=<NllLossBackward>) recon_loss: tensor(70.2091)\n",
      "train f1 score: 0.9285714285714286\n",
      "val f1 score: 0.9454545454545454\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 32 Loss: train 0.029, valid 0.029. Accuracy: train: 0.988, valid 0.942. Thickness Accuracy: train: 0.950, valid 0.944\n",
      "cl_loss: tensor(0.0441) cl_loss_thickness: tensor(0.0844, grad_fn=<NllLossBackward>) recon_loss: tensor(108.1480)\n",
      "train f1 score: 0.983050847457627\n",
      "val f1 score: 0.9615384615384615\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 33 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.940. Thickness Accuracy: train: 0.946, valid 0.942\n",
      "cl_loss: tensor(0.0256) cl_loss_thickness: tensor(0.1071, grad_fn=<NllLossBackward>) recon_loss: tensor(64.9253)\n",
      "train f1 score: 0.983050847457627\n",
      "val f1 score: 0.9454545454545454\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 34 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.938. Thickness Accuracy: train: 0.948, valid 0.944\n",
      "cl_loss: tensor(0.0257) cl_loss_thickness: tensor(0.1656, grad_fn=<NllLossBackward>) recon_loss: tensor(132.1971)\n",
      "train f1 score: 0.9491525423728813\n",
      "val f1 score: 0.9642857142857143\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 35 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.942. Thickness Accuracy: train: 0.948, valid 0.944\n",
      "cl_loss: tensor(0.0369) cl_loss_thickness: tensor(0.0642, grad_fn=<NllLossBackward>) recon_loss: tensor(65.5226)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9655172413793104\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 36 Loss: train 0.029, valid 0.029. Accuracy: train: 0.989, valid 0.938. Thickness Accuracy: train: 0.945, valid 0.952\n",
      "cl_loss: tensor(0.0969) cl_loss_thickness: tensor(0.0376, grad_fn=<NllLossBackward>) recon_loss: tensor(136.3770)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9824561403508771\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 37 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.942. Thickness Accuracy: train: 0.952, valid 0.946\n",
      "cl_loss: tensor(0.0550) cl_loss_thickness: tensor(0.2261, grad_fn=<NllLossBackward>) recon_loss: tensor(84.8022)\n",
      "train f1 score: 0.9642857142857143\n",
      "val f1 score: 0.9615384615384615\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 38 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.948, valid 0.948\n",
      "cl_loss: tensor(0.0329) cl_loss_thickness: tensor(0.0746, grad_fn=<NllLossBackward>) recon_loss: tensor(69.7196)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9666666666666666\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 39 Loss: train 0.028, valid 0.029. Accuracy: train: 0.988, valid 0.946. Thickness Accuracy: train: 0.950, valid 0.948\n",
      "cl_loss: tensor(0.0412) cl_loss_thickness: tensor(0.0865, grad_fn=<NllLossBackward>) recon_loss: tensor(271.1899)\n",
      "train f1 score: 0.9666666666666666\n",
      "val f1 score: 0.9473684210526316\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 40 Loss: train 0.028, valid 0.030. Accuracy: train: 0.989, valid 0.942. Thickness Accuracy: train: 0.950, valid 0.948\n",
      "cl_loss: tensor(0.0364) cl_loss_thickness: tensor(0.0510, grad_fn=<NllLossBackward>) recon_loss: tensor(65.1078)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9655172413793104\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 41 Loss: train 0.029, valid 0.029. Accuracy: train: 0.988, valid 0.942. Thickness Accuracy: train: 0.952, valid 0.950\n",
      "cl_loss: tensor(0.0676) cl_loss_thickness: tensor(0.1537, grad_fn=<NllLossBackward>) recon_loss: tensor(71.7482)\n",
      "train f1 score: 0.9491525423728813\n",
      "val f1 score: 0.9629629629629629\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 42 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.953, valid 0.944\n",
      "cl_loss: tensor(0.0392) cl_loss_thickness: tensor(0.1461, grad_fn=<NllLossBackward>) recon_loss: tensor(108.3421)\n",
      "train f1 score: 0.9803921568627451\n",
      "val f1 score: 0.9310344827586207\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 43 Loss: train 0.029, valid 0.031. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.952, valid 0.950\n",
      "cl_loss: tensor(0.1708) cl_loss_thickness: tensor(0.1090, grad_fn=<NllLossBackward>) recon_loss: tensor(71.0062)\n",
      "train f1 score: 0.9666666666666666\n",
      "val f1 score: 0.9642857142857143\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 44 Loss: train 0.028, valid 0.031. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.952, valid 0.952\n",
      "cl_loss: tensor(0.0720) cl_loss_thickness: tensor(0.2170, grad_fn=<NllLossBackward>) recon_loss: tensor(60.0843)\n",
      "train f1 score: 0.9491525423728815\n",
      "val f1 score: 0.983050847457627\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 45 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.952, valid 0.950\n",
      "cl_loss: tensor(0.0483) cl_loss_thickness: tensor(0.1425, grad_fn=<NllLossBackward>) recon_loss: tensor(77.1541)\n",
      "train f1 score: 0.9811320754716981\n",
      "val f1 score: 0.912280701754386\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 46 Loss: train 0.028, valid 0.030. Accuracy: train: 0.990, valid 0.944. Thickness Accuracy: train: 0.951, valid 0.950\n",
      "cl_loss: tensor(0.0570) cl_loss_thickness: tensor(0.1510, grad_fn=<NllLossBackward>) recon_loss: tensor(90.9670)\n",
      "train f1 score: 0.9655172413793104\n",
      "val f1 score: 0.9811320754716981\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 47 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.949, valid 0.956\n",
      "cl_loss: tensor(0.0251) cl_loss_thickness: tensor(0.2514, grad_fn=<NllLossBackward>) recon_loss: tensor(68.3579)\n",
      "train f1 score: 0.9056603773584904\n",
      "val f1 score: 0.9642857142857143\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 48 Loss: train 0.029, valid 0.029. Accuracy: train: 0.990, valid 0.940. Thickness Accuracy: train: 0.949, valid 0.952\n",
      "cl_loss: tensor(0.0748) cl_loss_thickness: tensor(0.1452, grad_fn=<NllLossBackward>) recon_loss: tensor(105.8012)\n",
      "train f1 score: 0.9666666666666666\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 49 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.953, valid 0.952\n",
      "cl_loss: tensor(0.0594) cl_loss_thickness: tensor(0.0876, grad_fn=<NllLossBackward>) recon_loss: tensor(57.4250)\n",
      "train f1 score: 0.9824561403508771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val f1 score: 0.9491525423728813\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 50 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.951, valid 0.952\n",
      "cl_loss: tensor(0.0483) cl_loss_thickness: tensor(0.1698, grad_fn=<NllLossBackward>) recon_loss: tensor(128.9028)\n",
      "train f1 score: 0.923076923076923\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 51 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.952, valid 0.950\n",
      "cl_loss: tensor(0.0406) cl_loss_thickness: tensor(0.1855, grad_fn=<NllLossBackward>) recon_loss: tensor(91.8329)\n",
      "train f1 score: 0.9411764705882353\n",
      "val f1 score: 0.9642857142857143\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 52 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.950, valid 0.952\n",
      "cl_loss: tensor(0.0568) cl_loss_thickness: tensor(0.1740, grad_fn=<NllLossBackward>) recon_loss: tensor(89.6085)\n",
      "train f1 score: 0.9818181818181818\n",
      "val f1 score: 0.9583333333333334\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 53 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.942. Thickness Accuracy: train: 0.955, valid 0.956\n",
      "cl_loss: tensor(0.0608) cl_loss_thickness: tensor(0.0940, grad_fn=<NllLossBackward>) recon_loss: tensor(79.8616)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.983050847457627\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 54 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.956, valid 0.954\n",
      "cl_loss: tensor(0.1376) cl_loss_thickness: tensor(0.2082, grad_fn=<NllLossBackward>) recon_loss: tensor(89.4489)\n",
      "train f1 score: 0.9387755102040816\n",
      "val f1 score: 0.9642857142857143\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 55 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.954, valid 0.958\n",
      "cl_loss: tensor(0.0383) cl_loss_thickness: tensor(0.2303, grad_fn=<NllLossBackward>) recon_loss: tensor(46.4608)\n",
      "train f1 score: 0.9285714285714286\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 56 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.942. Thickness Accuracy: train: 0.954, valid 0.954\n",
      "cl_loss: tensor(0.0334) cl_loss_thickness: tensor(0.1283, grad_fn=<NllLossBackward>) recon_loss: tensor(49.9752)\n",
      "train f1 score: 0.9655172413793104\n",
      "val f1 score: 0.9655172413793104\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 57 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.954, valid 0.956\n",
      "cl_loss: tensor(0.0565) cl_loss_thickness: tensor(0.0451, grad_fn=<NllLossBackward>) recon_loss: tensor(102.8683)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9824561403508771\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 58 Loss: train 0.029, valid 0.030. Accuracy: train: 0.990, valid 0.940. Thickness Accuracy: train: 0.952, valid 0.956\n",
      "cl_loss: tensor(0.0428) cl_loss_thickness: tensor(0.0942, grad_fn=<NllLossBackward>) recon_loss: tensor(80.6998)\n",
      "train f1 score: 0.9836065573770492\n",
      "val f1 score: 0.9473684210526316\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 59 Loss: train 0.029, valid 0.031. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.957, valid 0.952\n",
      "cl_loss: tensor(0.0235) cl_loss_thickness: tensor(0.1339, grad_fn=<NllLossBackward>) recon_loss: tensor(105.3804)\n",
      "train f1 score: 0.9666666666666666\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 60 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.942. Thickness Accuracy: train: 0.955, valid 0.956\n",
      "cl_loss: tensor(0.0277) cl_loss_thickness: tensor(0.1008, grad_fn=<NllLossBackward>) recon_loss: tensor(60.3142)\n",
      "train f1 score: 0.9655172413793104\n",
      "val f1 score: 0.9655172413793104\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 61 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.953, valid 0.952\n",
      "cl_loss: tensor(0.0536) cl_loss_thickness: tensor(0.1230, grad_fn=<NllLossBackward>) recon_loss: tensor(53.4304)\n",
      "train f1 score: 0.983050847457627\n",
      "val f1 score: 0.9655172413793104\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 62 Loss: train 0.028, valid 0.030. Accuracy: train: 0.989, valid 0.940. Thickness Accuracy: train: 0.954, valid 0.956\n",
      "cl_loss: tensor(0.0551) cl_loss_thickness: tensor(0.0807, grad_fn=<NllLossBackward>) recon_loss: tensor(103.7253)\n",
      "train f1 score: 0.9824561403508771\n",
      "val f1 score: 0.923076923076923\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 63 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.957, valid 0.954\n",
      "cl_loss: tensor(0.0348) cl_loss_thickness: tensor(0.1652, grad_fn=<NllLossBackward>) recon_loss: tensor(76.6072)\n",
      "train f1 score: 0.9818181818181818\n",
      "val f1 score: 0.983050847457627\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 64 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.957, valid 0.954\n",
      "cl_loss: tensor(0.0131) cl_loss_thickness: tensor(0.2460, grad_fn=<NllLossBackward>) recon_loss: tensor(92.8353)\n",
      "train f1 score: 0.9285714285714286\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 65 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.940. Thickness Accuracy: train: 0.954, valid 0.956\n",
      "cl_loss: tensor(0.0442) cl_loss_thickness: tensor(0.0943, grad_fn=<NllLossBackward>) recon_loss: tensor(50.6994)\n",
      "train f1 score: 0.9824561403508771\n",
      "val f1 score: 0.962962962962963\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 66 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.940. Thickness Accuracy: train: 0.955, valid 0.956\n",
      "cl_loss: tensor(0.0798) cl_loss_thickness: tensor(0.0738, grad_fn=<NllLossBackward>) recon_loss: tensor(117.0518)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 67 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.956, valid 0.954\n",
      "cl_loss: tensor(0.0663) cl_loss_thickness: tensor(0.0818, grad_fn=<NllLossBackward>) recon_loss: tensor(76.5042)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9836065573770492\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 68 Loss: train 0.029, valid 0.029. Accuracy: train: 0.989, valid 0.948. Thickness Accuracy: train: 0.960, valid 0.954\n",
      "cl_loss: tensor(0.0663) cl_loss_thickness: tensor(0.1739, grad_fn=<NllLossBackward>) recon_loss: tensor(66.7986)\n",
      "train f1 score: 0.9454545454545454\n",
      "val f1 score: 0.983050847457627\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 69 Loss: train 0.029, valid 0.031. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.958, valid 0.958\n",
      "cl_loss: tensor(0.0487) cl_loss_thickness: tensor(0.0320, grad_fn=<NllLossBackward>) recon_loss: tensor(48.8351)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9642857142857143\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 70 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.954, valid 0.960\n",
      "cl_loss: tensor(0.0392) cl_loss_thickness: tensor(0.1936, grad_fn=<NllLossBackward>) recon_loss: tensor(61.3523)\n",
      "train f1 score: 0.9655172413793104\n",
      "val f1 score: 0.9642857142857143\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 71 Loss: train 0.029, valid 0.029. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.957, valid 0.960\n",
      "cl_loss: tensor(0.0301) cl_loss_thickness: tensor(0.0942, grad_fn=<NllLossBackward>) recon_loss: tensor(75.9021)\n",
      "train f1 score: 0.983050847457627\n",
      "val f1 score: 0.9655172413793104\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 72 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.956, valid 0.954\n",
      "cl_loss: tensor(0.0491) cl_loss_thickness: tensor(0.0760, grad_fn=<NllLossBackward>) recon_loss: tensor(65.9648)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9655172413793104\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 73 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.956, valid 0.958\n",
      "cl_loss: tensor(0.0527) cl_loss_thickness: tensor(0.0761, grad_fn=<NllLossBackward>) recon_loss: tensor(93.8898)\n",
      "train f1 score: 0.9824561403508771\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 74 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.942. Thickness Accuracy: train: 0.958, valid 0.956\n",
      "cl_loss: tensor(0.0732) cl_loss_thickness: tensor(0.1296, grad_fn=<NllLossBackward>) recon_loss: tensor(105.8920)\n",
      "train f1 score: 0.9803921568627451\n",
      "val f1 score: 0.9824561403508771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num 1472, val_num 480\n",
      "Epoch: 75 Loss: train 0.029, valid 0.031. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.955, valid 0.960\n",
      "cl_loss: tensor(0.0203) cl_loss_thickness: tensor(0.0951, grad_fn=<NllLossBackward>) recon_loss: tensor(102.6214)\n",
      "train f1 score: 0.983050847457627\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 76 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.959, valid 0.958\n",
      "cl_loss: tensor(0.0399) cl_loss_thickness: tensor(0.0602, grad_fn=<NllLossBackward>) recon_loss: tensor(80.4106)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 77 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.959, valid 0.952\n",
      "cl_loss: tensor(0.0516) cl_loss_thickness: tensor(0.0991, grad_fn=<NllLossBackward>) recon_loss: tensor(71.0753)\n",
      "train f1 score: 0.9818181818181818\n",
      "val f1 score: 0.9818181818181818\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 78 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.957, valid 0.958\n",
      "cl_loss: tensor(0.0445) cl_loss_thickness: tensor(0.1003, grad_fn=<NllLossBackward>) recon_loss: tensor(68.7238)\n",
      "train f1 score: 0.9818181818181818\n",
      "val f1 score: 0.9310344827586207\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 79 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.940. Thickness Accuracy: train: 0.963, valid 0.960\n",
      "cl_loss: tensor(0.0377) cl_loss_thickness: tensor(0.0531, grad_fn=<NllLossBackward>) recon_loss: tensor(135.6863)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9642857142857143\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 80 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.946. Thickness Accuracy: train: 0.961, valid 0.960\n",
      "cl_loss: tensor(0.0254) cl_loss_thickness: tensor(0.0497, grad_fn=<NllLossBackward>) recon_loss: tensor(141.2918)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9811320754716981\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 81 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.942. Thickness Accuracy: train: 0.961, valid 0.960\n",
      "cl_loss: tensor(0.0396) cl_loss_thickness: tensor(0.0547, grad_fn=<NllLossBackward>) recon_loss: tensor(237.1427)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 82 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.942. Thickness Accuracy: train: 0.962, valid 0.963\n",
      "cl_loss: tensor(0.0187) cl_loss_thickness: tensor(0.0875, grad_fn=<NllLossBackward>) recon_loss: tensor(71.7290)\n",
      "train f1 score: 0.9818181818181818\n",
      "val f1 score: 0.9818181818181818\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 83 Loss: train 0.028, valid 0.030. Accuracy: train: 0.989, valid 0.940. Thickness Accuracy: train: 0.959, valid 0.960\n",
      "cl_loss: tensor(0.0704) cl_loss_thickness: tensor(0.0615, grad_fn=<NllLossBackward>) recon_loss: tensor(73.0471)\n",
      "train f1 score: 0.9836065573770492\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 84 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.940. Thickness Accuracy: train: 0.961, valid 0.960\n",
      "cl_loss: tensor(0.0741) cl_loss_thickness: tensor(0.0321, grad_fn=<NllLossBackward>) recon_loss: tensor(53.5243)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9666666666666666\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 85 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.962, valid 0.960\n",
      "cl_loss: tensor(0.0503) cl_loss_thickness: tensor(0.1221, grad_fn=<NllLossBackward>) recon_loss: tensor(70.7034)\n",
      "train f1 score: 0.9642857142857143\n",
      "val f1 score: 0.9824561403508771\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 86 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.944. Thickness Accuracy: train: 0.961, valid 0.960\n",
      "cl_loss: tensor(0.0428) cl_loss_thickness: tensor(0.0988, grad_fn=<NllLossBackward>) recon_loss: tensor(45.4666)\n",
      "train f1 score: 0.9824561403508771\n",
      "val f1 score: 0.9666666666666667\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 87 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.944. Thickness Accuracy: train: 0.963, valid 0.956\n",
      "cl_loss: tensor(0.1527) cl_loss_thickness: tensor(0.1353, grad_fn=<NllLossBackward>) recon_loss: tensor(84.2720)\n",
      "train f1 score: 0.9824561403508771\n",
      "val f1 score: 0.962962962962963\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 88 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.940. Thickness Accuracy: train: 0.965, valid 0.960\n",
      "cl_loss: tensor(0.0724) cl_loss_thickness: tensor(0.0443, grad_fn=<NllLossBackward>) recon_loss: tensor(86.4194)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9655172413793104\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 89 Loss: train 0.028, valid 0.030. Accuracy: train: 0.989, valid 0.942. Thickness Accuracy: train: 0.963, valid 0.958\n",
      "cl_loss: tensor(0.0452) cl_loss_thickness: tensor(0.0319, grad_fn=<NllLossBackward>) recon_loss: tensor(105.4976)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 90 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.963, valid 0.960\n",
      "cl_loss: tensor(0.0183) cl_loss_thickness: tensor(0.0547, grad_fn=<NllLossBackward>) recon_loss: tensor(64.1939)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9642857142857143\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 91 Loss: train 0.029, valid 0.030. Accuracy: train: 0.988, valid 0.938. Thickness Accuracy: train: 0.963, valid 0.960\n",
      "cl_loss: tensor(0.0440) cl_loss_thickness: tensor(0.2221, grad_fn=<NllLossBackward>) recon_loss: tensor(75.6593)\n",
      "train f1 score: 0.9152542372881356\n",
      "val f1 score: 0.9310344827586207\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 92 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.940. Thickness Accuracy: train: 0.963, valid 0.965\n",
      "cl_loss: tensor(0.0147) cl_loss_thickness: tensor(0.0996, grad_fn=<NllLossBackward>) recon_loss: tensor(95.3145)\n",
      "train f1 score: 0.9824561403508771\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 93 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.940. Thickness Accuracy: train: 0.962, valid 0.965\n",
      "cl_loss: tensor(0.0813) cl_loss_thickness: tensor(0.0588, grad_fn=<NllLossBackward>) recon_loss: tensor(101.0252)\n",
      "train f1 score: 0.9836065573770492\n",
      "val f1 score: 0.9836065573770492\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 94 Loss: train 0.028, valid 0.030. Accuracy: train: 0.988, valid 0.942. Thickness Accuracy: train: 0.964, valid 0.967\n",
      "cl_loss: tensor(0.0611) cl_loss_thickness: tensor(0.0998, grad_fn=<NllLossBackward>) recon_loss: tensor(104.3363)\n",
      "train f1 score: 0.9824561403508771\n",
      "val f1 score: 1.0\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 95 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.944. Thickness Accuracy: train: 0.963, valid 0.963\n",
      "cl_loss: tensor(0.0563) cl_loss_thickness: tensor(0.0433, grad_fn=<NllLossBackward>) recon_loss: tensor(99.2656)\n",
      "train f1 score: 1.0\n",
      "val f1 score: 0.9642857142857143\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 96 Loss: train 0.028, valid 0.030. Accuracy: train: 0.989, valid 0.944. Thickness Accuracy: train: 0.967, valid 0.963\n",
      "cl_loss: tensor(0.0347) cl_loss_thickness: tensor(0.1442, grad_fn=<NllLossBackward>) recon_loss: tensor(68.6113)\n",
      "train f1 score: 0.962962962962963\n",
      "val f1 score: 0.962962962962963\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 97 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.938. Thickness Accuracy: train: 0.965, valid 0.963\n",
      "cl_loss: tensor(0.0355) cl_loss_thickness: tensor(0.0669, grad_fn=<NllLossBackward>) recon_loss: tensor(106.1019)\n",
      "train f1 score: 0.9642857142857143\n",
      "val f1 score: 0.9600000000000001\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 98 Loss: train 0.029, valid 0.029. Accuracy: train: 0.988, valid 0.944. Thickness Accuracy: train: 0.965, valid 0.963\n",
      "cl_loss: tensor(0.0459) cl_loss_thickness: tensor(0.1188, grad_fn=<NllLossBackward>) recon_loss: tensor(66.5658)\n",
      "train f1 score: 0.9803921568627451\n",
      "val f1 score: 0.9824561403508771\n",
      "train_num 1472, val_num 480\n",
      "Epoch: 99 Loss: train 0.029, valid 0.030. Accuracy: train: 0.989, valid 0.942. Thickness Accuracy: train: 0.965, valid 0.960\n",
      "Best model at epoch 0 with acc 0.000\n",
      "training takes time 0:13:22.481807\n",
      " test batch 15 size 20 < 32, skip\n",
      "last epoch Test accuracy for 0  fold :  0.9229166666666667 0.9791666666666666\n"
     ]
    }
   ],
   "source": [
    "# Initialize training settings\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "cl_loss_fn = nn.NLLLoss()\n",
    "recon_loss_fn = nn.MSELoss()\n",
    "\n",
    "# model.load_state_dict(torch.load('models_and_stats/model_phased_LSTM_B30.pt', map_location='cpu'))\n",
    "saved_dicts = torch.load('models_and_stats/'+model_name+str(95)+'.pt', map_location='cpu')\n",
    "model.load_state_dict(saved_dicts['model_state_dict'])\n",
    "optimizer.load_state_dict(saved_dicts['optimizer_state_dict'])\n",
    "\n",
    "training_start=datetime.now()\n",
    "# create empty lists to fill stats later\n",
    "epoch_train_loss = []\n",
    "epoch_train_acc = []\n",
    "epoch_train_thickness_acc = []\n",
    "epoch_val_loss = []\n",
    "epoch_val_acc = []\n",
    "epoch_val_thickness_acc = []\n",
    "max_val_acc = 0\n",
    "max_val_epoch = 0\n",
    "if block == \"phased_LSTM\":\n",
    "    time = torch.Tensor(range(sequence_length))\n",
    "    times = time.repeat(batch_size, 1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # if epoch < 30:\n",
    "    #     continue\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    correct_texture = 0\n",
    "    correct_thickness = 0\n",
    "    train_loss = 0\n",
    "    train_num = 0\n",
    "    for i, (XB,  y) in enumerate(train_loader):\n",
    "        if model.header == 'CNN':\n",
    "            x = XI\n",
    "        else:\n",
    "            x = XB\n",
    "        # x = x[:, :, 1::2]\n",
    "        x, y = x.to(device), y.long().to(device) # 32, 19, 400\n",
    "        if x.size()[0] != batch_size:\n",
    "            break\n",
    "\n",
    "        # reduce data by data_reduction_ratio times\n",
    "        if i % data_reduction_ratio == 0:\n",
    "            train_num += x.size(0)\n",
    "            optimizer.zero_grad()\n",
    "            if block == \"phased_LSTM\":\n",
    "                x_decoded, latent, output = model(x, times)\n",
    "            else:\n",
    "                x_decoded, latent, output_texture, output_thickness = model(x)\n",
    "\n",
    "            # assert not torch.isnan(y).any(), \"batch_num=\"+str(i)\n",
    "            # print((output == 0).nonzero().size(0)==0)\n",
    "\n",
    "            # assert (output == 0).nonzero().size(0)==0, 'output contain zero, batch_num'+str(i)+' indices:'+str((output == 0).nonzero())\n",
    "            if (output_texture == 0).nonzero().size(0) != 0:\n",
    "                print('batch_num'+str(i)+' indices:'+str((output_texture == 0).nonzero()))\n",
    "                cl_loss = cl_loss_fn(output_texture+1e-5, y) # avoid nan\n",
    "            else:\n",
    "                cl_loss = cl_loss_fn(output_texture, y) \n",
    "\n",
    "            thick_y = property_label(y)\n",
    "\n",
    "            cl_loss_thickness = cl_loss_fn(output_thickness, thick_y)\n",
    "\n",
    "            recon_loss = recon_loss_fn(x_decoded, x)\n",
    "            loss = w_c*cl_loss + w_r *recon_loss # TODO thickness loss\n",
    "\n",
    "            # compute classification acc\n",
    "            pred_texture = output_texture.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct_texture += pred_texture.eq(y.data.view_as(pred_texture)).long().cpu().sum().item()\n",
    "            pred_thickness = output_thickness.data.max(1, keepdim=True)[1]\n",
    "            correct_thickness += pred_thickness.eq(thick_y.data.view_as(pred_thickness)).long().cpu().sum().item()\n",
    "            # accumulator\n",
    "            train_loss += loss.item()\n",
    "            start_bp = datetime.now()\n",
    "            cl_loss_thickness.backward()\n",
    "            figname = logDir + model_name + \"grad_flow_plot_epoch\" +str(epoch)+\".png\"\n",
    "            if i == 0: # and epoch%50 == 0:\n",
    "                print('cl_loss:', cl_loss, 'cl_loss_thickness:', cl_loss_thickness, 'recon_loss:', recon_loss)\n",
    "                f1 = f1_score(thick_y, pred_thickness)\n",
    "                print('train f1 score:', f1)\n",
    "\n",
    "                # print(\"grad flow for epoch {}\".format(epoch))\n",
    "                # plot_grad_flow(model.named_parameters(), figname, if_plot=False)\n",
    "\n",
    "                # if epoch % 5 == 0:\n",
    "                #     k = model.encoder.k_out\n",
    "                #     k = k.squeeze()\n",
    "                #     # print('k:',k[:, 0, 70])\n",
    "                #     # tau = model.encoder.model.phased_cell.tau\n",
    "                #     # print('tau:', tau)\n",
    "                #     # phase = model.encoder.model.phased_cell.phase\n",
    "                #     # print('phase:', phase)\n",
    "                #     k = k[:,0,:].cpu().detach().numpy()\n",
    "                #     x = x[0,:,:].permute(1,0)\n",
    "                #     x = x.cpu().detach().numpy()\n",
    "                #     fig, ax =plt.subplots(1,2)\n",
    "                #     sns.heatmap(x, ax=ax[0])\n",
    "                #     sns.heatmap(k, vmin=0, vmax=1, ax=ax[1])\n",
    "                #     fig.savefig(logDir + model_name + \"x_k_plot_epoch\" +str(epoch)+\".png\")\n",
    "                #     fig.clf()\n",
    "            optimizer.step()\n",
    "            # print('1 batch bp time:', datetime.now()-start_bp)\n",
    "\n",
    "    # if epoch == 0:\n",
    "    #     print('first epoch training time:', datetime.now()-training_start)\n",
    "\n",
    "    # if epoch < 20 or epoch%200 == 0:\n",
    "    # print(\"train last batch {} of {}: cl_loss {:.3f} recon_loss {:.3f}\".format(i, len(train_loader), cl_loss, recon_loss))\n",
    "\n",
    "    # fill stats\n",
    "    train_accuracy = correct_texture / train_num \n",
    "    train_thickness_accuracy = correct_thickness / train_num \n",
    "    train_loss /= train_num\n",
    "    epoch_train_loss.append(train_loss) \n",
    "    epoch_train_acc.append(train_accuracy) \n",
    "    epoch_train_thickness_acc.append(train_thickness_accuracy) \n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    correct_texture = 0\n",
    "    correct_thickness = 0\n",
    "    val_loss = 0\n",
    "    val_num = 0\n",
    "    for i, (XB, y) in enumerate(val_loader):\n",
    "        if model.header == 'CNN':\n",
    "            x = XI\n",
    "        else:\n",
    "            x = XB\n",
    "        # x = x[:, :, 1::2]\n",
    "        x, y = x.to(device), y.long().to(device)\n",
    "        if x.size()[0] != batch_size:\n",
    "            break\n",
    "        val_num += x.size(0)\n",
    "        if block == \"phased_LSTM\":\n",
    "            x_decoded, latent, output = model(x, times)\n",
    "        else:\n",
    "            x_decoded, latent, output_texture, output_thickness = model(x)\n",
    "\n",
    "        # construct loss function\n",
    "        thick_y = property_label(y)\n",
    "        cl_loss = cl_loss_fn(output_texture, y)\n",
    "        cl_loss_thickness = cl_loss_fn(output_thickness, thick_y)\n",
    "        recon_loss = recon_loss_fn(x_decoded, x)\n",
    "        loss = w_c*cl_loss + w_r *recon_loss\n",
    "\n",
    "        # compute classification acc\n",
    "        pred_texture = output_texture.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct_texture += pred_texture.eq(y.data.view_as(pred_texture)).long().cpu().sum().item()\n",
    "        pred_thickness = output_thickness.data.max(1, keepdim=True)[1]\n",
    "        correct_thickness += pred_thickness.eq(thick_y.data.view_as(pred_thickness)).long().cpu().sum().item()\n",
    "        \n",
    "        if i == 0:\n",
    "            f1 = f1_score(thick_y, pred_thickness)\n",
    "            print('val f1 score:', f1)\n",
    "\n",
    "        # accumulator\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    # fill stats\n",
    "    val_accuracy = correct_texture / val_num\n",
    "    val_loss /= val_num\n",
    "    val_thickness_accuracy = correct_thickness / val_num \n",
    "    epoch_val_loss.append(val_loss)  # only save the last batch\n",
    "    epoch_val_acc.append(val_accuracy)\n",
    "    epoch_val_thickness_acc.append(val_thickness_accuracy) \n",
    "\n",
    "    # if epoch < 20 or epoch%200 == 0:\n",
    "    print(\"train_num {}, val_num {}\".format(train_num, val_num))\n",
    "    print('Epoch: {} Loss: train {:.3f}, valid {:.3f}. Accuracy: train: {:.3f}, valid {:.3f}. Thickness Accuracy: train: {:.3f}, valid {:.3f}'.format(epoch, train_loss, val_loss, train_accuracy, val_accuracy, train_thickness_accuracy, val_thickness_accuracy))\n",
    "\n",
    "#     # choose model\n",
    "#     if max_val_acc <= val_accuracy:\n",
    "#         model_dir = logDir + model_name + str(epoch) + '.pt'\n",
    "#         print('Saving model at {} epoch to {}'.format(epoch, model_dir))\n",
    "#         max_val_acc = val_accuracy\n",
    "#         max_val_epoch = epoch\n",
    "#         torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, model_dir)\n",
    "    # TODO\n",
    "    # model_dir = logDir + model_name + str(epoch) + '.pt'\n",
    "    # print('Saving model at {} epoch to {}'.format(epoch, model_dir))\n",
    "    # max_val_acc = val_accuracy\n",
    "    # torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, model_dir)\n",
    "print('Best model at epoch {} with acc {:.3f}'.format(max_val_epoch, max_val_acc))\n",
    "training_end =  datetime.now()\n",
    "training_time = training_end -training_start \n",
    "print(\"training takes time {}\".format(training_time))\n",
    "\n",
    "model.is_fitted = True\n",
    "model.eval()\n",
    "\n",
    "# TEST at last epoch\n",
    "correct_texture = 0\n",
    "correct_thickness = 0\n",
    "test_num = 0\n",
    "for i, (XB,  y) in enumerate(test_loader):\n",
    "    if model.header == 'CNN':\n",
    "        x = XI\n",
    "    else:\n",
    "        x = XB\n",
    "    # x = x[:, :, 1::2]\n",
    "    x, y = x.to(device), y.long().to(device)\n",
    "\n",
    "    if x.size(0) != batch_size:\n",
    "        print(\" test batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "        break\n",
    "    test_num += x.size(0)\n",
    "    if block == \"phased_LSTM\":\n",
    "        x_decoded, latent, output = model(x, times)\n",
    "    else:\n",
    "        x_decoded, latent, output_texture, output_thickness = model(x)\n",
    "\n",
    "    # compute classification acc\n",
    "    thick_y = property_label(y)\n",
    "    pred_texture = output_texture.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct_texture += pred_texture.eq(y.data.view_as(pred_texture)).long().cpu().sum().item()\n",
    "    pred_thickness = output_thickness.data.max(1, keepdim=True)[1]\n",
    "    correct_thickness += pred_thickness.eq(thick_y.data.view_as(pred_thickness)).long().cpu().sum().item()\n",
    "\n",
    "test_acc1 = correct_texture / test_num #len(test_loader.dataset)\n",
    "test_thickness_acc1 = correct_thickness / test_num\n",
    "print('last epoch Test accuracy for', str(kfold_number), ' fold : ', test_acc1, test_thickness_acc1)\n",
    "\n",
    "# # TEST at the best model\n",
    "# TODO\n",
    "# correct_texture = 0\n",
    "# correct_thickness = 0\n",
    "# test_num = 0\n",
    "# saved_dicts = torch.load('models_and_stats/'+model_name+str(max_val_epoch)+'.pt', map_location='cpu')\n",
    "# model.load_state_dict(saved_dicts['model_state_dict'])\n",
    "\n",
    "# for i, (XB,  y) in enumerate(test_loader):\n",
    "#     if model.header == 'CNN':\n",
    "#         x = XI\n",
    "#     else:\n",
    "#         x = XB\n",
    "#     # x = x[:, :, 1::2]\n",
    "#     x, y = x.to(device), y.long().to(device)\n",
    "\n",
    "#     if x.size(0) != batch_size:\n",
    "#         print(\" test batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "#         break\n",
    "#     test_num += x.size(0)\n",
    "#     if block == \"phased_LSTM\":\n",
    "#         x_decoded, latent, output = model(x, times)\n",
    "#     else:\n",
    "#         x_decoded, latent, output_texture, output_thickness = model(x)\n",
    "\n",
    "#     # compute classification acc\n",
    "#     thick_y = property_label(y)\n",
    "#     pred_texture = output_texture.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "#     correct_texture += pred_texture.eq(y.data.view_as(pred_texture)).long().cpu().sum().item()\n",
    "#     pred_thickness = output_thickness.data.max(1, keepdim=True)[1]\n",
    "#     correct_thickness += pred_thickness.eq(thick_y.data.view_as(pred_thickness)).long().cpu().sum().item()\n",
    "\n",
    "# test_acc2 = correct_texture / test_num #len(test_loader.dataset)\n",
    "# test_thickness_acc2 = correct_thickness / test_num\n",
    "# print('at the best model Test accuracy for', str(kfold_number), ' fold : ', test_acc2, test_thickness_acc2)\n",
    "\n",
    "# Save stats\n",
    "# TODO\n",
    "# results_dict = {\"epoch_train_loss\": epoch_train_loss,\n",
    "#                 \"epoch_train_acc\": epoch_train_acc,\n",
    "#                 \"epoch_train_thickness_acc\": epoch_train_thickness_acc,\n",
    "#                 \"epoch_val_loss\": epoch_val_loss,\n",
    "#                 \"epoch_val_acc\": epoch_val_acc,\n",
    "#                 \"epoch_val_thickness_acc\": epoch_val_thickness_acc,\n",
    "#                 \"test_acc1\": test_acc1,\n",
    "#                 \"test_acc2\": test_acc2,\n",
    "#                 \"test_thickness_acc1\": test_thickness_acc1,\n",
    "#                 \"test_thickness_acc2\": test_thickness_acc2}\n",
    "\n",
    "# dict_name = model_name + '_stats_fold{}_{}.pkl'.format(str(kfold_number), args.rep)\n",
    "# pickle.dump(results_dict, open(logDir + dict_name, 'wb'))\n",
    "# print(\"dump results dict to {}\".format(dict_name))\n",
    "\n",
    "# assert n_epochs == len(epoch_train_acc), \"different epoch length {} {}\".format(n_epochs, len(epoch_train_acc))\n",
    "# fig, ax = plt.subplots(figsize=(15, 7))\n",
    "# ax.plot(np.arange(n_epochs), epoch_train_acc, label=\"train acc\")\n",
    "# ax.set_xlabel('epoch')\n",
    "# ax.set_ylabel('acc')\n",
    "# ax.grid(True)\n",
    "# plt.legend(loc='upper right')\n",
    "# figname = logDir + model_name +\"_train_acc.png\"\n",
    "# if if_plot:\n",
    "#     plt.show()\n",
    "\n",
    "# TODO plot_stats(logDir + dict_name, model_name)\n",
    "\n",
    "\n",
    "# for ds in ['c20_2', 'c20new', 'c50']:\n",
    "#     args.dataset = ds\n",
    "#     print(args)\n",
    "#     train(args)\n",
    "    # 87.5 95.83, 84.17 v2-32\n",
    "\n",
    "    # 0.932, 95.31, 82.08\n",
    "\n",
    "    # for r_on in [0.1, 0.2, 0.3, 0.4]:\n",
    "    #     args.r_on = r_on\n",
    "    #     print(args)\n",
    "    #     train(args)\n",
    "\n",
    "    # for p_max in [75.0, 60.0, 45.0, 30.0]:\n",
    "    #     args.p_max = p_max\n",
    "    #     print(args)\n",
    "    #     train(args) \n",
    "\n",
    "    # for hidden_size in [60, 70, 80, 90, 100]:\n",
    "    #     args.h_s = hidden_size\n",
    "    #     print(args)\n",
    "    #     train(args)\n",
    "\n",
    "    # for w_r in [0, 0.0005, 0.001, 0.005, 0.01]:\n",
    "    #     args.w_r = w_r\n",
    "    #     print(args)\n",
    "    #     train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chop org data of length 400 into 1 segments, each of which is has length 400\n",
      "196\n",
      "391\n",
      " test batch 2 size 100 < 32, skip\n",
      "last epoch Test accuracy for 0  fold :  0.925 0.9775\n",
      "0.9858356940509916\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "train_loader, val_loader, test_loader = get_TrainValTestLoader(dataset_dir, k=0, batch_size=200,shuffle=shuffle, prefix=prefix,seq_len=sequence_length)\n",
    "correct_texture = 0\n",
    "correct_thickness = 0\n",
    "test_num = 0\n",
    "for i, (XB,  y) in enumerate(test_loader):\n",
    "    if model.header == 'CNN':\n",
    "        x = XI\n",
    "    else:\n",
    "        x = XB\n",
    "    # x = x[:, :, 1::2]\n",
    "    x, y = x.to(device), y.long().to(device)\n",
    "\n",
    "    if x.size(0) != 200:\n",
    "        print(\" test batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "        break\n",
    "    test_num += x.size(0)\n",
    "    if block == \"phased_LSTM\":\n",
    "        x_decoded, latent, output = model(x, times)\n",
    "    else:\n",
    "        x_decoded, latent, output_texture, output_thickness = model(x)\n",
    "\n",
    "    # compute classification acc\n",
    "    thick_y = property_label(y)\n",
    "    pred_texture = output_texture.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct_texture += pred_texture.eq(y.data.view_as(pred_texture)).long().cpu().sum().item()\n",
    "    pred_thickness = output_thickness.data.max(1, keepdim=True)[1]\n",
    "    correct_thickness += pred_thickness.eq(thick_y.data.view_as(pred_thickness)).long().cpu().sum().item()\n",
    "    print(correct_thickness)\n",
    "\n",
    "test_acc1 = correct_texture / test_num #len(test_loader.dataset)\n",
    "test_thickness_acc1 = correct_thickness / test_num\n",
    "print('last epoch Test accuracy for', str(kfold_number), ' fold : ', test_acc1, test_thickness_acc1)\n",
    "f1 = f1_score(thick_y, pred_thickness)\n",
    "print(f1)\n",
    "cf_matrix = confusion_matrix(thick_y, pred_thickness)\n",
    "sns_plot = sns.heatmap(cf_matrix, cmap='Blues', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'thick_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d028546ce7c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvrae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix_plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthick_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_thickness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconfusion_matrix_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthick_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_thickness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c20_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'thick_y' is not defined"
     ]
    }
   ],
   "source": [
    "from vrae.visual import confusion_matrix_plot\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(thick_y, pred_thickness)\n",
    "print(f1)\n",
    "confusion_matrix_plot(thick_y, pred_thickness, 'c20_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AE]",
   "language": "python",
   "name": "conda-env-AE-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
