{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "'''\n",
    "sample command: python T4_BT19_ae.py -k 0 -c 0 -r 1 --data_dir /home/ruihan/data\n",
    "Individual training for BioTac data (full/partial data)\n",
    "if -r=1, train with full data\n",
    "if -r=2, train with half data\n",
    "loss = classification loss + recon loss \n",
    "'''\n",
    "\n",
    "# Import\n",
    "import os,sys\n",
    "import pickle\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vrae.vrae import VRAEC\n",
    "from preprocess_data import get_TrainValTestLoader, get_TrainValTestDataset, get_TrainValTestData\n",
    "from vrae.visual import plot_grad_flow\n",
    "\n",
    "# # Parse argument\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-i\", \"--rep\", type=int, default=0, help='index of running repetition')\n",
    "# parser.add_argument('--data_dir', type=str, default='data', help=\"DIR set in 'gh_download.sh' to store compiled_data\")\n",
    "# parser.add_argument(\"-k\", \"--kfold\", type=int, default=0, help=\"kfold_number for loading data\")\n",
    "# parser.add_argument(\"-r\", \"--reduction\", type=int, default=1, help=\"data reduction ratio for partial training\")\n",
    "# parser.add_argument(\"-c\", \"--cuda\", default=0, help=\"index of cuda gpu to use\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# dummy class to replace argparser, if running jupyter notebook\n",
    "class Args:\n",
    "    reduction = 0\n",
    "    data_dir = 'data'\n",
    "    kfold = 0\n",
    "    cuda = '0'\n",
    "    reduction = 1\n",
    "\n",
    "args=Args()\n",
    "\n",
    "# Set hyper params\n",
    "args_data_dir = args.data_dir\n",
    "kfold_number = args.kfold\n",
    "data_reduction_ratio = args.reduction\n",
    "shuffle = True # set to False for partial training\n",
    "num_class = 20\n",
    "sequence_length = 400\n",
    "number_of_features = 19\n",
    "\n",
    "hidden_size = 90\n",
    "hidden_layer_depth = 1\n",
    "latent_length = 40\n",
    "batch_size = 32\n",
    "learning_rate = 0.001 # 0.0005\n",
    "n_epochs = 100\n",
    "dropout_rate = 0.2\n",
    "cuda = True # options: True, False\n",
    "header = None\n",
    "\n",
    "# loss weightage\n",
    "w_r = 0.01\n",
    "w_c = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Load data\n",
    "data_dir = os.path.join(args_data_dir, \"compiled_data/\")\n",
    "logDir = 'models_and_stats/'\n",
    "if_plot = False\n",
    "\n",
    "# RNN block\n",
    "block = \"phased_LSTM\" # LSTM, GRU, phased_LSTM\n",
    "\n",
    "# model_name = 'BT19_ae_{}_wrI_{}_wC_{}_{}'.format(data_reduction_ratio, w_r, w_c, str(kfold_number))\n",
    "model_name = \"model_\"+block+\"_B\"\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:{}\".format(args.cuda))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "if args.reduction != 1:\n",
    "    print(\"load {} kfold number, reduce data to {} folds, put to device: {}\".format(args.kfold, args.reduction, device))\n",
    "else:\n",
    "    print(\"load {} kfold number, train with full data, put to devide: {}\".format(args.kfold, device))\n",
    "\n",
    "prefix = \"\"\n",
    "dataset_dir = os.path.join(args_data_dir, \"c20/\") # TODO\n",
    "train_set, val_set, test_set = get_TrainValTestDataset(dataset_dir, k=0, prefix=prefix, seq_len=sequence_length)\n",
    "train_loader, val_loader, test_loader = get_TrainValTestLoader(dataset_dir, k=0, batch_size=batch_size,shuffle=shuffle, prefix=prefix,seq_len=sequence_length)\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = get_TrainValTestData(dataset_dir, k=0, prefix=prefix,seq_len=sequence_length)\n",
    "# Initialize models\n",
    "model = VRAEC(num_class=num_class,\n",
    "            block=block,\n",
    "            sequence_length=sequence_length,\n",
    "            number_of_features = number_of_features,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate,\n",
    "            cuda = cuda,\n",
    "            model_name=model_name,\n",
    "            header=header,\n",
    "            device = device)\n",
    "model.to(device)\n",
    "\n",
    "# Initialize training settings\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "cl_loss_fn = nn.NLLLoss()\n",
    "recon_loss_fn = nn.MSELoss()\n",
    "\n",
    "# model.load_state_dict(torch.load('models_and_stats/model_phased_LSTM_B30.pt', map_location='cpu'))\n",
    "saved_dicts = torch.load('models_and_stats/model_phased_LSTM_B32.pt', map_location='cpu')\n",
    "model.load_state_dict(saved_dicts['model_state_dict'])\n",
    "optimizer.load_state_dict(saved_dicts['optimizer_state_dict'])\n",
    "\n",
    "training_start=datetime.now()\n",
    "# create empty lists to fill stats later\n",
    "epoch_train_loss = []\n",
    "epoch_train_acc = []\n",
    "epoch_val_loss = []\n",
    "epoch_val_acc = []\n",
    "max_val_acc = 0\n",
    "\n",
    "if block == \"phased_LSTM\":\n",
    "    times = torch.ones(batch_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B33 nan grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    if epoch < 33:\n",
    "        continue\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    train_loss = 0\n",
    "    train_num = 0\n",
    "    for i, (XB,  y) in enumerate(train_loader):\n",
    "        if model.header == 'CNN':\n",
    "            x = XI\n",
    "        else:\n",
    "            x = XB\n",
    "        x, y = x.to(device), y.long().to(device)\n",
    "        if x.size()[0] != batch_size:\n",
    "            break\n",
    "        \n",
    "        # reduce data by data_reduction_ratio times\n",
    "        if i % data_reduction_ratio == 0:\n",
    "            train_num += x.size(0)\n",
    "            optimizer.zero_grad()\n",
    "            if block == \"phased_LSTM\":\n",
    "                x_decoded, latent, output = model(x, times)\n",
    "            else:\n",
    "                x_decoded, latent, output = model(x)\n",
    "\n",
    "            # assert not torch.isnan(y).any(), \"batch_num=\"+str(i)\n",
    "            # print((output == 0).nonzero().size(0)==0)\n",
    "\n",
    "            assert (output == 0).nonzero().size(0)==0, 'output contain zero, batch_num'+str(i)+' indices:'+str((output == 0).nonzero())\n",
    "            \n",
    "            cl_loss = cl_loss_fn(output, y)\n",
    "            recon_loss = recon_loss_fn(x_decoded, x)\n",
    "            loss = w_c*cl_loss + w_r *recon_loss\n",
    "            \n",
    "            # compute classification acc\n",
    "            pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "            # accumulator\n",
    "            train_loss += loss.item()\n",
    "            start_bp = datetime.now()\n",
    "            loss.backward()\n",
    "            figname = logDir + model_name + \"grad_flow_plot_epoch\" +str(epoch)+\".png\"\n",
    "            if i == 0: # and epoch%50 == 0:\n",
    "                for n, p in model.named_parameters():\n",
    "                    if p.requires_grad:\n",
    "                        print(n,p.grad)\n",
    "                print(\"grad flow for epoch {}\".format(epoch))\n",
    "                plot_grad_flow(model.named_parameters(), figname, if_plot=False)\n",
    "            optimizer.step()\n",
    "            # print('1 batch bp time:', datetime.now()-start_bp)\n",
    "\n",
    "    # if epoch == 0:\n",
    "    #     print('first epoch training time:', datetime.now()-training_start)\n",
    "    \n",
    "    # if epoch < 20 or epoch%200 == 0:\n",
    "    # print(\"train last batch {} of {}: cl_loss {:.3f} recon_loss {:.3f}\".format(i, len(train_loader), cl_loss, recon_loss))\n",
    "\n",
    "    # fill stats\n",
    "    train_accuracy = correct / train_num \n",
    "    train_loss /= train_num\n",
    "    epoch_train_loss.append(train_loss)\n",
    "    epoch_train_acc.append(train_accuracy) \n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "    val_num = 0\n",
    "    for i, (XB, y) in enumerate(val_loader):\n",
    "        if model.header == 'CNN':\n",
    "            x = XI\n",
    "        else:\n",
    "            x = XB\n",
    "        x, y = x.to(device), y.long().to(device)\n",
    "        if x.size()[0] != batch_size:\n",
    "            break\n",
    "        val_num += x.size(0)\n",
    "        if block == \"phased_LSTM\":\n",
    "            x_decoded, latent, output = model(x, times)\n",
    "        else:\n",
    "            x_decoded, latent, output = model(x)\n",
    "\n",
    "        # construct loss function\n",
    "        cl_loss = cl_loss_fn(output, y)\n",
    "        recon_loss = recon_loss_fn(x_decoded, x)\n",
    "        loss = w_c*cl_loss + w_r *recon_loss\n",
    "        \n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        # accumulator\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    # fill stats\n",
    "    val_accuracy = correct / val_num\n",
    "    val_loss /= val_num\n",
    "    epoch_val_loss.append(val_loss)  # only save the last batch\n",
    "    epoch_val_acc.append(val_accuracy)\n",
    "    \n",
    "    # if epoch < 20 or epoch%200 == 0:\n",
    "    print(\"train_num {}, val_num {}\".format(train_num, val_num))\n",
    "    print('Epoch: {} Loss: train {:.3f}, valid {:.3f}. Accuracy: train: {:.3f}, valid {:.3f}'.format(epoch, train_loss, val_loss, train_accuracy, val_accuracy))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose model\n",
    "    # if max_val_acc <= val_accuracy:\n",
    "    #     model_dir = logDir + model_name + str(epoch) + '.pt'\n",
    "    #     print('Saving model at {} epoch to {}'.format(epoch, model_dir))\n",
    "    #     max_val_acc = val_accuracy\n",
    "    #     torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, model_dir)\n",
    "    model_dir = logDir + model_name + str(epoch) + '.pt'\n",
    "    print('Saving model at {} epoch to {}'.format(epoch, model_dir))\n",
    "    max_val_acc = val_accuracy\n",
    "    torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, model_dir)\n",
    "\n",
    "training_end =  datetime.now()\n",
    "training_time = training_end -training_start \n",
    "print(\"training takes time {}\".format(training_time))\n",
    "\n",
    "model.is_fitted = True\n",
    "model.eval()\n",
    "\n",
    "# TEST\n",
    "correct = 0\n",
    "test_num = 0\n",
    "for i, (XB,  y) in enumerate(test_loader):\n",
    "    if model.header == 'CNN':\n",
    "        x = XI\n",
    "    else:\n",
    "        x = XB\n",
    "    x, y = x.to(device), y.long().to(device)\n",
    "    \n",
    "    if x.size(0) != batch_size:\n",
    "        print(\" test batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "        break\n",
    "    test_num += x.size(0)\n",
    "    if block == \"phased_LSTM\":\n",
    "        x_decoded, latent, output = model(x, times)\n",
    "    else:\n",
    "        x_decoded, latent, output = model(x)\n",
    "\n",
    "    # compute classification acc\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "    \n",
    "test_acc = correct / test_num #len(test_loader.dataset)\n",
    "print('Test accuracy for', str(kfold_number), ' fold : ', test_acc)\n",
    "\n",
    "# Save stats\n",
    "results_dict = {\"epoch_train_loss\": epoch_train_loss,\n",
    "                \"epoch_train_acc\": epoch_train_acc,\n",
    "                \"epoch_val_loss\": epoch_val_loss,\n",
    "                \"epoch_val_acc\": epoch_val_acc,\n",
    "                \"test_acc\": test_acc}\n",
    "\n",
    "dict_name = model_name + '_stats_fold{}_{}.pkl'.format(str(kfold_number), args.rep)\n",
    "pickle.dump(results_dict, open(logDir + dict_name, 'wb'))\n",
    "print(\"dump results dict to {}\".format(dict_name))\n",
    "\n",
    "assert n_epochs == len(epoch_train_acc), \"different epoch length {} {}\".format(n_epochs, len(epoch_train_acc))\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "ax.plot(np.arange(n_epochs), epoch_train_acc, label=\"train acc\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('acc')\n",
    "ax.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "figname = logDir + model_name +\"_train_acc.png\"\n",
    "if if_plot:\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AE]",
   "language": "python",
   "name": "conda-env-AE-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
