{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "'''\n",
    "sample command: python T4_BT19_ae.py -k 0 -c 0 -r 1 --data_dir /home/ruihan/data\n",
    "Individual training for BioTac data (full/partial data)\n",
    "if -r=1, train with full data\n",
    "if -r=2, train with half data\n",
    "loss = classification loss + recon loss \n",
    "'''\n",
    "\n",
    "# Import\n",
    "import os,sys\n",
    "import pickle\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from vrae.vrae import VRAEC_v2\n",
    "from preprocess_data_copy import get_TrainValTestLoader, get_TrainValTestDataset, get_TrainValTestData, property_label\n",
    "from vrae.visual import plot_grad_flow, plot_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parse argument\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-i\", \"--rep\", type=int, default=0, help='index of running repetition')\n",
    "# parser.add_argument('--data_dir', type=str, default='data', help=\"DIR set in 'gh_download.sh' to store compiled_data\")\n",
    "# parser.add_argument(\"-k\", \"--kfold\", type=int, default=0, help=\"kfold_number for loading data\")\n",
    "# parser.add_argument(\"-r\", \"--reduction\", type=int, default=1, help=\"data reduction ratio for partial training\")\n",
    "# parser.add_argument(\"-c\", \"--cuda\", default=0, help=\"index of cuda gpu to use\")\n",
    "# parser.add_argument(\"--r_on\", default=0.1, help=\"ratio_on for phased lstm\")\n",
    "# parser.add_argument(\"--p_max\", default=200, help=\"period_init_max for phased lstm\")\n",
    "# parser.add_argument(\"--w_r\", default=0.01, type=float, help=\"weight of recon loss\")\n",
    "# parser.add_argument(\"--h_s\", default=90, type=int, help=\"hidden size of rnn layers\")\n",
    "# parser.add_argument(\"--dataset\", default='c20', type=str, help=\"name of dataset\")\n",
    "# parser.add_argument(\"--thickness_lat\", default=10, type=int, help=\"size for the thickness latent space\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# dummy class to replace argparser, if running jupyter notebook\n",
    "class Args:\n",
    "    rep = 0\n",
    "    data_dir = 'data'\n",
    "    kfold = 0\n",
    "    cuda = '0'\n",
    "    reduction = 1\n",
    "    r_on = 0.1\n",
    "    p_max = 200\n",
    "    w_r = 0.01\n",
    "    h_s = 90\n",
    "    dataset = 'c20_2'\n",
    "    thickness_lat = 10\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 0 kfold number, train with full data, put to device: cpu\n",
      "chop org data of length 400 into 1 segments, each of which is has length 400\n",
      "chop org data of length 400 into 1 segments, each of which is has length 400\n",
      "chop org data of length 400 into 1 segments, each of which is has length 400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VRAE(n_epochs=100,batch_size=32,cuda=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set hyper params\n",
    "args_data_dir = args.data_dir\n",
    "kfold_number = args.kfold\n",
    "data_reduction_ratio = args.reduction\n",
    "shuffle = True # set to False for partial training\n",
    "sequence_length = 400\n",
    "number_of_features = 19\n",
    "\n",
    "hidden_size = args.h_s\n",
    "hidden_layer_depth = 1\n",
    "latent_length = 40\n",
    "batch_size = 32\n",
    "learning_rate = 0.001 # 0.0005\n",
    "n_epochs = 100\n",
    "dropout_rate = 0.2\n",
    "cuda = True # options: True, False\n",
    "header = None\n",
    "dataset = args.dataset\n",
    "if dataset == 'c50':\n",
    "    num_class_texture = 50\n",
    "else:\n",
    "    num_class_texture = 20\n",
    "\n",
    "thickness_latent_length = args.thickness_lat\n",
    "\n",
    "# loss weightage\n",
    "w_r = args.w_r\n",
    "w_c = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Load data\n",
    "# data_dir = os.path.join(args_data_dir, \"compiled_data/\")\n",
    "logDir = 'models_and_stats/'\n",
    "if_plot = False\n",
    "\n",
    "# RNN block\n",
    "block = \"LSTM\" # LSTM, GRU, phased_LSTM\n",
    "\n",
    "model_name = 'B_block_{}_data_{}_wrI_{}_wC_{}_hidden_{}_latent_{}_r_on_{}_p_max_{}'.format(block, dataset, w_r, w_c, str(hidden_size), str(latent_length), str(args.r_on), str(args.p_max))\n",
    "\n",
    "if torch.cuda.is_available() and cuda:\n",
    "    device = torch.device(\"cuda:{}\".format(args.cuda))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "if args.reduction != 1:\n",
    "    print(\"load {} kfold number, reduce data to {} folds, put to device: {}\".format(args.kfold, args.reduction, device))\n",
    "else:\n",
    "    print(\"load {} kfold number, train with full data, put to device: {}\".format(args.kfold, device))\n",
    "\n",
    "prefix = \"\"\n",
    "dataset_dir = os.path.join(args_data_dir, dataset+\"/\") # TODO\n",
    "train_set, val_set, test_set = get_TrainValTestDataset(dataset_dir, k=0, prefix=prefix, seq_len=sequence_length)\n",
    "train_loader, val_loader, test_loader = get_TrainValTestLoader(dataset_dir, k=0, batch_size=batch_size,shuffle=shuffle, prefix=prefix,seq_len=sequence_length)\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = get_TrainValTestData(dataset_dir, k=0, prefix=prefix,seq_len=sequence_length)\n",
    "# Initialize models\n",
    "model = VRAEC_v2(num_class_texture=num_class_texture,\n",
    "            block=block,\n",
    "            sequence_length=sequence_length, # TODO\n",
    "            number_of_features = number_of_features,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            thickness_latent_length = thickness_latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate,\n",
    "            cuda = cuda,\n",
    "            model_name=model_name,\n",
    "            header=header,\n",
    "            device = device,\n",
    "            ratio_on=args.r_on, period_init_max=args.p_max)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc\n",
      "encoder.fc.weight False\n",
      "enc\n",
      "encoder.fc.bias False\n",
      "enc\n",
      "encoder.model.weight_ih_l0 False\n",
      "enc\n",
      "encoder.model.weight_hh_l0 False\n",
      "enc\n",
      "encoder.model.bias_ih_l0 False\n",
      "enc\n",
      "encoder.model.bias_hh_l0 False\n",
      "lmb\n",
      "lmbd.hidden_to_mean.weight False\n",
      "lmb\n",
      "lmbd.hidden_to_mean.bias False\n",
      "cla\n",
      "classifier_texture.0.weight False\n",
      "cla\n",
      "classifier_texture.0.bias False\n",
      "lat_to_lat.hidden_to_mean.weight True\n",
      "lat_to_lat.hidden_to_mean.bias True\n",
      "cla\n",
      "classifier_thickness.0.weight False\n",
      "cla\n",
      "classifier_thickness.0.bias False\n",
      "dec\n",
      "decoder.latent_to_hidden.weight False\n",
      "dec\n",
      "decoder.latent_to_hidden.bias False\n",
      "dec\n",
      "decoder.model.weight_ih_l0 False\n",
      "dec\n",
      "decoder.model.weight_hh_l0 False\n",
      "dec\n",
      "decoder.model.bias_ih_l0 False\n",
      "dec\n",
      "decoder.model.bias_hh_l0 False\n",
      "dec\n",
      "decoder.hidden_to_output.weight False\n",
      "dec\n",
      "decoder.hidden_to_output.bias False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name[:3] != 'lat':\n",
    "        param.requires_grad = False\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['encoder.fc.weight', 'encoder.fc.bias', 'encoder.model.lstm.weight_ih', 'encoder.model.lstm.weight_hh', 'encoder.model.lstm.bias_ih', 'encoder.model.lstm.bias_hh', 'encoder.model.phased_cell.tau', 'encoder.model.phased_cell.phase', 'lmbd.hidden_to_mean.weight', 'lmbd.hidden_to_mean.bias', 'classifier.0.weight', 'classifier.0.bias', 'decoder.latent_to_hidden.weight', 'decoder.latent_to_hidden.bias', 'decoder.model.lstm.weight_ih', 'decoder.model.lstm.weight_hh', 'decoder.model.lstm.bias_ih', 'decoder.model.lstm.bias_hh', 'decoder.model.phased_cell.tau', 'decoder.model.phased_cell.phase', 'decoder.hidden_to_output.weight', 'decoder.hidden_to_output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(saved_dicts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl_loss: tensor(0.0084) cl_loss_thickness: tensor(0.6087, grad_fn=<NllLossBackward>) recon_loss: tensor(20.9798)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 0 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.708, valid 0.745\n",
      "cl_loss: tensor(0.0170) cl_loss_thickness: tensor(0.4617, grad_fn=<NllLossBackward>) recon_loss: tensor(20.1539)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 1 Loss: train 0.021, valid 0.016. Accuracy: train: 1.000, valid 0.938. Thickness Accuracy: train: 0.781, valid 0.781\n",
      "cl_loss: tensor(0.0156) cl_loss_thickness: tensor(0.3876, grad_fn=<NllLossBackward>) recon_loss: tensor(602.7182)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 2 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.816, valid 0.797\n",
      "cl_loss: tensor(0.0100) cl_loss_thickness: tensor(0.3313, grad_fn=<NllLossBackward>) recon_loss: tensor(23.7650)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 3 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.851, valid 0.849\n",
      "cl_loss: tensor(0.0125) cl_loss_thickness: tensor(0.2466, grad_fn=<NllLossBackward>) recon_loss: tensor(13.9718)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 4 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.872, valid 0.849\n",
      "cl_loss: tensor(0.0157) cl_loss_thickness: tensor(0.2708, grad_fn=<NllLossBackward>) recon_loss: tensor(19.0720)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 5 Loss: train 0.021, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.898, valid 0.849\n",
      "cl_loss: tensor(0.0077) cl_loss_thickness: tensor(0.3603, grad_fn=<NllLossBackward>) recon_loss: tensor(31.1990)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 6 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.903, valid 0.859\n",
      "cl_loss: tensor(0.0171) cl_loss_thickness: tensor(0.2389, grad_fn=<NllLossBackward>) recon_loss: tensor(24.5548)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 7 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.917, valid 0.870\n",
      "cl_loss: tensor(0.0124) cl_loss_thickness: tensor(0.2257, grad_fn=<NllLossBackward>) recon_loss: tensor(194.3083)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 8 Loss: train 0.021, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.922, valid 0.870\n",
      "cl_loss: tensor(0.0173) cl_loss_thickness: tensor(0.2524, grad_fn=<NllLossBackward>) recon_loss: tensor(23.9091)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 9 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.931, valid 0.865\n",
      "cl_loss: tensor(0.0161) cl_loss_thickness: tensor(0.2578, grad_fn=<NllLossBackward>) recon_loss: tensor(25.9570)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 10 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.936, valid 0.865\n",
      "cl_loss: tensor(0.0186) cl_loss_thickness: tensor(0.2122, grad_fn=<NllLossBackward>) recon_loss: tensor(34.1072)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 11 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.938, valid 0.865\n",
      "cl_loss: tensor(0.0110) cl_loss_thickness: tensor(0.1927, grad_fn=<NllLossBackward>) recon_loss: tensor(20.2823)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 12 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.938. Thickness Accuracy: train: 0.939, valid 0.901\n",
      "cl_loss: tensor(0.0035) cl_loss_thickness: tensor(0.1049, grad_fn=<NllLossBackward>) recon_loss: tensor(24.3183)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 13 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.946, valid 0.896\n",
      "cl_loss: tensor(0.0150) cl_loss_thickness: tensor(0.1947, grad_fn=<NllLossBackward>) recon_loss: tensor(18.8175)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 14 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.951, valid 0.901\n",
      "cl_loss: tensor(0.0154) cl_loss_thickness: tensor(0.2595, grad_fn=<NllLossBackward>) recon_loss: tensor(14.9452)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 15 Loss: train 0.011, valid 0.018. Accuracy: train: 1.000, valid 0.938. Thickness Accuracy: train: 0.948, valid 0.901\n",
      "cl_loss: tensor(0.0180) cl_loss_thickness: tensor(0.1369, grad_fn=<NllLossBackward>) recon_loss: tensor(24.1491)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 16 Loss: train 0.021, valid 0.016. Accuracy: train: 1.000, valid 0.938. Thickness Accuracy: train: 0.948, valid 0.917\n",
      "cl_loss: tensor(0.0177) cl_loss_thickness: tensor(0.1322, grad_fn=<NllLossBackward>) recon_loss: tensor(20.0203)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 17 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.938. Thickness Accuracy: train: 0.953, valid 0.906\n",
      "cl_loss: tensor(0.0200) cl_loss_thickness: tensor(0.1414, grad_fn=<NllLossBackward>) recon_loss: tensor(597.7419)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 18 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.958, valid 0.911\n",
      "cl_loss: tensor(0.0174) cl_loss_thickness: tensor(0.1430, grad_fn=<NllLossBackward>) recon_loss: tensor(21.8278)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 19 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.955, valid 0.911\n",
      "cl_loss: tensor(0.0109) cl_loss_thickness: tensor(0.1291, grad_fn=<NllLossBackward>) recon_loss: tensor(19.9463)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 20 Loss: train 0.020, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.964, valid 0.911\n",
      "cl_loss: tensor(0.0085) cl_loss_thickness: tensor(0.1073, grad_fn=<NllLossBackward>) recon_loss: tensor(19.1200)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 21 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.938. Thickness Accuracy: train: 0.962, valid 0.922\n",
      "cl_loss: tensor(0.0087) cl_loss_thickness: tensor(0.1691, grad_fn=<NllLossBackward>) recon_loss: tensor(26.0277)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 22 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.965, valid 0.922\n",
      "cl_loss: tensor(0.0055) cl_loss_thickness: tensor(0.1589, grad_fn=<NllLossBackward>) recon_loss: tensor(17.7932)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 23 Loss: train 0.021, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.969, valid 0.932\n",
      "cl_loss: tensor(0.0221) cl_loss_thickness: tensor(0.1575, grad_fn=<NllLossBackward>) recon_loss: tensor(21.2095)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 24 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.967, valid 0.927\n",
      "cl_loss: tensor(0.0071) cl_loss_thickness: tensor(0.2191, grad_fn=<NllLossBackward>) recon_loss: tensor(13.8669)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 25 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.969, valid 0.932\n",
      "cl_loss: tensor(0.0181) cl_loss_thickness: tensor(0.1372, grad_fn=<NllLossBackward>) recon_loss: tensor(21.7023)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 26 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.970, valid 0.922\n",
      "cl_loss: tensor(0.0178) cl_loss_thickness: tensor(0.0935, grad_fn=<NllLossBackward>) recon_loss: tensor(25.9586)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 27 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.969, valid 0.927\n",
      "cl_loss: tensor(0.0191) cl_loss_thickness: tensor(0.2241, grad_fn=<NllLossBackward>) recon_loss: tensor(30.8497)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 28 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.969, valid 0.927\n",
      "cl_loss: tensor(0.0169) cl_loss_thickness: tensor(0.1066, grad_fn=<NllLossBackward>) recon_loss: tensor(31.4306)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 29 Loss: train 0.018, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.969, valid 0.943\n",
      "cl_loss: tensor(0.0268) cl_loss_thickness: tensor(0.2414, grad_fn=<NllLossBackward>) recon_loss: tensor(23.7091)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 30 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.974, valid 0.943\n",
      "cl_loss: tensor(0.0279) cl_loss_thickness: tensor(0.2504, grad_fn=<NllLossBackward>) recon_loss: tensor(25.0069)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num 576, val_num 192\n",
      "Epoch: 31 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.974, valid 0.932\n",
      "cl_loss: tensor(0.0112) cl_loss_thickness: tensor(0.1116, grad_fn=<NllLossBackward>) recon_loss: tensor(27.2097)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 32 Loss: train 0.021, valid 0.016. Accuracy: train: 1.000, valid 0.938. Thickness Accuracy: train: 0.977, valid 0.943\n",
      "cl_loss: tensor(0.0218) cl_loss_thickness: tensor(0.1367, grad_fn=<NllLossBackward>) recon_loss: tensor(22.4791)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 33 Loss: train 0.011, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.979, valid 0.943\n",
      "cl_loss: tensor(0.0128) cl_loss_thickness: tensor(0.0505, grad_fn=<NllLossBackward>) recon_loss: tensor(606.2637)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 34 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.977, valid 0.943\n",
      "cl_loss: tensor(0.0153) cl_loss_thickness: tensor(0.1720, grad_fn=<NllLossBackward>) recon_loss: tensor(20.6270)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 35 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.988, valid 0.932\n",
      "cl_loss: tensor(0.0208) cl_loss_thickness: tensor(0.1943, grad_fn=<NllLossBackward>) recon_loss: tensor(20.5941)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 36 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.983, valid 0.938\n",
      "cl_loss: tensor(0.0066) cl_loss_thickness: tensor(0.0567, grad_fn=<NllLossBackward>) recon_loss: tensor(610.5125)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 37 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.983, valid 0.943\n",
      "cl_loss: tensor(0.0132) cl_loss_thickness: tensor(0.0786, grad_fn=<NllLossBackward>) recon_loss: tensor(29.1054)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 38 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.984, valid 0.943\n",
      "cl_loss: tensor(0.0176) cl_loss_thickness: tensor(0.0760, grad_fn=<NllLossBackward>) recon_loss: tensor(20.2395)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 39 Loss: train 0.021, valid 0.016. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.984, valid 0.938\n",
      "cl_loss: tensor(0.0101) cl_loss_thickness: tensor(0.0852, grad_fn=<NllLossBackward>) recon_loss: tensor(29.5556)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 40 Loss: train 0.021, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.983, valid 0.948\n",
      "cl_loss: tensor(0.0164) cl_loss_thickness: tensor(0.0615, grad_fn=<NllLossBackward>) recon_loss: tensor(27.1667)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 41 Loss: train 0.021, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.984, valid 0.938\n",
      "cl_loss: tensor(0.0158) cl_loss_thickness: tensor(0.1458, grad_fn=<NllLossBackward>) recon_loss: tensor(23.6078)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 42 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.938. Thickness Accuracy: train: 0.983, valid 0.943\n",
      "cl_loss: tensor(0.0084) cl_loss_thickness: tensor(0.0572, grad_fn=<NllLossBackward>) recon_loss: tensor(32.5500)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 43 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.983, valid 0.943\n",
      "cl_loss: tensor(0.0044) cl_loss_thickness: tensor(0.0694, grad_fn=<NllLossBackward>) recon_loss: tensor(26.1602)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 44 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.981, valid 0.938\n",
      "cl_loss: tensor(0.0219) cl_loss_thickness: tensor(0.0967, grad_fn=<NllLossBackward>) recon_loss: tensor(16.1314)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 45 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.981, valid 0.938\n",
      "cl_loss: tensor(0.0063) cl_loss_thickness: tensor(0.0847, grad_fn=<NllLossBackward>) recon_loss: tensor(26.5841)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 46 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.943. Thickness Accuracy: train: 0.983, valid 0.948\n",
      "cl_loss: tensor(0.0205) cl_loss_thickness: tensor(0.1453, grad_fn=<NllLossBackward>) recon_loss: tensor(18.7329)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 47 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.979, valid 0.943\n",
      "cl_loss: tensor(0.0052) cl_loss_thickness: tensor(0.0557, grad_fn=<NllLossBackward>) recon_loss: tensor(16.0697)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 48 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.981, valid 0.938\n",
      "cl_loss: tensor(0.0114) cl_loss_thickness: tensor(0.1008, grad_fn=<NllLossBackward>) recon_loss: tensor(31.5167)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 49 Loss: train 0.010, valid 0.017. Accuracy: train: 1.000, valid 0.943. Thickness Accuracy: train: 0.983, valid 0.948\n",
      "cl_loss: tensor(0.0098) cl_loss_thickness: tensor(0.0637, grad_fn=<NllLossBackward>) recon_loss: tensor(17.0874)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 50 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.981, valid 0.948\n",
      "cl_loss: tensor(0.0123) cl_loss_thickness: tensor(0.0556, grad_fn=<NllLossBackward>) recon_loss: tensor(25.3250)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 51 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.938. Thickness Accuracy: train: 0.981, valid 0.938\n",
      "cl_loss: tensor(0.0164) cl_loss_thickness: tensor(0.0697, grad_fn=<NllLossBackward>) recon_loss: tensor(17.0865)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 52 Loss: train 0.021, valid 0.016. Accuracy: train: 1.000, valid 0.938. Thickness Accuracy: train: 0.983, valid 0.948\n",
      "cl_loss: tensor(0.0128) cl_loss_thickness: tensor(0.0802, grad_fn=<NllLossBackward>) recon_loss: tensor(27.1512)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 53 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.984, valid 0.938\n",
      "cl_loss: tensor(0.0052) cl_loss_thickness: tensor(0.0877, grad_fn=<NllLossBackward>) recon_loss: tensor(38.5454)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 54 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.984, valid 0.948\n",
      "cl_loss: tensor(0.0172) cl_loss_thickness: tensor(0.0608, grad_fn=<NllLossBackward>) recon_loss: tensor(16.1316)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 55 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.983, valid 0.943\n",
      "cl_loss: tensor(0.0173) cl_loss_thickness: tensor(0.0412, grad_fn=<NllLossBackward>) recon_loss: tensor(20.7940)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 56 Loss: train 0.021, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.983, valid 0.938\n",
      "cl_loss: tensor(0.0288) cl_loss_thickness: tensor(0.0251, grad_fn=<NllLossBackward>) recon_loss: tensor(187.4065)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 57 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.983, valid 0.938\n",
      "cl_loss: tensor(0.0244) cl_loss_thickness: tensor(0.1700, grad_fn=<NllLossBackward>) recon_loss: tensor(23.2569)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 58 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.938. Thickness Accuracy: train: 0.983, valid 0.948\n",
      "cl_loss: tensor(0.0140) cl_loss_thickness: tensor(0.0589, grad_fn=<NllLossBackward>) recon_loss: tensor(16.2897)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 59 Loss: train 0.018, valid 0.017. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.983, valid 0.938\n",
      "cl_loss: tensor(0.0197) cl_loss_thickness: tensor(0.1866, grad_fn=<NllLossBackward>) recon_loss: tensor(16.8733)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 60 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.983, valid 0.948\n",
      "cl_loss: tensor(0.0089) cl_loss_thickness: tensor(0.0476, grad_fn=<NllLossBackward>) recon_loss: tensor(33.0224)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 61 Loss: train 0.021, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.984, valid 0.948\n",
      "cl_loss: tensor(0.0094) cl_loss_thickness: tensor(0.0572, grad_fn=<NllLossBackward>) recon_loss: tensor(24.1096)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num 576, val_num 192\n",
      "Epoch: 62 Loss: train 0.021, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.984, valid 0.948\n",
      "cl_loss: tensor(0.0070) cl_loss_thickness: tensor(0.0528, grad_fn=<NllLossBackward>) recon_loss: tensor(35.5258)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 63 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.984, valid 0.948\n",
      "cl_loss: tensor(0.0189) cl_loss_thickness: tensor(0.0257, grad_fn=<NllLossBackward>) recon_loss: tensor(16.6024)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 64 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.938. Thickness Accuracy: train: 0.983, valid 0.958\n",
      "cl_loss: tensor(0.0262) cl_loss_thickness: tensor(0.0675, grad_fn=<NllLossBackward>) recon_loss: tensor(21.0495)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 65 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.983, valid 0.958\n",
      "cl_loss: tensor(0.0109) cl_loss_thickness: tensor(0.0948, grad_fn=<NllLossBackward>) recon_loss: tensor(21.5008)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 66 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.984, valid 0.948\n",
      "cl_loss: tensor(0.0105) cl_loss_thickness: tensor(0.0850, grad_fn=<NllLossBackward>) recon_loss: tensor(195.4977)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 67 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.984, valid 0.948\n",
      "cl_loss: tensor(0.0125) cl_loss_thickness: tensor(0.0178, grad_fn=<NllLossBackward>) recon_loss: tensor(23.8807)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 68 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.983, valid 0.953\n",
      "cl_loss: tensor(0.0060) cl_loss_thickness: tensor(0.0429, grad_fn=<NllLossBackward>) recon_loss: tensor(17.0111)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 69 Loss: train 0.021, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.983, valid 0.953\n",
      "cl_loss: tensor(0.0101) cl_loss_thickness: tensor(0.0286, grad_fn=<NllLossBackward>) recon_loss: tensor(17.4813)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 70 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.986, valid 0.958\n",
      "cl_loss: tensor(0.0074) cl_loss_thickness: tensor(0.0439, grad_fn=<NllLossBackward>) recon_loss: tensor(30.0453)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 71 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.988, valid 0.953\n",
      "cl_loss: tensor(0.0158) cl_loss_thickness: tensor(0.0655, grad_fn=<NllLossBackward>) recon_loss: tensor(24.7228)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 72 Loss: train 0.021, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.984, valid 0.958\n",
      "cl_loss: tensor(0.0125) cl_loss_thickness: tensor(0.0479, grad_fn=<NllLossBackward>) recon_loss: tensor(17.5464)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 73 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.988, valid 0.948\n",
      "cl_loss: tensor(0.0059) cl_loss_thickness: tensor(0.0361, grad_fn=<NllLossBackward>) recon_loss: tensor(30.7346)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 74 Loss: train 0.021, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.986, valid 0.943\n",
      "cl_loss: tensor(0.0111) cl_loss_thickness: tensor(0.0593, grad_fn=<NllLossBackward>) recon_loss: tensor(24.0739)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 75 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.988, valid 0.953\n",
      "cl_loss: tensor(0.0183) cl_loss_thickness: tensor(0.0598, grad_fn=<NllLossBackward>) recon_loss: tensor(21.6365)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 76 Loss: train 0.011, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.984, valid 0.953\n",
      "cl_loss: tensor(0.0058) cl_loss_thickness: tensor(0.0717, grad_fn=<NllLossBackward>) recon_loss: tensor(20.4369)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 77 Loss: train 0.021, valid 0.016. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.986, valid 0.953\n",
      "cl_loss: tensor(0.0096) cl_loss_thickness: tensor(0.0407, grad_fn=<NllLossBackward>) recon_loss: tensor(24.4550)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 78 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.986, valid 0.958\n",
      "cl_loss: tensor(0.0129) cl_loss_thickness: tensor(0.0562, grad_fn=<NllLossBackward>) recon_loss: tensor(28.2972)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 79 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.984, valid 0.953\n",
      "cl_loss: tensor(0.0156) cl_loss_thickness: tensor(0.0536, grad_fn=<NllLossBackward>) recon_loss: tensor(26.7346)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 80 Loss: train 0.021, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.986, valid 0.953\n",
      "cl_loss: tensor(0.0057) cl_loss_thickness: tensor(0.0390, grad_fn=<NllLossBackward>) recon_loss: tensor(22.9363)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 81 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.988, valid 0.943\n",
      "cl_loss: tensor(0.0044) cl_loss_thickness: tensor(0.0195, grad_fn=<NllLossBackward>) recon_loss: tensor(23.1947)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 82 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.986, valid 0.953\n",
      "cl_loss: tensor(0.0108) cl_loss_thickness: tensor(0.0454, grad_fn=<NllLossBackward>) recon_loss: tensor(25.8016)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 83 Loss: train 0.010, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.986, valid 0.953\n",
      "cl_loss: tensor(0.0082) cl_loss_thickness: tensor(0.1024, grad_fn=<NllLossBackward>) recon_loss: tensor(26.3309)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 84 Loss: train 0.010, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.988, valid 0.958\n",
      "cl_loss: tensor(0.0123) cl_loss_thickness: tensor(0.1207, grad_fn=<NllLossBackward>) recon_loss: tensor(25.7366)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 85 Loss: train 0.018, valid 0.017. Accuracy: train: 1.000, valid 0.938. Thickness Accuracy: train: 0.986, valid 0.953\n",
      "cl_loss: tensor(0.0301) cl_loss_thickness: tensor(0.0406, grad_fn=<NllLossBackward>) recon_loss: tensor(17.9639)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 86 Loss: train 0.021, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.986, valid 0.953\n",
      "cl_loss: tensor(0.0131) cl_loss_thickness: tensor(0.0675, grad_fn=<NllLossBackward>) recon_loss: tensor(186.3482)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 87 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.991, valid 0.958\n",
      "cl_loss: tensor(0.0065) cl_loss_thickness: tensor(0.0363, grad_fn=<NllLossBackward>) recon_loss: tensor(22.1429)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 88 Loss: train 0.021, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.988, valid 0.953\n",
      "cl_loss: tensor(0.0157) cl_loss_thickness: tensor(0.0296, grad_fn=<NllLossBackward>) recon_loss: tensor(604.2169)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 89 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.990, valid 0.958\n",
      "cl_loss: tensor(0.0103) cl_loss_thickness: tensor(0.0443, grad_fn=<NllLossBackward>) recon_loss: tensor(21.0092)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 90 Loss: train 0.020, valid 0.019. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.988, valid 0.953\n",
      "cl_loss: tensor(0.0157) cl_loss_thickness: tensor(0.0361, grad_fn=<NllLossBackward>) recon_loss: tensor(613.1139)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 91 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.990, valid 0.953\n",
      "cl_loss: tensor(0.0092) cl_loss_thickness: tensor(0.0271, grad_fn=<NllLossBackward>) recon_loss: tensor(185.6877)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 92 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.990, valid 0.953\n",
      "cl_loss: tensor(0.0072) cl_loss_thickness: tensor(0.0404, grad_fn=<NllLossBackward>) recon_loss: tensor(19.3517)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num 576, val_num 192\n",
      "Epoch: 93 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.986, valid 0.953\n",
      "cl_loss: tensor(0.0269) cl_loss_thickness: tensor(0.0896, grad_fn=<NllLossBackward>) recon_loss: tensor(23.5228)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 94 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.988, valid 0.953\n",
      "cl_loss: tensor(0.0245) cl_loss_thickness: tensor(0.0286, grad_fn=<NllLossBackward>) recon_loss: tensor(22.5943)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 95 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.988, valid 0.953\n",
      "cl_loss: tensor(0.0114) cl_loss_thickness: tensor(0.0431, grad_fn=<NllLossBackward>) recon_loss: tensor(21.5866)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 96 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.927. Thickness Accuracy: train: 0.988, valid 0.953\n",
      "cl_loss: tensor(0.0104) cl_loss_thickness: tensor(0.0516, grad_fn=<NllLossBackward>) recon_loss: tensor(23.1378)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 97 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.986, valid 0.953\n",
      "cl_loss: tensor(0.0122) cl_loss_thickness: tensor(0.0191, grad_fn=<NllLossBackward>) recon_loss: tensor(23.9187)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 98 Loss: train 0.021, valid 0.018. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.986, valid 0.953\n",
      "cl_loss: tensor(0.0069) cl_loss_thickness: tensor(0.0305, grad_fn=<NllLossBackward>) recon_loss: tensor(39.5149)\n",
      "train_num 576, val_num 192\n",
      "Epoch: 99 Loss: train 0.021, valid 0.017. Accuracy: train: 1.000, valid 0.932. Thickness Accuracy: train: 0.988, valid 0.953\n",
      "Best model at epoch 0 with acc 0.000\n",
      "training takes time 0:04:44.012037\n",
      " test batch 6 size 8 < 32, skip\n",
      "last epoch Test accuracy for 0  fold :  0.90625 0.6145833333333334\n"
     ]
    }
   ],
   "source": [
    "# Initialize training settings\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "cl_loss_fn = nn.NLLLoss()\n",
    "recon_loss_fn = nn.MSELoss()\n",
    "\n",
    "# model.load_state_dict(torch.load('models_and_stats/model_phased_LSTM_B30.pt', map_location='cpu'))\n",
    "saved_dicts = torch.load('models_and_stats/'+model_name+str(92)+'.pt', map_location='cpu')\n",
    "model.load_state_dict(saved_dicts['model_state_dict'])\n",
    "optimizer.load_state_dict(saved_dicts['optimizer_state_dict'])\n",
    "\n",
    "training_start=datetime.now()\n",
    "# create empty lists to fill stats later\n",
    "epoch_train_loss = []\n",
    "epoch_train_acc = []\n",
    "epoch_train_thickness_acc = []\n",
    "epoch_val_loss = []\n",
    "epoch_val_acc = []\n",
    "epoch_val_thickness_acc = []\n",
    "max_val_acc = 0\n",
    "max_val_epoch = 0\n",
    "if block == \"phased_LSTM\":\n",
    "    time = torch.Tensor(range(sequence_length))\n",
    "    times = time.repeat(batch_size, 1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # if epoch < 30:\n",
    "    #     continue\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    correct_texture = 0\n",
    "    correct_thickness = 0\n",
    "    train_loss = 0\n",
    "    train_num = 0\n",
    "    for i, (XB,  y) in enumerate(train_loader):\n",
    "        if model.header == 'CNN':\n",
    "            x = XI\n",
    "        else:\n",
    "            x = XB\n",
    "        # x = x[:, :, 1::2]\n",
    "        x, y = x.to(device), y.long().to(device) # 32, 19, 400\n",
    "        if x.size()[0] != batch_size:\n",
    "            break\n",
    "\n",
    "        # reduce data by data_reduction_ratio times\n",
    "        if i % data_reduction_ratio == 0:\n",
    "            train_num += x.size(0)\n",
    "            optimizer.zero_grad()\n",
    "            if block == \"phased_LSTM\":\n",
    "                x_decoded, latent, output = model(x, times)\n",
    "            else:\n",
    "                x_decoded, latent, output_texture, output_thickness = model(x)\n",
    "\n",
    "            # assert not torch.isnan(y).any(), \"batch_num=\"+str(i)\n",
    "            # print((output == 0).nonzero().size(0)==0)\n",
    "\n",
    "            # assert (output == 0).nonzero().size(0)==0, 'output contain zero, batch_num'+str(i)+' indices:'+str((output == 0).nonzero())\n",
    "            if (output_texture == 0).nonzero().size(0) != 0:\n",
    "                print('batch_num'+str(i)+' indices:'+str((output_texture == 0).nonzero()))\n",
    "                cl_loss = cl_loss_fn(output_texture+1e-5, y) # avoid nan\n",
    "            else:\n",
    "                cl_loss = cl_loss_fn(output_texture, y) \n",
    "\n",
    "            thick_y = property_label(y)\n",
    "\n",
    "            cl_loss_thickness = cl_loss_fn(output_thickness, thick_y)\n",
    "\n",
    "            recon_loss = recon_loss_fn(x_decoded, x)\n",
    "            loss = w_c*cl_loss + w_r *recon_loss # TODO thickness loss\n",
    "\n",
    "            # compute classification acc\n",
    "            pred_texture = output_texture.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct_texture += pred_texture.eq(y.data.view_as(pred_texture)).long().cpu().sum().item()\n",
    "            pred_thickness = output_thickness.data.max(1, keepdim=True)[1]\n",
    "            correct_thickness += pred_thickness.eq(thick_y.data.view_as(pred_thickness)).long().cpu().sum().item()\n",
    "            # accumulator\n",
    "            train_loss += loss.item()\n",
    "            start_bp = datetime.now()\n",
    "            cl_loss_thickness.backward()\n",
    "            figname = logDir + model_name + \"grad_flow_plot_epoch\" +str(epoch)+\".png\"\n",
    "            if i == 0: # and epoch%50 == 0:\n",
    "                print('cl_loss:', cl_loss, 'cl_loss_thickness:', cl_loss_thickness, 'recon_loss:', recon_loss)\n",
    "                # print(\"grad flow for epoch {}\".format(epoch))\n",
    "                # plot_grad_flow(model.named_parameters(), figname, if_plot=False)\n",
    "\n",
    "                # if epoch % 5 == 0:\n",
    "                #     k = model.encoder.k_out\n",
    "                #     k = k.squeeze()\n",
    "                #     # print('k:',k[:, 0, 70])\n",
    "                #     # tau = model.encoder.model.phased_cell.tau\n",
    "                #     # print('tau:', tau)\n",
    "                #     # phase = model.encoder.model.phased_cell.phase\n",
    "                #     # print('phase:', phase)\n",
    "                #     k = k[:,0,:].cpu().detach().numpy()\n",
    "                #     x = x[0,:,:].permute(1,0)\n",
    "                #     x = x.cpu().detach().numpy()\n",
    "                #     fig, ax =plt.subplots(1,2)\n",
    "                #     sns.heatmap(x, ax=ax[0])\n",
    "                #     sns.heatmap(k, vmin=0, vmax=1, ax=ax[1])\n",
    "                #     fig.savefig(logDir + model_name + \"x_k_plot_epoch\" +str(epoch)+\".png\")\n",
    "                #     fig.clf()\n",
    "            optimizer.step()\n",
    "            # print('1 batch bp time:', datetime.now()-start_bp)\n",
    "\n",
    "    # if epoch == 0:\n",
    "    #     print('first epoch training time:', datetime.now()-training_start)\n",
    "\n",
    "    # if epoch < 20 or epoch%200 == 0:\n",
    "    # print(\"train last batch {} of {}: cl_loss {:.3f} recon_loss {:.3f}\".format(i, len(train_loader), cl_loss, recon_loss))\n",
    "\n",
    "    # fill stats\n",
    "    train_accuracy = correct_texture / train_num \n",
    "    train_thickness_accuracy = correct_thickness / train_num \n",
    "    train_loss /= train_num\n",
    "    epoch_train_loss.append(train_loss) \n",
    "    epoch_train_acc.append(train_accuracy) \n",
    "    epoch_train_thickness_acc.append(train_thickness_accuracy) \n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    correct_texture = 0\n",
    "    correct_thickness = 0\n",
    "    val_loss = 0\n",
    "    val_num = 0\n",
    "    for i, (XB, y) in enumerate(val_loader):\n",
    "        if model.header == 'CNN':\n",
    "            x = XI\n",
    "        else:\n",
    "            x = XB\n",
    "        # x = x[:, :, 1::2]\n",
    "        x, y = x.to(device), y.long().to(device)\n",
    "        if x.size()[0] != batch_size:\n",
    "            break\n",
    "        val_num += x.size(0)\n",
    "        if block == \"phased_LSTM\":\n",
    "            x_decoded, latent, output = model(x, times)\n",
    "        else:\n",
    "            x_decoded, latent, output_texture, output_thickness = model(x)\n",
    "\n",
    "        # construct loss function\n",
    "        thick_y = property_label(y)\n",
    "        cl_loss = cl_loss_fn(output_texture, y)\n",
    "        cl_loss_thickness = cl_loss_fn(output_thickness, thick_y)\n",
    "        recon_loss = recon_loss_fn(x_decoded, x)\n",
    "        loss = w_c*cl_loss + w_r *recon_loss\n",
    "\n",
    "        # compute classification acc\n",
    "        pred_texture = output_texture.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct_texture += pred_texture.eq(y.data.view_as(pred_texture)).long().cpu().sum().item()\n",
    "        pred_thickness = output_thickness.data.max(1, keepdim=True)[1]\n",
    "        correct_thickness += pred_thickness.eq(thick_y.data.view_as(pred_thickness)).long().cpu().sum().item()\n",
    "        \n",
    "        if i == 0:\n",
    "            f1 = f1_score(thick_y, pred_thickness)\n",
    "            print('f1 score: f1)\n",
    "\n",
    "        # accumulator\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    # fill stats\n",
    "    val_accuracy = correct_texture / val_num\n",
    "    val_loss /= val_num\n",
    "    val_thickness_accuracy = correct_thickness / val_num \n",
    "    epoch_val_loss.append(val_loss)  # only save the last batch\n",
    "    epoch_val_acc.append(val_accuracy)\n",
    "    epoch_val_thickness_acc.append(val_thickness_accuracy) \n",
    "\n",
    "    # if epoch < 20 or epoch%200 == 0:\n",
    "    print(\"train_num {}, val_num {}\".format(train_num, val_num))\n",
    "    print('Epoch: {} Loss: train {:.3f}, valid {:.3f}. Accuracy: train: {:.3f}, valid {:.3f}. Thickness Accuracy: train: {:.3f}, valid {:.3f}'.format(epoch, train_loss, val_loss, train_accuracy, val_accuracy, train_thickness_accuracy, val_thickness_accuracy))\n",
    "\n",
    "#     # choose model\n",
    "#     if max_val_acc <= val_accuracy:\n",
    "#         model_dir = logDir + model_name + str(epoch) + '.pt'\n",
    "#         print('Saving model at {} epoch to {}'.format(epoch, model_dir))\n",
    "#         max_val_acc = val_accuracy\n",
    "#         max_val_epoch = epoch\n",
    "#         torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, model_dir)\n",
    "    # TODO\n",
    "    # model_dir = logDir + model_name + str(epoch) + '.pt'\n",
    "    # print('Saving model at {} epoch to {}'.format(epoch, model_dir))\n",
    "    # max_val_acc = val_accuracy\n",
    "    # torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, model_dir)\n",
    "print('Best model at epoch {} with acc {:.3f}'.format(max_val_epoch, max_val_acc))\n",
    "training_end =  datetime.now()\n",
    "training_time = training_end -training_start \n",
    "print(\"training takes time {}\".format(training_time))\n",
    "\n",
    "model.is_fitted = True\n",
    "model.eval()\n",
    "\n",
    "# TEST at last epoch\n",
    "correct_texture = 0\n",
    "correct_thickness = 0\n",
    "test_num = 0\n",
    "for i, (XB,  y) in enumerate(test_loader):\n",
    "    if model.header == 'CNN':\n",
    "        x = XI\n",
    "    else:\n",
    "        x = XB\n",
    "    # x = x[:, :, 1::2]\n",
    "    x, y = x.to(device), y.long().to(device)\n",
    "\n",
    "    if x.size(0) != batch_size:\n",
    "        print(\" test batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "        break\n",
    "    test_num += x.size(0)\n",
    "    if block == \"phased_LSTM\":\n",
    "        x_decoded, latent, output = model(x, times)\n",
    "    else:\n",
    "        x_decoded, latent, output_texture, output_thickness = model(x)\n",
    "\n",
    "    # compute classification acc\n",
    "    pred_texture = output_texture.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct_texture += pred_texture.eq(y.data.view_as(pred_texture)).long().cpu().sum().item()\n",
    "    pred_thickness = output_thickness.data.max(1, keepdim=True)[1]\n",
    "    correct_thickness += pred_thickness.eq(thick_y.data.view_as(pred_thickness)).long().cpu().sum().item()\n",
    "\n",
    "test_acc1 = correct_texture / test_num #len(test_loader.dataset)\n",
    "test_thickness_acc1 = correct_thickness / test_num\n",
    "print('last epoch Test accuracy for', str(kfold_number), ' fold : ', test_acc1, test_thickness_acc1)\n",
    "\n",
    "# # TEST at the best model\n",
    "# TODO\n",
    "# correct_texture = 0\n",
    "# correct_thickness = 0\n",
    "# test_num = 0\n",
    "# saved_dicts = torch.load('models_and_stats/'+model_name+str(max_val_epoch)+'.pt', map_location='cpu')\n",
    "# model.load_state_dict(saved_dicts['model_state_dict'])\n",
    "\n",
    "# for i, (XB,  y) in enumerate(test_loader):\n",
    "#     if model.header == 'CNN':\n",
    "#         x = XI\n",
    "#     else:\n",
    "#         x = XB\n",
    "#     # x = x[:, :, 1::2]\n",
    "#     x, y = x.to(device), y.long().to(device)\n",
    "\n",
    "#     if x.size(0) != batch_size:\n",
    "#         print(\" test batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "#         break\n",
    "#     test_num += x.size(0)\n",
    "#     if block == \"phased_LSTM\":\n",
    "#         x_decoded, latent, output = model(x, times)\n",
    "#     else:\n",
    "#         x_decoded, latent, output_texture, output_thickness = model(x)\n",
    "\n",
    "#     # compute classification acc\n",
    "#     pred_texture = output_texture.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "#     correct_texture += pred_texture.eq(y.data.view_as(pred_texture)).long().cpu().sum().item()\n",
    "#     pred_thickness = output_thickness.data.max(1, keepdim=True)[1]\n",
    "#     correct_thickness += pred_thickness.eq(thick_y.data.view_as(pred_thickness)).long().cpu().sum().item()\n",
    "\n",
    "# test_acc2 = correct_texture / test_num #len(test_loader.dataset)\n",
    "# test_thickness_acc2 = correct_thickness / test_num\n",
    "# print('at the best model Test accuracy for', str(kfold_number), ' fold : ', test_acc2, test_thickness_acc2)\n",
    "\n",
    "# Save stats\n",
    "# TODO\n",
    "# results_dict = {\"epoch_train_loss\": epoch_train_loss,\n",
    "#                 \"epoch_train_acc\": epoch_train_acc,\n",
    "#                 \"epoch_train_thickness_acc\": epoch_train_thickness_acc,\n",
    "#                 \"epoch_val_loss\": epoch_val_loss,\n",
    "#                 \"epoch_val_acc\": epoch_val_acc,\n",
    "#                 \"epoch_val_thickness_acc\": epoch_val_thickness_acc,\n",
    "#                 \"test_acc1\": test_acc1,\n",
    "#                 \"test_acc2\": test_acc2,\n",
    "#                 \"test_thickness_acc1\": test_thickness_acc1,\n",
    "#                 \"test_thickness_acc2\": test_thickness_acc2}\n",
    "\n",
    "# dict_name = model_name + '_stats_fold{}_{}.pkl'.format(str(kfold_number), args.rep)\n",
    "# pickle.dump(results_dict, open(logDir + dict_name, 'wb'))\n",
    "# print(\"dump results dict to {}\".format(dict_name))\n",
    "\n",
    "# assert n_epochs == len(epoch_train_acc), \"different epoch length {} {}\".format(n_epochs, len(epoch_train_acc))\n",
    "# fig, ax = plt.subplots(figsize=(15, 7))\n",
    "# ax.plot(np.arange(n_epochs), epoch_train_acc, label=\"train acc\")\n",
    "# ax.set_xlabel('epoch')\n",
    "# ax.set_ylabel('acc')\n",
    "# ax.grid(True)\n",
    "# plt.legend(loc='upper right')\n",
    "# figname = logDir + model_name +\"_train_acc.png\"\n",
    "# if if_plot:\n",
    "#     plt.show()\n",
    "\n",
    "# TODO plot_stats(logDir + dict_name, model_name)\n",
    "\n",
    "\n",
    "# for ds in ['c20_2', 'c20new', 'c50']:\n",
    "#     args.dataset = ds\n",
    "#     print(args)\n",
    "#     train(args)\n",
    "    # 87.5 95.83, 84.17 v2-32\n",
    "\n",
    "    # 0.932, 95.31, 82.08\n",
    "\n",
    "    # for r_on in [0.1, 0.2, 0.3, 0.4]:\n",
    "    #     args.r_on = r_on\n",
    "    #     print(args)\n",
    "    #     train(args)\n",
    "\n",
    "    # for p_max in [75.0, 60.0, 45.0, 30.0]:\n",
    "    #     args.p_max = p_max\n",
    "    #     print(args)\n",
    "    #     train(args) \n",
    "\n",
    "    # for hidden_size in [60, 70, 80, 90, 100]:\n",
    "    #     args.h_s = hidden_size\n",
    "    #     print(args)\n",
    "    #     train(args)\n",
    "\n",
    "    # for w_r in [0, 0.0005, 0.001, 0.005, 0.01]:\n",
    "    #     args.w_r = w_r\n",
    "    #     print(args)\n",
    "    #     train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7755102040816326\n",
      "plotting conf matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vrae.visual import confusion_matrix_plot\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(thick_y, pred_thickness)\n",
    "print(f1)\n",
    "confusion_matrix_plot(thick_y, pred_thickness, 'c20_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AE]",
   "language": "python",
   "name": "conda-env-AE-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
