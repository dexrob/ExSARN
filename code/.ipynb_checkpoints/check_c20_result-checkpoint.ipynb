{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "'''\n",
    "sample command: python T4_BT19_ae.py -k 0 -c 0 -r 1 --data_dir /home/ruihan/data\n",
    "Individual training for BioTac data (full/partial data)\n",
    "if -r=1, train with full data\n",
    "if -r=2, train with half data\n",
    "loss = classification loss + recon loss \n",
    "'''\n",
    "\n",
    "# Import\n",
    "import os,sys\n",
    "import pickle\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from vrae.vrae import VRAEC\n",
    "from preprocess_data import get_TrainValTestLoader, get_TrainValTestDataset, get_TrainValTestData\n",
    "from vrae.visual import plot_grad_flow, plot_stats\n",
    "\n",
    "def train(args):\n",
    "    # Set hyper params\n",
    "    args_data_dir = args.data_dir\n",
    "    kfold_number = args.kfold\n",
    "    data_reduction_ratio = args.reduction\n",
    "    shuffle = True # set to False for partial training\n",
    "    sequence_length = 400\n",
    "    number_of_features = 19\n",
    "\n",
    "    hidden_size = args.h_s\n",
    "    hidden_layer_depth = 1\n",
    "    latent_length = 40\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.001 # 0.0005\n",
    "    n_epochs = 100\n",
    "    dropout_rate = 0.2\n",
    "    cuda = True # options: True, False\n",
    "    header = None\n",
    "    dataset = args.dataset\n",
    "    if dataset == 'c50':\n",
    "        num_class = 50\n",
    "    else:\n",
    "        num_class = 20\n",
    "\n",
    "    # loss weightage\n",
    "    w_r = args.w_r\n",
    "    w_c = 1\n",
    "\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    # Load data\n",
    "    # data_dir = os.path.join(args_data_dir, \"compiled_data/\")\n",
    "    logDir = 'models_and_stats/'\n",
    "    if_plot = False\n",
    "\n",
    "    # RNN block\n",
    "    block = \"phased_LSTM\" # LSTM, GRU, phased_LSTM\n",
    "\n",
    "    model_name = 'B_block_{}_data_{}_wrI_{}_wC_{}_hidden_{}_latent_{}_r_on_{}_p_max_{}'.format(block, dataset, w_r, w_c, str(hidden_size), str(latent_length), str(args.r_on), str(args.p_max))\n",
    "\n",
    "    if torch.cuda.is_available() and cuda:\n",
    "        device = torch.device(\"cuda:{}\".format(args.cuda))\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    if args.reduction != 1:\n",
    "        print(\"load {} kfold number, reduce data to {} folds, put to device: {}\".format(args.kfold, args.reduction, device))\n",
    "    else:\n",
    "        print(\"load {} kfold number, train with full data, put to device: {}\".format(args.kfold, device))\n",
    "\n",
    "    prefix = \"\"\n",
    "    dataset_dir = os.path.join(args_data_dir, dataset+\"/\") # TODO\n",
    "    train_set, val_set, test_set = get_TrainValTestDataset(dataset_dir, k=0, prefix=prefix, seq_len=sequence_length)\n",
    "    train_loader, val_loader, test_loader = get_TrainValTestLoader(dataset_dir, k=0, batch_size=batch_size,shuffle=shuffle, prefix=prefix,seq_len=sequence_length)\n",
    "    X_train, X_val, X_test, Y_train, Y_val, Y_test = get_TrainValTestData(dataset_dir, k=0, prefix=prefix,seq_len=sequence_length)\n",
    "    # Initialize models\n",
    "    model = VRAEC(num_class=num_class,\n",
    "                block=block,\n",
    "                sequence_length=sequence_length, # TODO\n",
    "                number_of_features = number_of_features,\n",
    "                hidden_size = hidden_size, \n",
    "                hidden_layer_depth = hidden_layer_depth,\n",
    "                latent_length = latent_length,\n",
    "                batch_size = batch_size,\n",
    "                learning_rate = learning_rate,\n",
    "                n_epochs = n_epochs,\n",
    "                dropout_rate = dropout_rate,\n",
    "                cuda = cuda,\n",
    "                model_name=model_name,\n",
    "                header=header,\n",
    "                device = device,\n",
    "                ratio_on=args.r_on, period_init_max=args.p_max)\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize training settings\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    cl_loss_fn = nn.NLLLoss()\n",
    "    recon_loss_fn = nn.MSELoss()\n",
    "\n",
    "    # model.load_state_dict(torch.load('models_and_stats/model_phased_LSTM_B30.pt', map_location='cpu'))\n",
    "    # saved_dicts = torch.load('models_and_stats/model_phased_LSTM_B.pt', map_location='cpu')\n",
    "    # model.load_state_dict(saved_dicts['model_state_dict'])\n",
    "    # optimizer.load_state_dict(saved_dicts['optimizer_state_dict'])\n",
    "\n",
    "    training_start=datetime.now()\n",
    "    # create empty lists to fill stats later\n",
    "    epoch_train_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_val_loss = []\n",
    "    epoch_val_acc = []\n",
    "    max_val_acc = 0\n",
    "    max_val_epoch = 0\n",
    "    if block == \"phased_LSTM\":\n",
    "        time = torch.Tensor(range(sequence_length))\n",
    "        times = time.repeat(batch_size, 1)\n",
    "\n",
    "#     for epoch in range(n_epochs):\n",
    "#         # if epoch < 30:\n",
    "#         #     continue\n",
    "#         # TRAIN\n",
    "#         model.train()\n",
    "#         correct = 0\n",
    "#         train_loss = 0\n",
    "#         train_num = 0\n",
    "#         for i, (XB,  y) in enumerate(train_loader):\n",
    "#             if model.header == 'CNN':\n",
    "#                 x = XI\n",
    "#             else:\n",
    "#                 x = XB\n",
    "#             # x = x[:, :, 1::2]\n",
    "#             x, y = x.to(device), y.long().to(device) # 32, 19, 400\n",
    "#             if x.size()[0] != batch_size:\n",
    "#                 break\n",
    "            \n",
    "#             # reduce data by data_reduction_ratio times\n",
    "#             if i % data_reduction_ratio == 0:\n",
    "#                 train_num += x.size(0)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 if block == \"phased_LSTM\":\n",
    "#                     x_decoded, latent, output = model(x, times)\n",
    "#                 else:\n",
    "#                     x_decoded, latent, output = model(x)\n",
    "\n",
    "#                 # assert not torch.isnan(y).any(), \"batch_num=\"+str(i)\n",
    "#                 # print((output == 0).nonzero().size(0)==0)\n",
    "\n",
    "#                 # assert (output == 0).nonzero().size(0)==0, 'output contain zero, batch_num'+str(i)+' indices:'+str((output == 0).nonzero())\n",
    "#                 if (output == 0).nonzero().size(0) != 0:\n",
    "#                     print('batch_num'+str(i)+' indices:'+str((output == 0).nonzero()))\n",
    "#                     cl_loss = cl_loss_fn(output+1e-5, y) # avoid nan\n",
    "#                 else:\n",
    "#                     cl_loss = cl_loss_fn(output, y) \n",
    "\n",
    "#                 recon_loss = recon_loss_fn(x_decoded, x)\n",
    "#                 loss = w_c*cl_loss + w_r *recon_loss\n",
    "                \n",
    "#                 # compute classification acc\n",
    "#                 pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "#                 correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "#                 # accumulator\n",
    "#                 train_loss += loss.item()\n",
    "#                 start_bp = datetime.now()\n",
    "#                 loss.backward()\n",
    "#                 figname = logDir + model_name + \"grad_flow_plot_epoch\" +str(epoch)+\".png\"\n",
    "#                 if i == 0: # and epoch%50 == 0:\n",
    "#                     print('cl_loss:', cl_loss, 'recon_loss:', recon_loss)\n",
    "#                     print(\"grad flow for epoch {}\".format(epoch))\n",
    "#                     plot_grad_flow(model.named_parameters(), figname, if_plot=False)\n",
    "#                     # if epoch % 5 == 0:\n",
    "#                     #     k = model.encoder.k_out\n",
    "#                     #     k = k.squeeze()\n",
    "#                     #     # print('k:',k[:, 0, 70])\n",
    "#                     #     # tau = model.encoder.model.phased_cell.tau\n",
    "#                     #     # print('tau:', tau)\n",
    "#                     #     # phase = model.encoder.model.phased_cell.phase\n",
    "#                     #     # print('phase:', phase)\n",
    "#                     #     k = k[:,0,:].cpu().detach().numpy()\n",
    "#                     #     x = x[0,:,:].permute(1,0)\n",
    "#                     #     x = x.cpu().detach().numpy()\n",
    "#                     #     fig, ax =plt.subplots(1,2)\n",
    "#                     #     sns.heatmap(x, ax=ax[0])\n",
    "#                     #     sns.heatmap(k, vmin=0, vmax=1, ax=ax[1])\n",
    "#                     #     fig.savefig(logDir + model_name + \"x_k_plot_epoch\" +str(epoch)+\".png\")\n",
    "#                     #     fig.clf()\n",
    "#                 optimizer.step()\n",
    "#                 # print('1 batch bp time:', datetime.now()-start_bp)\n",
    "\n",
    "#         # if epoch == 0:\n",
    "#         #     print('first epoch training time:', datetime.now()-training_start)\n",
    "        \n",
    "#         # if epoch < 20 or epoch%200 == 0:\n",
    "#         # print(\"train last batch {} of {}: cl_loss {:.3f} recon_loss {:.3f}\".format(i, len(train_loader), cl_loss, recon_loss))\n",
    "\n",
    "#         # fill stats\n",
    "#         train_accuracy = correct / train_num \n",
    "#         train_loss /= train_num\n",
    "#         epoch_train_loss.append(train_loss)\n",
    "#         epoch_train_acc.append(train_accuracy) \n",
    "        \n",
    "#         # VALIDATION\n",
    "#         model.eval()\n",
    "#         correct = 0\n",
    "#         val_loss = 0\n",
    "#         val_num = 0\n",
    "#         for i, (XB, y) in enumerate(val_loader):\n",
    "#             if model.header == 'CNN':\n",
    "#                 x = XI\n",
    "#             else:\n",
    "#                 x = XB\n",
    "#             # x = x[:, :, 1::2]\n",
    "#             x, y = x.to(device), y.long().to(device)\n",
    "#             if x.size()[0] != batch_size:\n",
    "#                 break\n",
    "#             val_num += x.size(0)\n",
    "#             if block == \"phased_LSTM\":\n",
    "#                 x_decoded, latent, output = model(x, times)\n",
    "#             else:\n",
    "#                 x_decoded, latent, output = model(x)\n",
    "\n",
    "#             # construct loss function\n",
    "#             cl_loss = cl_loss_fn(output, y)\n",
    "#             recon_loss = recon_loss_fn(x_decoded, x)\n",
    "#             loss = w_c*cl_loss + w_r *recon_loss\n",
    "            \n",
    "#             # compute classification acc\n",
    "#             pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "#             correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "#             # accumulator\n",
    "#             val_loss += loss.item()\n",
    "        \n",
    "#         # fill stats\n",
    "#         val_accuracy = correct / val_num\n",
    "#         val_loss /= val_num\n",
    "#         epoch_val_loss.append(val_loss)  # only save the last batch\n",
    "#         epoch_val_acc.append(val_accuracy)\n",
    "        \n",
    "#         # if epoch < 20 or epoch%200 == 0:\n",
    "#         print(\"train_num {}, val_num {}\".format(train_num, val_num))\n",
    "#         print('Epoch: {} Loss: train {:.3f}, valid {:.3f}. Accuracy: train: {:.3f}, valid {:.3f}'.format(epoch, train_loss, val_loss, train_accuracy, val_accuracy))\n",
    "        \n",
    "#         # choose model\n",
    "#         if max_val_acc <= val_accuracy:\n",
    "#             model_dir = logDir + model_name + str(epoch) + '.pt'\n",
    "#             print('Saving model at {} epoch to {}'.format(epoch, model_dir))\n",
    "#             max_val_acc = val_accuracy\n",
    "#             max_val_epoch = epoch\n",
    "#             torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, model_dir)\n",
    "#         # model_dir = logDir + model_name + str(epoch) + '.pt'\n",
    "#         # print('Saving model at {} epoch to {}'.format(epoch, model_dir))\n",
    "#         # max_val_acc = val_accuracy\n",
    "#         # torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, model_dir)\n",
    "#     print('Best model at epoch {} with acc {:.3f}'.format(max_val_epoch, max_val_acc))\n",
    "#     training_end =  datetime.now()\n",
    "#     training_time = training_end -training_start \n",
    "#     print(\"training takes time {}\".format(training_time))\n",
    "\n",
    "#     model.is_fitted = True\n",
    "#     model.eval()\n",
    "\n",
    "#     # TEST at last epoch\n",
    "#     correct = 0\n",
    "#     test_num = 0\n",
    "#     for i, (XB,  y) in enumerate(test_loader):\n",
    "#         if model.header == 'CNN':\n",
    "#             x = XI\n",
    "#         else:\n",
    "#             x = XB\n",
    "#         # x = x[:, :, 1::2]\n",
    "#         x, y = x.to(device), y.long().to(device)\n",
    "        \n",
    "#         if x.size(0) != batch_size:\n",
    "#             print(\" test batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "#             break\n",
    "#         test_num += x.size(0)\n",
    "#         if block == \"phased_LSTM\":\n",
    "#             x_decoded, latent, output = model(x, times)\n",
    "#         else:\n",
    "#             x_decoded, latent, output = model(x)\n",
    "\n",
    "#         # compute classification acc\n",
    "#         pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "#         correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        \n",
    "#     test_acc1 = correct / test_num #len(test_loader.dataset)\n",
    "#     print('last epoch Test accuracy for', str(kfold_number), ' fold : ', test_acc1)\n",
    "\n",
    "    # TEST at the best model\n",
    "    correct = 0\n",
    "    test_num = 0\n",
    "    \n",
    "    saved_dicts = torch.load('models_and_stats/'+'B_block_phased_LSTM_data_c20_wrI_0.005_wC_1_hidden_90_latent_40_r_on_0.1_p_max_20060.pt', map_location='cpu')\n",
    "    # saved_dicts = torch.load('models_and_stats/'+'model_name+str(max_val_epoch)+'.pt'', map_location='cpu')\n",
    "    model.load_state_dict(saved_dicts['model_state_dict'])\n",
    "\n",
    "    for i, (XB,  y) in enumerate(test_loader):\n",
    "        if model.header == 'CNN':\n",
    "            x = XI\n",
    "        else:\n",
    "            x = XB\n",
    "        # x = x[:, :, 1::2]\n",
    "        x, y = x.to(device), y.long().to(device)\n",
    "        \n",
    "        if x.size(0) != batch_size:\n",
    "            print(\" test batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "            break\n",
    "        test_num += x.size(0)\n",
    "        if block == \"phased_LSTM\":\n",
    "            x_decoded, latent, output = model(x, times)\n",
    "        else:\n",
    "            x_decoded, latent, output = model(x)\n",
    "\n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        print(i, pred.eq(y.data.view_as(pred)).long().cpu().sum().item())\n",
    "        correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        \n",
    "    print(correct, test_num)    \n",
    "    test_acc2 = correct / test_num #len(test_loader.dataset)\n",
    "    print('at the best model Test accuracy for', str(kfold_number), ' fold : ', test_acc2)\n",
    "\n",
    "#     # Save stats\n",
    "#     results_dict = {\"epoch_train_loss\": epoch_train_loss,\n",
    "#                     \"epoch_train_acc\": epoch_train_acc,\n",
    "#                     \"epoch_val_loss\": epoch_val_loss,\n",
    "#                     \"epoch_val_acc\": epoch_val_acc,\n",
    "#                     \"test_acc1\": test_acc1,\n",
    "#                     \"test_acc2\": test_acc2}\n",
    "\n",
    "#     dict_name = model_name + '_stats_fold{}_{}.pkl'.format(str(kfold_number), args.rep)\n",
    "#     pickle.dump(results_dict, open(logDir + dict_name, 'wb'))\n",
    "#     print(\"dump results dict to {}\".format(dict_name))\n",
    "\n",
    "    # assert n_epochs == len(epoch_train_acc), \"different epoch length {} {}\".format(n_epochs, len(epoch_train_acc))\n",
    "    # fig, ax = plt.subplots(figsize=(15, 7))\n",
    "    # ax.plot(np.arange(n_epochs), epoch_train_acc, label=\"train acc\")\n",
    "    # ax.set_xlabel('epoch')\n",
    "    # ax.set_ylabel('acc')\n",
    "    # ax.grid(True)\n",
    "    # plt.legend(loc='upper right')\n",
    "    # figname = logDir + model_name +\"_train_acc.png\"\n",
    "    # if if_plot:\n",
    "    #     plt.show()\n",
    "\n",
    "    # plot_stats(logDir + dict_name)\n",
    "\n",
    "# # Parse argument\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-i\", \"--rep\", type=int, default=0, help='index of running repetition')\n",
    "# parser.add_argument('--data_dir', type=str, default='data', help=\"DIR set in 'gh_download.sh' to store compiled_data\")\n",
    "# parser.add_argument(\"-k\", \"--kfold\", type=int, default=0, help=\"kfold_number for loading data\")\n",
    "# parser.add_argument(\"-r\", \"--reduction\", type=int, default=1, help=\"data reduction ratio for partial training\")\n",
    "# parser.add_argument(\"-c\", \"--cuda\", default=0, help=\"index of cuda gpu to use\")\n",
    "# parser.add_argument(\"--r_on\", default=0.1, help=\"ratio_on for phased lstm\")\n",
    "# parser.add_argument(\"--p_max\", default=200, help=\"period_init_max for phased lstm\")\n",
    "# parser.add_argument(\"--w_r\", default=0.005, type=float, help=\"weight of recon loss\")\n",
    "# parser.add_argument(\"--h_s\", default=90, type=int, help=\"hidden size of rnn layers\")\n",
    "# parser.add_argument(\"--dataset\", default='c20', type=str, help=\"name of dataset\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# dummy class to replace argparser, if running jupyter notebook\n",
    "class Args:\n",
    "    rep = 0\n",
    "    data_dir = 'data'\n",
    "    kfold = 0\n",
    "    cuda = '0'\n",
    "    reduction = 1\n",
    "    r_on = 0.1\n",
    "    p_max = 200\n",
    "    w_r = 0.005\n",
    "    h_s = 90\n",
    "    dataset = 'c20'\n",
    "\n",
    "args=Args()\n",
    "\n",
    "# for ds in ['c20', 'c20new', 'c50']:\n",
    "#     args.dataset = ds\n",
    "#     print(args)\n",
    "#     train(args)\n",
    "\n",
    "\n",
    "    # 0.932\n",
    "\n",
    "    # for r_on in [0.1, 0.2, 0.3, 0.4]:\n",
    "    #     args.r_on = r_on\n",
    "    #     print(args)\n",
    "    #     train(args)\n",
    "\n",
    "    # for p_max in [75.0, 60.0, 45.0, 30.0]:\n",
    "    #     args.p_max = p_max\n",
    "    #     print(args)\n",
    "    #     train(args) \n",
    "\n",
    "    # for hidden_size in [60, 70, 80, 90, 100]:\n",
    "    #     args.h_s = hidden_size\n",
    "    #     print(args)\n",
    "    #     train(args)\n",
    "\n",
    "    # for w_r in [0, 0.0005, 0.001, 0.005, 0.01]:\n",
    "    #     args.w_r = w_r\n",
    "    #     print(args)\n",
    "    #     train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args object at 0x7ffa609b7490>\n",
      "load 0 kfold number, train with full data, put to device: cpu\n",
      "chop org data of length 400 into 1 segments, each of which is has length 400\n",
      "chop org data of length 400 into 1 segments, each of which is has length 400\n",
      "chop org data of length 400 into 1 segments, each of which is has length 400\n",
      "0 25\n",
      "1 27\n",
      "2 28\n",
      "3 26\n",
      "4 30\n",
      "5 29\n",
      " test batch 6 size 8 < 32, skip\n",
      "165 192\n",
      "at the best model Test accuracy for 0  fold :  0.859375\n"
     ]
    }
   ],
   "source": [
    "print(args)\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AE]",
   "language": "python",
   "name": "conda-env-AE-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
