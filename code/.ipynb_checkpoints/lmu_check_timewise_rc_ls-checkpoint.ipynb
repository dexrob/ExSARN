{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/tian/opt/anaconda3/envs/AE/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Args object at 0x7ff7104217c0>\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "'''\n",
    "sample command: python T4_BT19_ae.py \n",
    "Individual training for BioTac data (full/partial data)\n",
    "if -r=1, train with full data\n",
    "if -r=2, train with half data\n",
    "loss = classification loss + recon loss \n",
    "'''\n",
    "\n",
    "# Import\n",
    "import os,sys\n",
    "import pickle\n",
    "# import argparse\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vrae.lmu import CNNLMU\n",
    "from vrae.tas_utils_bs import get_trainValLoader, get_testLoader\n",
    "\n",
    "from vrae.visual import confusion_matrix_plot, tsne\n",
    "\n",
    "import scipy.io as sio\n",
    "\n",
    "# if running jupyter notebook\n",
    "class Args:\n",
    "    kfold = 0\n",
    "    cuda = '0'\n",
    "    rep = 0\n",
    "    reduction = 1\n",
    "    data_dir = 'data'\n",
    "    data = 'B'\n",
    "    w_r = 0.01\n",
    "    w_c = 1\n",
    "    if data == 'I':\n",
    "        sequence_length = 75\n",
    "        number_of_features = 60\n",
    "        header = 'CNN'\n",
    "\n",
    "    if data == 'B':\n",
    "        sequence_length = 400\n",
    "        number_of_features = 19\n",
    "        header = None\n",
    "        \n",
    "args=Args()\n",
    "print(args)\n",
    "\n",
    "# # Parse argument\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-i\", \"--rep\", type=int, default=0, help='index of running repetition')\n",
    "# parser.add_argument('--data_dir', type=str, required=True, help=\"DIR set in 'gh_download.sh' to store compiled_data\")\n",
    "# parser.add_argument(\"-k\", \"--kfold\", type=int, default=0, help=\"kfold_number for loading data\")\n",
    "# parser.add_argument(\"-r\", \"--reduction\", type=int, default=1, help=\"data reduction ratio for partial training\")\n",
    "# parser.add_argument(\"-c\", \"--cuda\", default=0, help=\"index of cuda gpu to use\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "def check_timewise_rcls(args, sequence_length, number_of_features, header):\n",
    "    # Set hyper params\n",
    "    args_data_dir = args.data_dir\n",
    "    kfold_number = args.kfold\n",
    "    data_reduction_ratio = args.reduction\n",
    "    shuffle = True # set to False for partial training\n",
    "    num_class = 20\n",
    "\n",
    "    hidden_size = 90\n",
    "    hidden_layer_depth = 1\n",
    "    latent_length = 40\n",
    "    batch_size = 192\n",
    "    learning_rate = 0.0005\n",
    "    n_epochs = 1 # TODO reset the parameter\n",
    "    dropout_rate = 0.2\n",
    "    cuda = True # options: True, False\n",
    "\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    # Load data\n",
    "    data_dir = os.path.join(args_data_dir, \"compiled_data/\")\n",
    "    logDir = 'models_and_stats/'\n",
    "    if_plot = False\n",
    "\n",
    "    # model_name = 'BT19_ae_{}_wrI_{}_wC_{}_{}'.format(data_reduction_ratio, w_r, w_c, str(kfold_number))\n",
    "    model_name = \"test_indiv_B\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:{}\".format(args.cuda))\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    if args.reduction != 1:\n",
    "        print(\"load {} kfold number, reduce data to {} folds, put to device: {}\".format(args.kfold, args.reduction, device))\n",
    "    else:\n",
    "        print(\"load {} kfold number, train with full data, put to devide: {}\".format(args.kfold, device))\n",
    "\n",
    "    # train_loader, val_loader, train_dataset, val_dataset = get_trainValLoader(data_dir, k=kfold_number, spike_ready=False, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader, test_dataset = get_testLoader(data_dir, spike_ready=False, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    # Initialize models\n",
    "    model = CNNLMU(num_class=num_class,\n",
    "                sequence_length=sequence_length,\n",
    "                number_of_features = number_of_features,\n",
    "                hidden_size = hidden_size, \n",
    "                hidden_layer_depth = hidden_layer_depth,\n",
    "                latent_length = latent_length,\n",
    "                batch_size = batch_size,\n",
    "                learning_rate = learning_rate,\n",
    "                n_epochs = n_epochs,\n",
    "                dropout_rate = dropout_rate,\n",
    "                cuda = cuda,\n",
    "                model_name=model_name,\n",
    "                header=header,\n",
    "                device = device)\n",
    "\n",
    "#     # TEST before training\n",
    "#     correct = 0\n",
    "#     test_num = 0\n",
    "#     for i, (XI, XB,  y) in enumerate(test_loader):\n",
    "#         if model.header == 'CNN':\n",
    "#             x = XI\n",
    "#         else:\n",
    "#             x = XB\n",
    "#         x, y = x.to(device), y.long().to(device)\n",
    "        \n",
    "#         if x.size(0) != batch_size:\n",
    "#             print(\" test batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "#             break\n",
    "#         test_num += x.size(0)\n",
    "#         x_decoded, latent, output = model(x)\n",
    "\n",
    "#         # compute classification acc\n",
    "#         pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "#         correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "\n",
    "#         if i == 0:\n",
    "#             all_latent = latent\n",
    "#             all_y = y\n",
    "#             all_pred = pred\n",
    "#         else:\n",
    "#             all_latent = torch.cat((all_latent, latent), 0)\n",
    "#             all_y = torch.cat((all_y, y), 0)\n",
    "#             all_pred = torch.cat((all_pred, pred), 0)\n",
    "\n",
    "#     # plot latent space and confusion matrix at the start of training\n",
    "#     tsne(all_latent, all_y, 'start')\n",
    "#     confusion_matrix_plot(all_y, all_pred, 'start')\n",
    "#     sio.savemat('save/latent_before_training.mat', {'latent':all_latent})\n",
    "              \n",
    "#     test_acc = correct / test_num #len(test_loader.dataset)\n",
    "#     print('before training, test accuracy for', str(kfold_number), ' fold : ', test_acc)\n",
    "\n",
    "    # load pretrained model\n",
    "    model_dir = logDir + model_name + '.pt'\n",
    "    model_dict = torch.load(model_dir)\n",
    "    model.load_state_dict(model_dict, strict = False)\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "\n",
    "    # Initialize training settings\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # cl_loss_fn = nn.NLLLoss()\n",
    "    recon_loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "#     training_start=datetime.now()\n",
    "#     # create empty lists to fill stats later\n",
    "#     epoch_train_loss = []\n",
    "#     epoch_train_acc = []\n",
    "#     epoch_val_loss = []\n",
    "#     epoch_val_acc = []\n",
    "#     epoch_cl_loss = []\n",
    "#     epoch_rc_loss = []\n",
    "#     max_val_acc = 0\n",
    "\n",
    "#     for epoch in range(n_epochs):\n",
    "        \n",
    "#         # TRAIN\n",
    "#         model.train()\n",
    "#         correct = 0\n",
    "#         train_loss = 0\n",
    "#         train_num = 0\n",
    "#         for i, (XI, XB,  y) in enumerate(train_loader):\n",
    "            \n",
    "#             if model.header == 'CNN':\n",
    "#                 x = XI\n",
    "#             else:\n",
    "#                 x = XB\n",
    "#             x, y = x.to(device), y.long().to(device)\n",
    "#             if x.size()[0] != batch_size:\n",
    "#                 break\n",
    "            \n",
    "#             # reduce data by data_reduction_ratio times\n",
    "#             if i % data_reduction_ratio == 0:\n",
    "#                 train_num += x.size(0)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 # print('X shape:', x.size()) torch.Size([32, 6, 10, 75]) for iCub; torch.Size([32, 19, 400]) for BioTac \n",
    "#                 x_decoded, latent, output = model(x)\n",
    "\n",
    "#                 # construct loss function\n",
    "#                 # print('output shape', output.size())\n",
    "#                 # print('y shape', y.size())\n",
    "#                 # print('x_decoded shape', x_decoded.size())\n",
    "#                 # print('x shape', x.size())\n",
    "#                 # output shape torch.Size([32, 20])\n",
    "#                 # y shape torch.Size([32])\n",
    "#                 # x_decoded shape torch.Size([32, 19, 1])\n",
    "#                 # x shape torch.Size([32, 19, 400])\n",
    "#                 cl_loss = cl_loss_fn(output, y)\n",
    "#                 recon_loss = recon_loss_fn(x_decoded, x)\n",
    "#                 loss = args.w_c*cl_loss + args.w_r *recon_loss\n",
    "                \n",
    "#                 # compute classification acc\n",
    "#                 pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "#                 correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "#                 # accumulator\n",
    "#                 train_loss += loss.item()\n",
    "                \n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "        \n",
    "#         # if epoch < 20 or epoch%200 == 0:\n",
    "#         print(\"train last batch {} of {}: cl_loss {:.3f} recon_loss {:.3f}\".format(i,len(train_loader),cl_loss, recon_loss))\n",
    "\n",
    "#         # fill stats\n",
    "#         train_accuracy = correct / train_num \n",
    "#         train_loss /= train_num\n",
    "#         epoch_train_loss.append(train_loss)\n",
    "#         epoch_train_acc.append(train_accuracy) \n",
    "#         epoch_cl_loss.append(cl_loss)\n",
    "#         epoch_rc_loss.append(recon_loss)\n",
    "        \n",
    "#         # VALIDATION\n",
    "#         model.eval()\n",
    "#         correct = 0\n",
    "#         val_loss = 0\n",
    "#         val_num = 0\n",
    "#         all\n",
    "#         for i, (XI, XB,  y) in enumerate(val_loader):\n",
    "#             if model.header == 'CNN':\n",
    "#                 x = XI\n",
    "#             else:\n",
    "#                 x = XB\n",
    "#             x, y = x.to(device), y.long().to(device)\n",
    "#             if x.size()[0] != batch_size:\n",
    "#                 break\n",
    "#             val_num += x.size(0)\n",
    "#             x_decoded, latent, output = model(x)\n",
    "\n",
    "#             # construct reconstruction loss function for each time step\n",
    "#             for i in range(args.sequence_length):\n",
    "#                 recon_loss = recon_loss_fn(x_decoded, x)\n",
    "#             # cl_loss = cl_loss_fn(output, y)\n",
    "#             recon_loss = recon_loss_fn(x_decoded, x)\n",
    "#             loss = args.w_c*cl_loss + args.w_r *recon_loss\n",
    "            \n",
    "#             # compute classification acc\n",
    "#             pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "#             correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "#             # accumulator\n",
    "#             val_loss += loss.item()\n",
    "        \n",
    "#         # fill stats\n",
    "#         val_accuracy = correct / val_num\n",
    "#         val_loss /= val_num\n",
    "#         epoch_val_loss.append(val_loss)  # only save the last batch\n",
    "#         epoch_val_acc.append(val_accuracy)\n",
    "        \n",
    "#         # if epoch < 20 or epoch%200 == 0:\n",
    "#         print(\"train_num {}, val_num {}\".format(train_num, val_num))\n",
    "#         print('Epoch: {} Loss: train {:.3f}, valid {:.3f}. Accuracy: train: {:.3f}, valid {:.3f}'.format(epoch, train_loss, val_loss, train_accuracy, val_accuracy))\n",
    "        \n",
    "#         # choose model\n",
    "#         if max_val_acc <= val_accuracy:\n",
    "#             model_dir = logDir + model_name + '.pt'\n",
    "#             print('Saving model at {} epoch to {}'.format(epoch, model_dir))\n",
    "#             max_val_acc = val_accuracy\n",
    "#             torch.save(model.state_dict(), model_dir)\n",
    "\n",
    "            \n",
    "#     training_end =  datetime.now()\n",
    "#     training_time = training_end -training_start \n",
    "#     print(\"training takes time {}\".format(training_time))\n",
    "\n",
    "#     model.is_fitted = True\n",
    "#     model.eval()\n",
    "\n",
    "    # TEST after training\n",
    "    correct = 0\n",
    "    test_num = 0\n",
    "    for i, (XI, XB,  y) in enumerate(test_loader):\n",
    "        if model.header == 'CNN':\n",
    "            x = XI\n",
    "        else:\n",
    "            x = XB\n",
    "        x, y = x.to(device), y.long().to(device)\n",
    "        \n",
    "        if x.size(0) != batch_size:\n",
    "            print(\" test batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "            break\n",
    "        test_num += x.size(0)\n",
    "        x_decoded, latent, output = model(x)\n",
    "\n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        \n",
    "        if i == 0:\n",
    "            all_latent = latent\n",
    "            all_y = y\n",
    "            all_pred = pred\n",
    "            all_x = x\n",
    "            all_decoded = x_decoded\n",
    "        else:\n",
    "            all_latent = torch.cat((all_latent, latent), 0)\n",
    "            all_y = torch.cat((all_y, y), 0)\n",
    "            all_pred = torch.cat((all_pred, pred), 0)\n",
    "            all_x = torch.cat((all_x, x), 0)\n",
    "            all_decoded = torch.cat((all_decoded, c_decoded), 0)\n",
    "            \n",
    "    recon_loss = recon_loss_fn(all_x, all_decoded)\n",
    "    print('recon_loss size:', recon_loss.size())\n",
    "    return recon_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rc loss for E1: 40-90-19 no reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 0 kfold number, train with full data, put to devide: cpu\n",
      "training with BioTac dataset...\n",
      "CNNLMU(n_epochs=1,batch_size=192,cuda=False)\n",
      "recon_loss size: torch.Size([])\n",
      " test batch 1 size 8 < 192, skip\n"
     ]
    }
   ],
   "source": [
    "check_timewise_rcls(args, args.sequence_length, args.number_of_features, args.header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recon_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4cc8bfd69705>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'recon_loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(recon_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AE]",
   "language": "python",
   "name": "conda-env-AE-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
