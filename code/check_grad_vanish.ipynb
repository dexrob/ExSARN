{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 0 kfold number, train with full data, put to devide: cpu\n",
      "chop org data of length 400 into 1 segments, each of which is has length 400\n",
      "chop org data of length 400 into 1 segments, each of which is has length 400\n",
      "chop org data of length 400 into 1 segments, each of which is has length 400\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "'''\n",
    "sample command: python T4_BT19_ae.py -k 0 -c 0 -r 1 --data_dir /home/ruihan/data\n",
    "Individual training for BioTac data (full/partial data)\n",
    "if -r=1, train with full data\n",
    "if -r=2, train with half data\n",
    "loss = classification loss + recon loss \n",
    "'''\n",
    "\n",
    "# Import\n",
    "import os,sys\n",
    "import pickle\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vrae.vrae import VRAEC\n",
    "from preprocess_data import get_TrainValTestLoader, get_TrainValTestDataset, get_TrainValTestData\n",
    "from vrae.visual import plot_grad_flow\n",
    "\n",
    "# # Parse argument\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-i\", \"--rep\", type=int, default=0, help='index of running repetition')\n",
    "# parser.add_argument('--data_dir', type=str, default='data', help=\"DIR set in 'gh_download.sh' to store compiled_data\")\n",
    "# parser.add_argument(\"-k\", \"--kfold\", type=int, default=0, help=\"kfold_number for loading data\")\n",
    "# parser.add_argument(\"-r\", \"--reduction\", type=int, default=1, help=\"data reduction ratio for partial training\")\n",
    "# parser.add_argument(\"-c\", \"--cuda\", default=0, help=\"index of cuda gpu to use\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# dummy class to replace argparser, if running jupyter notebook\n",
    "class Args:\n",
    "    reduction = 0\n",
    "    data_dir = 'data'\n",
    "    kfold = 0\n",
    "    cuda = '0'\n",
    "    reduction = 1\n",
    "\n",
    "args=Args()\n",
    "\n",
    "# Set hyper params\n",
    "args_data_dir = args.data_dir\n",
    "kfold_number = args.kfold\n",
    "data_reduction_ratio = args.reduction\n",
    "shuffle = True # set to False for partial training\n",
    "num_class = 20\n",
    "sequence_length = 400\n",
    "number_of_features = 19\n",
    "\n",
    "hidden_size = 90\n",
    "hidden_layer_depth = 1\n",
    "latent_length = 40\n",
    "batch_size = 32\n",
    "learning_rate = 0.001 # 0.0005\n",
    "n_epochs = 100\n",
    "dropout_rate = 0.2\n",
    "cuda = True # options: True, False\n",
    "header = None\n",
    "\n",
    "# loss weightage\n",
    "w_r = 0.01\n",
    "w_c = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Load data\n",
    "data_dir = os.path.join(args_data_dir, \"compiled_data/\")\n",
    "logDir = 'models_and_stats/'\n",
    "if_plot = False\n",
    "\n",
    "# RNN block\n",
    "block = \"phased_LSTM\" # LSTM, GRU, phased_LSTM\n",
    "\n",
    "# model_name = 'BT19_ae_{}_wrI_{}_wC_{}_{}'.format(data_reduction_ratio, w_r, w_c, str(kfold_number))\n",
    "model_name = \"model_\"+block+\"_B\"\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:{}\".format(args.cuda))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "if args.reduction != 1:\n",
    "    print(\"load {} kfold number, reduce data to {} folds, put to device: {}\".format(args.kfold, args.reduction, device))\n",
    "else:\n",
    "    print(\"load {} kfold number, train with full data, put to devide: {}\".format(args.kfold, device))\n",
    "\n",
    "prefix = \"\"\n",
    "dataset_dir = os.path.join(args_data_dir, \"c20/\") # TODO\n",
    "train_set, val_set, test_set = get_TrainValTestDataset(dataset_dir, k=0, prefix=prefix, seq_len=sequence_length)\n",
    "train_loader, val_loader, test_loader = get_TrainValTestLoader(dataset_dir, k=0, batch_size=batch_size,shuffle=shuffle, prefix=prefix,seq_len=sequence_length)\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = get_TrainValTestData(dataset_dir, k=0, prefix=prefix,seq_len=sequence_length)\n",
    "# Initialize models\n",
    "model = VRAEC(num_class=num_class,\n",
    "            block=block,\n",
    "            sequence_length=sequence_length,\n",
    "            number_of_features = number_of_features,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate,\n",
    "            cuda = cuda,\n",
    "            model_name=model_name,\n",
    "            header=header,\n",
    "            device = device)\n",
    "model.to(device)\n",
    "\n",
    "# Initialize training settings\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "cl_loss_fn = nn.NLLLoss()\n",
    "recon_loss_fn = nn.MSELoss()\n",
    "\n",
    "# model.load_state_dict(torch.load('models_and_stats/model_phased_LSTMlstm_B.pt', map_location='cpu'))\n",
    "saved_dicts = torch.load('models_and_stats/models_contain_nan_grad/model_phased_LSTM_B33.pt', map_location='cpu')\n",
    "model.load_state_dict(saved_dicts['model_state_dict'])\n",
    "optimizer.load_state_dict(saved_dicts['optimizer_state_dict'])\n",
    "\n",
    "training_start=datetime.now()\n",
    "# create empty lists to fill stats later\n",
    "epoch_train_loss = []\n",
    "epoch_train_acc = []\n",
    "epoch_val_loss = []\n",
    "epoch_val_acc = []\n",
    "max_val_acc = 0\n",
    "\n",
    "if block == \"phased_LSTM\":\n",
    "    times = torch.ones(batch_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl_loss: tensor(nan, grad_fn=<NllLossBackward>) recon_loss: tensor(nan, grad_fn=<MseLossBackward>)\n",
      "k: tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]]]], grad_fn=<CatBackward>)\n",
      "tau: Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       requires_grad=True)\n",
      "phase: Parameter containing:\n",
      "tensor([4.3579, 1.0059, 3.9284, 2.2026, 1.6147, 1.8327, 5.5256, 0.7834, 2.0463,\n",
      "        0.6080, 0.2822, 0.6000, 0.0391, 0.1464, 0.5916, 3.9748, 0.4988, 0.5253,\n",
      "        1.6675, 0.7863, 0.2739, 2.8919, 0.2172, 0.8328, 0.4840, 3.0377, 1.4090,\n",
      "        0.1706, 1.4264, 0.2346, 2.3351, 1.2407, 1.8235, 1.7531, 1.8264, 4.8685,\n",
      "        1.5669, 0.8452, 1.6010, 3.4169, 0.7335, 0.3655, 0.2062, 0.7669, 0.8432,\n",
      "        1.0561, 1.4645, 2.1673, 0.0534, 1.0374, 1.3027, 0.9339, 4.0773, 0.7087,\n",
      "        1.3104, 0.9358, 0.3599, 0.6380, 2.5711, 0.7495, 0.6246, 0.4843, 0.7930,\n",
      "        0.1423, 1.0167, 0.2031, 1.7839, 0.7762, 0.8769, 0.5366, 0.7869, 2.4897,\n",
      "        0.8787, 1.7472, 1.3667, 0.3119, 2.3740, 2.2957, 0.6830, 0.2749, 0.7797,\n",
      "        1.1204, 0.9850, 0.3341, 0.7986, 0.7178, 1.1555, 0.8661, 1.5925, 0.2110],\n",
      "       requires_grad=True)\n",
      "train_num 32, val_num 192\n",
      "Epoch: 0 Loss: train nan, valid nan. Accuracy: train: 0.031, valid 0.047\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    if epoch > 0:\n",
    "        break\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    train_loss = 0\n",
    "    train_num = 0\n",
    "    for i, (XB,  y) in enumerate(train_loader):\n",
    "        if i > 0:\n",
    "            break\n",
    "        if model.header == 'CNN':\n",
    "            x = XI\n",
    "        else:\n",
    "            x = XB\n",
    "        x, y = x.to(device), y.long().to(device)\n",
    "        if x.size()[0] != batch_size:\n",
    "            break\n",
    "        \n",
    "        # reduce data by data_reduction_ratio times\n",
    "        if i % data_reduction_ratio == 0:\n",
    "            train_num += x.size(0)\n",
    "            optimizer.zero_grad()\n",
    "            if block == \"phased_LSTM\":\n",
    "                x_decoded, latent, output = model(x, times)\n",
    "            else:\n",
    "                x_decoded, latent, output = model(x)\n",
    "\n",
    "            # assert not torch.isnan(y).any(), \"batch_num=\"+str(i)\n",
    "            # print((output == 0).nonzero().size(0)==0)\n",
    "\n",
    "            assert (output == 0).nonzero().size(0)==0, 'output contain zero, batch_num'+str(i)+' indices:'+str((output == 0).nonzero())\n",
    "            \n",
    "            cl_loss = cl_loss_fn(output, y)\n",
    "            recon_loss = recon_loss_fn(x_decoded, x)\n",
    "            loss = w_c*cl_loss + w_r *recon_loss\n",
    "            \n",
    "            # compute classification acc\n",
    "            pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "            # accumulator\n",
    "            train_loss += loss.item()\n",
    "            start_bp = datetime.now()\n",
    "            print('cl_loss:', cl_loss, 'recon_loss:', recon_loss)\n",
    "            loss.backward()\n",
    "            figname = logDir + model_name + \"grad_flow_plot_epoch\" +str(epoch)+\".png\"\n",
    "            if i == 0: # and epoch%50 == 0:\n",
    "#                 for n, p in model.named_parameters():\n",
    "#                     if p.requires_grad:\n",
    "#                         print(n,p.grad)\n",
    "#                 print(\"grad flow for epoch {}\".format(epoch))\n",
    "#                 plot_grad_flow(model.named_parameters(), figname, if_plot=False)\n",
    "                k = model.encoder.k_out\n",
    "                tau = model.encoder.model.phased_cell.tau\n",
    "                phase = model.encoder.model.phased_cell.phase\n",
    "                print('k:', k)\n",
    "                print('tau:', tau)\n",
    "                print('phase:', phase)\n",
    "            optimizer.step()\n",
    "            # print('1 batch bp time:', datetime.now()-start_bp)\n",
    "\n",
    "    # fill stats\n",
    "    train_accuracy = correct / train_num \n",
    "    train_loss /= train_num\n",
    "    epoch_train_loss.append(train_loss)\n",
    "    epoch_train_acc.append(train_accuracy) \n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "    val_num = 0\n",
    "    for i, (XB, y) in enumerate(val_loader):\n",
    "        if model.header == 'CNN':\n",
    "            x = XI\n",
    "        else:\n",
    "            x = XB\n",
    "        x, y = x.to(device), y.long().to(device)\n",
    "        if x.size()[0] != batch_size:\n",
    "            break\n",
    "        val_num += x.size(0)\n",
    "        if block == \"phased_LSTM\":\n",
    "            x_decoded, latent, output = model(x, times)\n",
    "        else:\n",
    "            x_decoded, latent, output = model(x)\n",
    "\n",
    "        # construct loss function\n",
    "        cl_loss = cl_loss_fn(output, y)\n",
    "        recon_loss = recon_loss_fn(x_decoded, x)\n",
    "        loss = w_c*cl_loss + w_r *recon_loss\n",
    "        \n",
    "        # compute classification acc\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "        # accumulator\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    # fill stats\n",
    "    val_accuracy = correct / val_num\n",
    "    val_loss /= val_num\n",
    "    epoch_val_loss.append(val_loss)  # only save the last batch\n",
    "    epoch_val_acc.append(val_accuracy)\n",
    "    \n",
    "    # if epoch < 20 or epoch%200 == 0:\n",
    "    print(\"train_num {}, val_num {}\".format(train_num, val_num))\n",
    "    print('Epoch: {} Loss: train {:.3f}, valid {:.3f}. Accuracy: train: {:.3f}, valid {:.3f}'.format(epoch, train_loss, val_loss, train_accuracy, val_accuracy))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.fc.weight \n",
      " tensor([[ 0.0476, -0.0020,  0.0085,  ..., -0.0071,  0.0072,  0.0087],\n",
      "        [-0.0559, -0.0111, -0.0030,  ..., -0.0050, -0.0098, -0.0135],\n",
      "        [ 0.1894,  0.0213,  0.0165,  ...,  0.0282,  0.0224,  0.0381],\n",
      "        ...,\n",
      "        [-0.0744, -0.0036, -0.0090,  ..., -0.0238, -0.0055, -0.0157],\n",
      "        [-0.0915, -0.0070, -0.0116,  ...,  0.0034, -0.0177, -0.0224],\n",
      "        [ 0.1446,  0.0138,  0.0299,  ...,  0.0080,  0.0271,  0.0369]]) \n",
      " tensor([[ 0.0962, -0.1339, -0.0391,  ...,  0.0079, -0.0035,  0.1963],\n",
      "        [ 0.0472, -0.1132, -0.1328,  ..., -0.1675,  0.1865, -0.0610],\n",
      "        [ 0.0998, -0.0946,  0.2460,  ...,  0.1904,  0.1961, -0.0175],\n",
      "        ...,\n",
      "        [-0.0637,  0.1104,  0.0194,  ...,  0.2182, -0.1952, -0.0084],\n",
      "        [-0.1803, -0.0063, -0.0896,  ...,  0.2106, -0.0826, -0.1429],\n",
      "        [ 0.0562, -0.0702, -0.2841,  ..., -0.0558, -0.0312,  0.0923]])\n",
      "encoder.fc.bias \n",
      " tensor([ 0.0039, -0.0025,  0.0049,  0.0014, -0.0234, -0.0361, -0.0280,  0.0021,\n",
      "        -0.0089,  0.0296,  0.0081,  0.0059,  0.0094, -0.0102,  0.0002,  0.0083,\n",
      "         0.0407, -0.0137,  0.0050,  0.0206, -0.0018, -0.0298,  0.0125,  0.0128,\n",
      "         0.0059,  0.0152, -0.0136,  0.0078,  0.0069,  0.0208,  0.0171, -0.0016,\n",
      "        -0.0084,  0.0097, -0.0260, -0.0191,  0.0223,  0.0173,  0.0431, -0.0380,\n",
      "        -0.0053,  0.0232, -0.0072,  0.0134,  0.0196,  0.0116, -0.0037,  0.0008,\n",
      "        -0.0078,  0.0066,  0.0270,  0.0036, -0.0121,  0.0079, -0.0097, -0.0184,\n",
      "         0.0015, -0.0627,  0.0087, -0.0218,  0.0108,  0.0086, -0.0066,  0.0153,\n",
      "        -0.0030, -0.0151,  0.0250, -0.0138,  0.0014, -0.0022, -0.0124,  0.0081,\n",
      "        -0.0021, -0.0230,  0.0017, -0.0097, -0.0393, -0.0253, -0.0177, -0.0054,\n",
      "        -0.0147, -0.0113,  0.0225,  0.0049,  0.0296, -0.0477, -0.0357, -0.0034,\n",
      "        -0.0127,  0.0130]) \n",
      " tensor([-0.0285, -0.0212,  0.1772,  0.2459, -0.1234,  0.0561,  0.2176, -0.0365,\n",
      "         0.0627,  0.2395,  0.1815,  0.0552,  0.1856, -0.2574,  0.0532,  0.0166,\n",
      "        -0.0995, -0.0865,  0.0266, -0.3288, -0.1944,  0.2062, -0.1135, -0.1345,\n",
      "         0.1219, -0.1183, -0.0174, -0.2264,  0.1459, -0.2859, -0.0061, -0.1748,\n",
      "        -0.2199,  0.0320, -0.0224,  0.0277, -0.0372,  0.2828, -0.0404,  0.1501,\n",
      "        -0.0680, -0.0966,  0.2418,  0.1525, -0.0056, -0.1687, -0.1424,  0.0871,\n",
      "        -0.1257,  0.0340,  0.1348, -0.0984, -0.1706,  0.2313, -0.0745, -0.1066,\n",
      "        -0.1538,  0.1584,  0.1424,  0.2333,  0.2512,  0.2105, -0.0033,  0.1422,\n",
      "        -0.1130, -0.0451,  0.0626, -0.0533,  0.0281,  0.1965,  0.1869, -0.1140,\n",
      "        -0.1331,  0.2190,  0.1243, -0.1430,  0.0055,  0.1234,  0.1191,  0.0115,\n",
      "        -0.0581,  0.2381, -0.1784,  0.1538, -0.0867,  0.1823,  0.1232,  0.0078,\n",
      "         0.2004, -0.0393])\n",
      "encoder.model.lstm.weight_ih \n",
      " tensor([[ 2.4219e-06, -1.0941e-07,  1.0185e-06,  ..., -3.3926e-07,\n",
      "         -2.2822e-06, -1.3097e-06],\n",
      "        [-8.5877e-03, -3.1957e-03, -8.9373e-04,  ...,  2.1916e-03,\n",
      "          8.9729e-04,  1.2262e-02],\n",
      "        [ 1.4367e-04,  1.5600e-04, -2.7746e-05,  ..., -4.0364e-04,\n",
      "         -2.6214e-04, -6.7813e-05],\n",
      "        ...,\n",
      "        [-7.8378e-04,  9.1242e-04, -6.7179e-04,  ...,  2.7660e-04,\n",
      "          2.2828e-03,  1.3205e-03],\n",
      "        [-1.4037e-08,  1.0978e-04, -6.7454e-05,  ..., -1.0286e-04,\n",
      "         -4.6913e-05,  9.9874e-06],\n",
      "        [ 1.9157e-03, -1.3337e-03,  2.8291e-03,  ...,  7.2179e-04,\n",
      "         -1.6079e-03, -4.9010e-03]]) \n",
      " tensor([[ 0.1427,  0.0769, -0.0640,  ..., -0.1496, -0.1296,  0.0065],\n",
      "        [ 0.0513, -0.0504,  0.0156,  ...,  0.0343, -0.1551,  0.0402],\n",
      "        [-0.0789,  0.0428,  0.0417,  ...,  0.0525, -0.1406, -0.1218],\n",
      "        ...,\n",
      "        [ 0.1175,  0.0974,  0.0755,  ...,  0.0796, -0.0170,  0.0797],\n",
      "        [-0.0577,  0.0640, -0.0846,  ..., -0.1857, -0.1826,  0.1997],\n",
      "        [ 0.0678, -0.0814,  0.0281,  ..., -0.0506, -0.0532,  0.0340]])\n",
      "encoder.model.lstm.weight_hh \n",
      " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) \n",
      " tensor([[-0.0277,  0.0690, -0.0019,  ..., -0.0893,  0.1323,  0.2602],\n",
      "        [-0.0913, -0.0282, -0.1730,  ..., -0.1810,  0.1666,  0.1863],\n",
      "        [-0.3537, -0.0407, -0.1014,  ..., -0.1358,  0.0914,  0.3833],\n",
      "        ...,\n",
      "        [-0.1189,  0.0666, -0.1348,  ...,  0.1546,  0.1117,  0.3609],\n",
      "        [ 0.1062,  0.0296,  0.1391,  ..., -0.2351,  0.2877, -0.0165],\n",
      "        [-0.2694,  0.2976, -0.5356,  ...,  0.0063,  0.0100,  0.4059]])\n",
      "encoder.model.lstm.bias_ih \n",
      " tensor([ 2.5099e-06, -4.6482e-03,  2.1420e-04,  1.7041e-05,  6.1265e-04,\n",
      "         4.0093e-04, -6.1882e-05, -1.4776e-06, -1.5122e-05,  9.6755e-04,\n",
      "         4.5055e-05,  4.1561e-06, -1.1401e-03, -3.5111e-04,  1.5627e-04,\n",
      "         9.6029e-04,  1.0881e-06,  5.3144e-05,  4.2015e-06, -1.5244e-06,\n",
      "        -7.7649e-06, -1.4794e-05,  5.3385e-03,  9.3312e-07, -8.2265e-04,\n",
      "         5.2767e-04,  1.4882e-05, -7.3974e-06,  8.8071e-06,  2.1368e-03,\n",
      "        -7.2695e-05, -1.2359e-04,  1.8693e-03, -2.1407e-04, -2.3648e-05,\n",
      "         1.0819e-05,  9.9644e-05,  4.7104e-04,  1.3726e-04,  1.8083e-05,\n",
      "         1.9331e-06,  4.7122e-04, -2.5881e-05, -5.3877e-04,  1.1779e-04,\n",
      "        -3.2052e-02,  8.7075e-06,  4.2327e-06, -3.1276e-06, -1.0905e-02,\n",
      "         9.2741e-06, -8.8829e-03,  2.5102e-04, -1.5399e-04, -1.4216e-05,\n",
      "        -1.9495e-03,  1.5924e-06,  4.8195e-05, -1.0126e-04,  1.3276e-04,\n",
      "         2.6507e-03, -6.0874e-04,  1.7008e-06,  4.0344e-04, -2.0421e-03,\n",
      "         7.9556e-05, -1.9663e-03, -1.7227e-06, -5.7024e-02,  2.2559e-04,\n",
      "         3.5005e-04, -4.4010e-07,  1.4881e-04,  1.6248e-04,  1.7469e-04,\n",
      "        -1.0737e-05, -1.0544e-05,  5.9710e-04,  2.9574e-07,  1.4591e-07,\n",
      "         2.1273e-03,  3.8992e-04, -3.4025e-02,  9.4572e-06,  3.2992e-03,\n",
      "         1.7241e-04,  5.1372e-06,  5.6483e-03,  6.8332e-04, -2.3248e-04,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -8.7247e-03, -5.8850e-02, -6.8814e-03,  1.0622e-03, -2.1629e-03,\n",
      "        -1.2601e-03,  6.4341e-04,  2.1444e-05, -7.4767e-03, -1.4638e-04,\n",
      "        -2.1852e-04,  1.7023e-03, -2.7727e-03,  9.1401e-02, -3.4998e-03,\n",
      "        -1.3559e-03, -6.4277e-05, -1.0760e-03, -5.9971e-05,  2.5397e-04,\n",
      "         1.3769e-02,  1.1582e-05,  7.7476e-05, -2.7917e-05,  6.5175e-04,\n",
      "        -7.0360e-07, -1.6761e-03,  3.7122e-06, -3.8655e-04,  1.9597e-03,\n",
      "         2.3332e-04,  1.2363e-05, -2.2616e-06,  3.6852e-06, -2.5893e-03,\n",
      "         8.7761e-03,  2.5574e-05,  1.1162e-01,  3.5025e-05, -1.5143e-03,\n",
      "        -4.7611e-05, -2.5377e-04, -2.4654e-06,  6.0318e-06,  6.1928e-04,\n",
      "         3.5992e-02,  6.6995e-04,  2.6580e-05,  1.6720e-05,  7.3313e-03,\n",
      "        -2.5738e-03, -3.4099e-03,  8.0092e-03,  2.6251e-02,  1.8569e-02,\n",
      "        -2.2126e-02,  2.4015e-03, -1.4831e-03,  7.8234e-06,  1.2338e-05,\n",
      "         1.5356e-05, -4.8142e-05, -4.6145e-05,  5.4544e-06, -2.4588e-03,\n",
      "        -2.6332e-04,  1.4606e-04, -5.5125e-05,  1.5503e-03,  1.0102e-04,\n",
      "         2.5021e-03, -2.5049e-04,  1.7974e-05, -2.4196e-04,  5.4226e-06,\n",
      "        -9.0976e-03,  1.1656e-03, -3.1035e-06, -4.9574e-06, -7.2196e-07,\n",
      "         2.0534e-02, -1.1913e-04,  1.0463e-02,  7.0538e-05, -1.7291e-02,\n",
      "        -2.6775e-04, -7.2937e-04, -5.4450e-02,  8.4960e-04, -3.7038e-04,\n",
      "        -1.3099e-04, -1.0831e-03,  2.9085e-05,  1.2786e-05,  8.2435e-06,\n",
      "         2.1291e-03, -5.4942e-03, -1.2920e-04, -3.7700e-05, -1.8070e-05,\n",
      "         1.2776e-03,  2.7160e-05, -2.7708e-03,  1.4458e-03,  5.1028e-04,\n",
      "         1.1082e-03,  6.7118e-05, -9.9389e-06,  4.1457e-04,  1.1679e-03,\n",
      "        -6.6266e-06,  2.5337e-04,  1.1880e-04,  9.5276e-07, -5.5580e-07,\n",
      "        -1.0773e-05,  1.6937e-05,  1.0101e-04,  8.3388e-05, -7.7656e-06,\n",
      "         2.3069e-04,  1.0450e-06,  7.1668e-05, -1.2630e-04, -1.5828e-04,\n",
      "         6.4102e-06,  4.9807e-06,  2.0075e-02,  1.9005e-04,  8.7323e-04,\n",
      "         2.4718e-03,  3.5861e-02, -3.1785e-03, -8.4113e-04,  3.5212e-05,\n",
      "        -5.7100e-04,  9.7278e-05,  2.4220e-06,  1.9880e-04, -2.6519e-06,\n",
      "         2.3065e-05, -4.7262e-02,  9.3857e-05,  1.5746e-02, -9.3195e-06,\n",
      "        -3.2476e-04,  1.2281e-07,  9.8316e-06, -3.3814e-04,  7.2719e-04,\n",
      "        -1.8056e-05, -1.1688e-06,  2.2050e-06,  7.4339e-04, -1.8347e-02,\n",
      "         2.4179e-05,  1.2645e-06, -2.9248e-05, -3.2315e-03,  1.4505e-05,\n",
      "         2.5482e-06, -5.4709e-05,  2.8925e-04,  1.1306e-03,  7.3052e-04,\n",
      "        -2.6649e-05, -2.3274e-04,  7.8727e-06,  1.1498e-06, -1.4433e-03,\n",
      "         2.0657e-04,  1.1995e-03, -6.7695e-04, -1.2563e-03,  8.0161e-04,\n",
      "         1.5593e-03,  3.8220e-05, -3.1866e-03,  7.1493e-05, -5.7537e-04]) \n",
      " tensor([ 0.0581, -0.0003,  0.1396,  0.0615,  0.1693, -0.0522,  0.0924,  0.1105,\n",
      "         0.0559,  0.0804,  0.1310, -0.0230,  0.0107, -0.0842,  0.1408, -0.0946,\n",
      "         0.0047,  0.0046,  0.0708,  0.1372, -0.0519,  0.0808, -0.0498, -0.0331,\n",
      "         0.1473,  0.1099,  0.0958,  0.0544,  0.0973, -0.2101,  0.1080,  0.0451,\n",
      "        -0.0667,  0.1134, -0.0464, -0.1200,  0.1675,  0.0888,  0.0132,  0.0067,\n",
      "         0.1410,  0.0836,  0.0835,  0.0573, -0.0449,  0.2250, -0.0394,  0.0712,\n",
      "         0.1616,  0.1373, -0.0061,  0.1119,  0.0702,  0.1038, -0.0713,  0.1135,\n",
      "         0.1171, -0.0544,  0.0282,  0.1524, -0.0837,  0.0802,  0.0441,  0.0739,\n",
      "        -0.0830,  0.0354,  0.2089,  0.0752,  0.0075, -0.0723,  0.0382,  0.0062,\n",
      "        -0.0268, -0.0955,  0.0877,  0.1505,  0.1504, -0.0533,  0.1529,  0.0299,\n",
      "        -0.0074,  0.1534,  0.1111,  0.1370,  0.1459,  0.0784, -0.0501,  0.0257,\n",
      "        -0.0143,  0.0341,  0.1946, -0.1064,  0.2043,  0.0191,  0.1540,  0.0213,\n",
      "         0.0761,  0.1238, -0.0661,  0.2598,  0.0063, -0.0914, -0.0111,  0.0054,\n",
      "        -0.0656, -0.0540, -0.0683,  0.0286,  0.0678,  0.0329, -0.0757, -0.0159,\n",
      "         0.2330, -0.0516,  0.1856,  0.1295, -0.1816, -0.0842, -0.0508, -0.0778,\n",
      "         0.0649,  0.1528,  0.1433, -0.0170,  0.2012,  0.0529,  0.0356,  0.0758,\n",
      "         0.0600,  0.0657,  0.0107, -0.0467,  0.0381,  0.1295, -0.0346,  0.2829,\n",
      "        -0.0910,  0.1629,  0.0458,  0.1361,  0.0520,  0.0928,  0.0318,  0.0783,\n",
      "         0.0232,  0.0744,  0.1870, -0.0715,  0.0242,  0.0198,  0.1363, -0.0179,\n",
      "         0.1469, -0.0800, -0.1002,  0.1060, -0.1134,  0.1131,  0.0405,  0.0464,\n",
      "         0.0693,  0.1452, -0.0523, -0.2025,  0.2061, -0.0180, -0.0926,  0.0638,\n",
      "         0.1499, -0.0186, -0.0076,  0.0863,  0.1786,  0.0213,  0.0299, -0.0301,\n",
      "         0.0563,  0.1131, -0.0523,  0.0222,  0.1625,  0.1605, -0.1075, -0.0938,\n",
      "         0.0114,  0.1510, -0.1017, -0.0971,  0.1735, -0.1200,  0.0483,  0.0777,\n",
      "         0.0726, -0.1034,  0.0658,  0.1105,  0.0390,  0.0546, -0.0308,  0.0454,\n",
      "        -0.2006, -0.0063,  0.2065, -0.0183,  0.0274,  0.0853,  0.1134,  0.1369,\n",
      "         0.0380, -0.0497, -0.0902, -0.1320, -0.1189,  0.1468,  0.0351,  0.0452,\n",
      "         0.1021, -0.1818, -0.0428,  0.0467,  0.0152,  0.0826,  0.0285, -0.1628,\n",
      "        -0.0104,  0.0674,  0.0063, -0.0443, -0.0312, -0.0675,  0.0689,  0.1183,\n",
      "         0.1059, -0.0577, -0.0124,  0.1587,  0.0081,  0.0151, -0.1602,  0.0574,\n",
      "        -0.0405, -0.0019, -0.0958,  0.1441,  0.1266,  0.1379, -0.0074, -0.0478,\n",
      "         0.0275,  0.0573, -0.1614, -0.0068, -0.0914,  0.0795,  0.0939, -0.0516,\n",
      "         0.0146, -0.0844, -0.1286, -0.0056,  0.0559, -0.1111, -0.0581, -0.1289,\n",
      "         0.1680,  0.0031, -0.0770, -0.0554, -0.0711, -0.0408, -0.0231,  0.0724,\n",
      "         0.2825,  0.1611, -0.0792, -0.0145,  0.0829, -0.0150,  0.0642,  0.1598,\n",
      "        -0.0124, -0.0181,  0.0714, -0.0256,  0.1460,  0.0447,  0.0528, -0.0530,\n",
      "         0.0953, -0.0265, -0.0323,  0.0836,  0.1183,  0.0048,  0.2090,  0.1477,\n",
      "        -0.0737,  0.1473, -0.0212, -0.1222, -0.0204, -0.0399,  0.0308,  0.0012,\n",
      "         0.0439,  0.0509,  0.2270,  0.1356,  0.0828,  0.1759,  0.0853, -0.0068,\n",
      "         0.0482,  0.0937,  0.1278,  0.1349,  0.1239,  0.0667, -0.0103,  0.1808,\n",
      "        -0.1295,  0.2805,  0.1015,  0.0395, -0.0682,  0.1226,  0.0760, -0.0630,\n",
      "         0.0570,  0.2162, -0.0205,  0.2237,  0.0815, -0.0594,  0.2563,  0.0711,\n",
      "         0.1873,  0.0559,  0.1031, -0.1604,  0.1125,  0.1012, -0.1142, -0.1570,\n",
      "        -0.0120,  0.1888,  0.0070,  0.0741,  0.1843,  0.1385,  0.1378,  0.0799,\n",
      "         0.1650,  0.0205,  0.0093, -0.0027,  0.1862,  0.1945,  0.0531,  0.1216])\n",
      "encoder.model.lstm.bias_hh \n",
      " tensor([ 2.5099e-06, -4.6482e-03,  2.1420e-04,  1.7041e-05,  6.1265e-04,\n",
      "         4.0093e-04, -6.1882e-05, -1.4776e-06, -1.5122e-05,  9.6755e-04,\n",
      "         4.5055e-05,  4.1561e-06, -1.1401e-03, -3.5111e-04,  1.5627e-04,\n",
      "         9.6029e-04,  1.0881e-06,  5.3144e-05,  4.2015e-06, -1.5244e-06,\n",
      "        -7.7649e-06, -1.4794e-05,  5.3385e-03,  9.3312e-07, -8.2265e-04,\n",
      "         5.2767e-04,  1.4882e-05, -7.3974e-06,  8.8071e-06,  2.1368e-03,\n",
      "        -7.2695e-05, -1.2359e-04,  1.8693e-03, -2.1407e-04, -2.3648e-05,\n",
      "         1.0819e-05,  9.9644e-05,  4.7104e-04,  1.3726e-04,  1.8083e-05,\n",
      "         1.9331e-06,  4.7122e-04, -2.5881e-05, -5.3877e-04,  1.1779e-04,\n",
      "        -3.2052e-02,  8.7075e-06,  4.2327e-06, -3.1276e-06, -1.0905e-02,\n",
      "         9.2741e-06, -8.8829e-03,  2.5102e-04, -1.5399e-04, -1.4216e-05,\n",
      "        -1.9495e-03,  1.5924e-06,  4.8195e-05, -1.0126e-04,  1.3276e-04,\n",
      "         2.6507e-03, -6.0874e-04,  1.7008e-06,  4.0344e-04, -2.0421e-03,\n",
      "         7.9556e-05, -1.9663e-03, -1.7227e-06, -5.7024e-02,  2.2559e-04,\n",
      "         3.5005e-04, -4.4010e-07,  1.4881e-04,  1.6248e-04,  1.7469e-04,\n",
      "        -1.0737e-05, -1.0544e-05,  5.9710e-04,  2.9574e-07,  1.4591e-07,\n",
      "         2.1273e-03,  3.8992e-04, -3.4025e-02,  9.4572e-06,  3.2992e-03,\n",
      "         1.7241e-04,  5.1372e-06,  5.6483e-03,  6.8332e-04, -2.3248e-04,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -8.7247e-03, -5.8850e-02, -6.8814e-03,  1.0622e-03, -2.1629e-03,\n",
      "        -1.2601e-03,  6.4341e-04,  2.1444e-05, -7.4767e-03, -1.4638e-04,\n",
      "        -2.1852e-04,  1.7023e-03, -2.7727e-03,  9.1401e-02, -3.4998e-03,\n",
      "        -1.3559e-03, -6.4277e-05, -1.0760e-03, -5.9971e-05,  2.5397e-04,\n",
      "         1.3769e-02,  1.1582e-05,  7.7476e-05, -2.7917e-05,  6.5175e-04,\n",
      "        -7.0360e-07, -1.6761e-03,  3.7122e-06, -3.8655e-04,  1.9597e-03,\n",
      "         2.3332e-04,  1.2363e-05, -2.2616e-06,  3.6852e-06, -2.5893e-03,\n",
      "         8.7761e-03,  2.5574e-05,  1.1162e-01,  3.5025e-05, -1.5143e-03,\n",
      "        -4.7611e-05, -2.5377e-04, -2.4654e-06,  6.0318e-06,  6.1928e-04,\n",
      "         3.5992e-02,  6.6995e-04,  2.6580e-05,  1.6720e-05,  7.3313e-03,\n",
      "        -2.5738e-03, -3.4099e-03,  8.0092e-03,  2.6251e-02,  1.8569e-02,\n",
      "        -2.2126e-02,  2.4015e-03, -1.4831e-03,  7.8234e-06,  1.2338e-05,\n",
      "         1.5356e-05, -4.8142e-05, -4.6145e-05,  5.4544e-06, -2.4588e-03,\n",
      "        -2.6332e-04,  1.4606e-04, -5.5125e-05,  1.5503e-03,  1.0102e-04,\n",
      "         2.5021e-03, -2.5049e-04,  1.7974e-05, -2.4196e-04,  5.4226e-06,\n",
      "        -9.0976e-03,  1.1656e-03, -3.1035e-06, -4.9574e-06, -7.2196e-07,\n",
      "         2.0534e-02, -1.1913e-04,  1.0463e-02,  7.0538e-05, -1.7291e-02,\n",
      "        -2.6775e-04, -7.2937e-04, -5.4450e-02,  8.4960e-04, -3.7038e-04,\n",
      "        -1.3099e-04, -1.0831e-03,  2.9085e-05,  1.2786e-05,  8.2435e-06,\n",
      "         2.1291e-03, -5.4942e-03, -1.2920e-04, -3.7700e-05, -1.8070e-05,\n",
      "         1.2776e-03,  2.7160e-05, -2.7708e-03,  1.4458e-03,  5.1028e-04,\n",
      "         1.1082e-03,  6.7118e-05, -9.9389e-06,  4.1457e-04,  1.1679e-03,\n",
      "        -6.6266e-06,  2.5337e-04,  1.1880e-04,  9.5276e-07, -5.5580e-07,\n",
      "        -1.0773e-05,  1.6937e-05,  1.0101e-04,  8.3388e-05, -7.7656e-06,\n",
      "         2.3069e-04,  1.0450e-06,  7.1668e-05, -1.2630e-04, -1.5828e-04,\n",
      "         6.4102e-06,  4.9807e-06,  2.0075e-02,  1.9005e-04,  8.7323e-04,\n",
      "         2.4718e-03,  3.5861e-02, -3.1785e-03, -8.4113e-04,  3.5212e-05,\n",
      "        -5.7100e-04,  9.7278e-05,  2.4220e-06,  1.9880e-04, -2.6519e-06,\n",
      "         2.3065e-05, -4.7262e-02,  9.3857e-05,  1.5746e-02, -9.3195e-06,\n",
      "        -3.2476e-04,  1.2281e-07,  9.8316e-06, -3.3814e-04,  7.2719e-04,\n",
      "        -1.8056e-05, -1.1688e-06,  2.2050e-06,  7.4339e-04, -1.8347e-02,\n",
      "         2.4179e-05,  1.2645e-06, -2.9248e-05, -3.2315e-03,  1.4505e-05,\n",
      "         2.5482e-06, -5.4709e-05,  2.8925e-04,  1.1306e-03,  7.3052e-04,\n",
      "        -2.6649e-05, -2.3274e-04,  7.8727e-06,  1.1498e-06, -1.4433e-03,\n",
      "         2.0657e-04,  1.1995e-03, -6.7695e-04, -1.2563e-03,  8.0161e-04,\n",
      "         1.5593e-03,  3.8220e-05, -3.1866e-03,  7.1493e-05, -5.7537e-04]) \n",
      " tensor([ 1.3122e-01,  3.7782e-02,  1.2110e-01,  1.0197e-01,  9.8771e-02,\n",
      "         9.5707e-02,  9.1039e-03,  8.7605e-02,  1.7789e-01,  6.1341e-02,\n",
      "         4.8419e-02, -3.2220e-02, -2.8017e-02, -1.5354e-02, -3.9855e-02,\n",
      "        -4.9662e-02,  5.0248e-02,  1.4887e-01,  1.8132e-01,  7.5302e-02,\n",
      "         4.5753e-02, -3.0441e-02,  7.4211e-02,  4.1827e-02,  2.1640e-01,\n",
      "        -3.6059e-02,  1.1874e-01,  9.8943e-02,  1.3833e-01, -8.5742e-02,\n",
      "         3.3745e-02,  9.6594e-02, -1.0234e-01,  2.8988e-02,  5.0288e-02,\n",
      "        -1.1001e-01,  9.6444e-03,  9.6314e-02, -1.0638e-02,  1.8029e-01,\n",
      "         1.5912e-01, -5.4715e-03,  1.2075e-01, -6.4690e-02, -1.1393e-01,\n",
      "         2.0564e-01,  4.3511e-02,  8.7804e-02,  7.5623e-02,  8.7166e-02,\n",
      "        -6.1968e-03,  6.4473e-02,  5.6951e-02,  1.6073e-01,  5.5568e-02,\n",
      "         2.1234e-01,  9.6402e-02,  6.7358e-02,  1.3906e-01,  5.7069e-02,\n",
      "        -1.3767e-02,  3.6449e-02,  7.8290e-02,  6.3965e-02, -1.6562e-01,\n",
      "         5.9827e-02,  6.7114e-02,  6.1021e-02,  1.7182e-01,  1.0739e-01,\n",
      "         6.6990e-02,  7.1234e-02, -7.1290e-02, -6.0390e-02,  1.4601e-01,\n",
      "         6.8558e-03,  3.3133e-02, -3.2366e-02,  2.0744e-02,  2.8652e-02,\n",
      "        -7.0853e-02,  1.5511e-03,  5.8866e-02,  2.9018e-02,  1.0095e-01,\n",
      "        -2.7449e-04,  1.2044e-01,  5.0375e-02,  4.6833e-02,  7.0019e-02,\n",
      "         1.4546e-01, -1.5388e-03,  8.3806e-02,  1.3105e-01,  5.1005e-02,\n",
      "        -3.0287e-02, -1.4749e-02,  6.3205e-02,  4.3584e-02,  2.9999e-01,\n",
      "        -7.9127e-02,  2.3001e-02, -7.2141e-02,  1.7390e-02,  2.4071e-02,\n",
      "         1.1972e-01,  3.8231e-02, -5.2061e-02,  5.4552e-02, -9.1671e-03,\n",
      "        -1.5593e-01,  1.0968e-01,  2.5909e-01, -3.3163e-02,  4.1765e-02,\n",
      "         1.9459e-01, -1.5546e-01, -4.2719e-02,  6.9500e-02, -3.6360e-02,\n",
      "         4.4430e-03, -1.6453e-02, -1.4053e-02,  9.6766e-02,  1.8736e-01,\n",
      "         1.0779e-01,  8.8159e-03,  5.6783e-02,  2.9522e-03,  1.4572e-02,\n",
      "         5.7998e-02,  1.0747e-01,  1.2007e-01,  9.7132e-02, -1.1300e-01,\n",
      "         2.4171e-01, -6.1014e-03,  1.3275e-01,  1.0610e-01,  1.0326e-01,\n",
      "         3.0624e-02,  2.1938e-02,  6.2068e-02,  9.0969e-02,  1.4640e-01,\n",
      "         8.4453e-02,  1.3084e-01, -3.7691e-02,  5.8407e-02,  1.1673e-01,\n",
      "         2.9362e-02,  8.5592e-02,  2.6775e-02, -1.5289e-02,  7.8056e-02,\n",
      "         9.7714e-02,  5.5401e-02,  1.2546e-01,  1.5758e-02, -1.0232e-01,\n",
      "         3.3721e-02,  1.6836e-01, -1.2090e-01, -1.1126e-01,  1.6768e-01,\n",
      "         2.2145e-02, -7.7694e-02,  8.1864e-02,  1.6868e-01, -3.9532e-03,\n",
      "        -1.1820e-02,  2.1481e-01,  3.2495e-02,  1.3033e-02,  2.5900e-02,\n",
      "         4.6709e-02,  1.6402e-01,  1.4143e-01, -1.5483e-02, -2.7417e-03,\n",
      "         1.3005e-01,  8.9931e-02,  7.0575e-02,  3.8258e-02, -6.5598e-02,\n",
      "         5.5998e-02, -7.6745e-02, -1.1561e-01,  5.7171e-02, -9.8205e-02,\n",
      "        -7.3918e-03,  1.0738e-01,  1.5397e-01,  2.7618e-02,  1.2250e-01,\n",
      "        -1.4822e-03, -7.6172e-02,  2.5562e-02,  1.1616e-02,  8.9192e-02,\n",
      "        -1.8850e-01, -1.1912e-01,  4.2071e-02,  3.2151e-02,  4.2758e-02,\n",
      "         1.9599e-01,  4.8672e-02,  1.4674e-02, -3.7916e-02, -1.6986e-01,\n",
      "         8.2369e-02, -6.3773e-02, -1.3197e-01, -2.3244e-02,  7.9097e-02,\n",
      "         1.0059e-01, -1.2984e-02, -1.4413e-01, -9.6270e-02, -1.2441e-01,\n",
      "        -8.6226e-02,  1.4175e-01,  1.2064e-01, -3.4579e-02, -2.0210e-02,\n",
      "        -1.2509e-02,  6.7560e-03, -6.2711e-02,  2.8192e-02, -1.5531e-01,\n",
      "         5.7071e-03,  1.2880e-01,  5.6338e-02, -4.5813e-02, -2.9021e-02,\n",
      "         2.6060e-02,  9.4839e-02,  8.0876e-02, -9.7707e-02, -3.3248e-02,\n",
      "         3.8854e-02, -1.0531e-01, -1.5580e-01,  6.5921e-02,  9.8714e-03,\n",
      "        -3.2509e-02, -1.7033e-02,  1.7096e-02,  2.1396e-02, -7.7378e-02,\n",
      "         3.5294e-02, -1.0010e-01, -2.1239e-02,  2.7422e-02,  9.8125e-02,\n",
      "        -3.8796e-02,  8.6447e-02, -1.1215e-01, -4.2246e-02, -5.2670e-02,\n",
      "        -1.1191e-01, -1.2386e-01, -8.5927e-02, -1.5419e-01,  4.6165e-02,\n",
      "        -1.2703e-02,  1.0655e-01,  7.0685e-02, -1.4539e-01,  5.9490e-02,\n",
      "         4.7347e-02,  2.2948e-01,  1.9459e-01,  7.6947e-02,  5.3841e-03,\n",
      "         5.3033e-03,  2.0784e-01,  5.9514e-02,  1.2523e-01,  2.0411e-01,\n",
      "        -4.9922e-02,  4.4181e-02, -1.6665e-02, -7.3309e-02,  4.3279e-02,\n",
      "        -1.1043e-01,  4.5822e-02,  2.7324e-02,  1.6038e-01,  1.1093e-01,\n",
      "         4.9824e-02,  1.5409e-01,  1.8774e-01,  1.0723e-01,  6.8714e-02,\n",
      "         7.9084e-02, -9.6569e-03,  3.3901e-02, -5.4664e-03, -6.4679e-02,\n",
      "        -1.3376e-01, -1.5128e-02,  7.9935e-02,  3.3660e-02,  1.5903e-01,\n",
      "         9.7124e-02,  2.0432e-01,  2.5422e-01, -1.4229e-02,  1.6939e-01,\n",
      "         3.3635e-02, -7.5892e-04,  6.7589e-02,  1.4659e-01,  6.9391e-02,\n",
      "         2.7960e-02,  7.6993e-02,  1.4724e-01, -1.9734e-02,  2.2881e-01,\n",
      "         4.3015e-02,  1.2605e-01, -3.0394e-02,  4.8116e-02, -4.8238e-02,\n",
      "         2.7536e-01,  1.8046e-01, -7.0175e-02,  4.8468e-02,  7.5884e-02,\n",
      "         1.0484e-01,  2.0151e-01,  5.1627e-02,  2.5406e-02,  2.0293e-01,\n",
      "        -3.1440e-02,  1.4479e-01,  1.5116e-01,  1.9289e-01,  1.4303e-03,\n",
      "         1.1078e-01,  8.9480e-02, -2.6287e-02, -3.5544e-02, -5.8673e-02,\n",
      "         1.7092e-01,  2.0876e-02,  1.8628e-01,  2.6970e-02,  1.3629e-01,\n",
      "         2.6928e-02,  1.4948e-01,  1.5790e-01,  9.5643e-02,  5.7323e-02,\n",
      "        -3.2828e-02,  1.4045e-02,  1.1264e-01,  3.7695e-02,  2.1264e-02])\n",
      "encoder.model.phased_cell.tau \n",
      " tensor([ 4.2263e-03,  5.4018e-03,  3.9905e-03, -1.4396e-03, -4.1017e-03,\n",
      "        -8.0595e-03,  3.3825e-03,  7.3829e-04, -1.8129e-03,  1.3532e-03,\n",
      "        -4.4335e-05, -8.9492e-04, -1.8190e-03,  7.1178e-03, -6.3958e-02,\n",
      "        -8.7146e-03,  3.2025e-03, -1.8245e-03, -1.1412e-02, -2.7868e-03,\n",
      "         6.0014e-03,  1.4759e-04, -1.1629e-02,  1.0452e-03,  1.0100e-03,\n",
      "        -5.8023e-05, -3.4300e-02, -1.9547e-06,  7.2212e-04, -3.8697e-03,\n",
      "         3.1571e-04,  8.6830e-04, -3.4173e-03,  2.3589e-03,  4.3899e-03,\n",
      "         2.0010e-03,  3.1282e-03,  3.6147e-03,  7.3142e-04,  1.2757e-05,\n",
      "        -1.5260e-02, -1.0583e-04,  1.3785e-02,  7.0847e-03, -3.4420e-02,\n",
      "        -6.4344e-03, -9.1151e-03,  4.4134e-04, -8.4867e-04, -3.6736e-03,\n",
      "        -3.3785e-04,  3.1347e-04,  1.6564e-03, -1.9582e-02,  1.1158e-02,\n",
      "        -1.0652e-03,  2.9224e-04,  2.6345e-03,  1.6564e-03, -4.2273e-03,\n",
      "        -6.9736e-03,  1.0487e-03, -6.8989e-03, -2.5800e-02,  4.2242e-03,\n",
      "         3.7721e-02,  2.4212e-03,  3.1442e-03,  4.0176e-03, -3.1278e-04,\n",
      "         6.5009e-03,  2.2989e-03, -1.7846e-03, -1.0157e-02, -8.2482e-04,\n",
      "         3.0802e-03,  1.4397e-03, -2.5187e-03, -1.2227e-03,  9.8553e-03,\n",
      "        -9.5945e-02, -4.1858e-02, -7.6770e-04,  1.9710e-02, -1.6464e-02,\n",
      "        -2.1991e-02, -9.0036e-04, -2.8786e-03, -1.5118e-03,  6.8298e-03]) \n",
      " tensor([4.9840, 1.5880, 3.9498, 2.9722, 1.1307, 2.3547, 5.4702, 1.9667, 2.3906,\n",
      "        0.5062, 1.0378, 1.8842, 0.9397, 0.8423, 0.5441, 4.5482, 1.0162, 2.7216,\n",
      "        1.4733, 0.5273, 1.5052, 5.0870, 0.9547, 0.9676, 3.1240, 4.7185, 0.7988,\n",
      "        2.5780, 3.9741, 1.5084, 1.8841, 1.8962, 1.9384, 2.0745, 2.0251, 4.5669,\n",
      "        1.7989, 5.7187, 1.6196, 4.8428, 0.6472, 0.6137, 0.8635, 0.8099, 0.5691,\n",
      "        1.3260, 1.9008, 2.6183, 1.8668, 1.5419, 1.4129, 3.4943, 4.2410, 3.1692,\n",
      "        0.7600, 1.7436, 5.1167, 1.7177, 3.1606, 1.1026, 0.7086, 2.1883, 0.9456,\n",
      "        1.2968, 1.3303, 0.7970, 3.6614, 1.4701, 2.7962, 2.2751, 0.7493, 4.6276,\n",
      "        1.0470, 2.3090, 2.4084, 2.5408, 2.9826, 2.2752, 1.4497, 1.2478, 0.2564,\n",
      "        1.2236, 4.5707, 0.7901, 4.3380, 0.5811, 1.0831, 3.4581, 2.8719, 0.8421])\n",
      "encoder.model.phased_cell.phase \n",
      " None \n",
      " tensor([4.3579, 1.0059, 3.9284, 2.2026, 1.6147, 1.8327, 5.5256, 0.7834, 2.0463,\n",
      "        0.6080, 0.2822, 0.6000, 0.0391, 0.1464, 0.5916, 3.9748, 0.4988, 0.5253,\n",
      "        1.6675, 0.7863, 0.2739, 2.8919, 0.2172, 0.8328, 0.4840, 3.0377, 1.4090,\n",
      "        0.1706, 1.4264, 0.2346, 2.3351, 1.2407, 1.8235, 1.7531, 1.8264, 4.8685,\n",
      "        1.5669, 0.8452, 1.6010, 3.4169, 0.7335, 0.3655, 0.2062, 0.7669, 0.8432,\n",
      "        1.0561, 1.4645, 2.1673, 0.0534, 1.0374, 1.3027, 0.9339, 4.0773, 0.7087,\n",
      "        1.3104, 0.9358, 0.3599, 0.6380, 2.5711, 0.7495, 0.6246, 0.4843, 0.7930,\n",
      "        0.1423, 1.0167, 0.2031, 1.7839, 0.7762, 0.8769, 0.5366, 0.7869, 2.4897,\n",
      "        0.8787, 1.7472, 1.3667, 0.3119, 2.3740, 2.2957, 0.6830, 0.2749, 0.7797,\n",
      "        1.1204, 0.9850, 0.3341, 0.7986, 0.7178, 1.1555, 0.8661, 1.5925, 0.2110])\n",
      "lmbd.hidden_to_mean.weight \n",
      " tensor([[-0.0002,  0.0365, -0.0072,  ...,  0.0146, -0.0018, -0.0008],\n",
      "        [ 0.0061,  0.0065,  0.0090,  ..., -0.0234,  0.0040,  0.0099],\n",
      "        [ 0.0079,  0.0014,  0.0095,  ..., -0.0067,  0.0038,  0.0092],\n",
      "        ...,\n",
      "        [ 0.0001,  0.0235, -0.0025,  ...,  0.0092,  0.0003,  0.0042],\n",
      "        [-0.0002,  0.0364, -0.0008,  ...,  0.0059,  0.0002,  0.0051],\n",
      "        [ 0.0014, -0.0034,  0.0004,  ..., -0.0228,  0.0009,  0.0005]]) \n",
      " tensor([[-0.3447, -0.2994,  0.1624,  ..., -0.2406, -0.0924, -0.1468],\n",
      "        [-0.0240,  0.0665, -0.0693,  ..., -0.4182,  0.0478,  0.2808],\n",
      "        [-0.1626, -0.2076, -0.1654,  ..., -0.0894,  0.2783, -0.0889],\n",
      "        ...,\n",
      "        [ 0.4181, -0.3060,  0.4940,  ..., -0.3097,  0.3358, -0.3280],\n",
      "        [-0.2149, -0.3713,  0.0499,  ..., -0.4606,  0.1353, -0.0027],\n",
      "        [-0.1322, -0.0211, -0.4754,  ..., -0.3841, -0.0185,  0.3051]])\n",
      "lmbd.hidden_to_mean.bias \n",
      " tensor([-0.0577,  0.1036,  0.0968, -0.1103,  0.0082,  0.0038,  0.0195,  0.0343,\n",
      "         0.1134, -0.0568, -0.0255, -0.0319, -0.1658, -0.0656, -0.0779, -0.1105,\n",
      "         0.0683,  0.1050, -0.0029, -0.0050,  0.0216,  0.1135,  0.1003, -0.0281,\n",
      "         0.0535,  0.0318, -0.0176,  0.0263,  0.0069,  0.1248, -0.0719,  0.0943,\n",
      "        -0.0004,  0.0173,  0.1288, -0.0235, -0.0333,  0.0099,  0.0262,  0.0085]) \n",
      " tensor([ 0.0909, -0.0203,  0.0553, -0.0484, -0.0225,  0.0785, -0.0030,  0.1235,\n",
      "         0.0820, -0.1824,  0.0513, -0.0697,  0.0050,  0.0979,  0.0593, -0.0963,\n",
      "         0.0013,  0.0895, -0.0367,  0.1135,  0.0875,  0.1434, -0.0582, -0.1050,\n",
      "         0.0898, -0.1325,  0.0119,  0.0407, -0.0965,  0.0277,  0.1311, -0.0393,\n",
      "         0.0520, -0.0076,  0.0460, -0.1463, -0.0710,  0.0611, -0.0233,  0.0464])\n",
      "classifier.0.weight \n",
      " tensor([[-3.7178e-03,  5.5893e-03,  3.3249e-03, -4.0086e-03, -7.7900e-04,\n",
      "          2.1011e-03, -3.6989e-03, -2.9615e-04, -2.8884e-03, -5.8595e-03,\n",
      "         -1.3876e-03, -7.8177e-03,  1.9123e-03, -3.6464e-03,  6.0998e-05,\n",
      "          3.3188e-03,  3.4374e-04, -3.4400e-03, -1.4415e-03,  8.2795e-03,\n",
      "         -1.4035e-03,  6.9389e-03,  5.1101e-04, -4.8320e-03, -7.6171e-03,\n",
      "         -1.9887e-03, -6.7294e-03,  3.5718e-03,  6.6510e-04,  3.7219e-03,\n",
      "          3.1150e-03, -2.3995e-03,  2.1089e-03,  2.5361e-03, -1.0334e-04,\n",
      "         -6.5214e-03,  1.2624e-03,  7.3637e-03,  3.4872e-03,  2.6514e-03],\n",
      "        [ 1.9648e-02, -5.6627e-03, -3.1031e-02,  5.1227e-02, -1.7770e-02,\n",
      "          2.8945e-02, -3.7581e-02, -3.1966e-02,  1.6488e-02, -1.5841e-02,\n",
      "          3.8094e-02,  3.1607e-02,  3.3285e-02,  2.3713e-02, -2.5231e-02,\n",
      "          4.2483e-02,  1.2839e-02, -1.2107e-02,  9.7336e-03,  5.3461e-03,\n",
      "         -3.8573e-02,  4.9371e-03,  2.2881e-02,  4.7354e-02, -3.8472e-03,\n",
      "         -4.6624e-03,  3.1594e-02, -2.7678e-02,  8.7626e-04, -7.5193e-03,\n",
      "         -1.9572e-03, -3.3775e-02,  3.3392e-02, -5.8672e-02, -2.6897e-03,\n",
      "          6.5563e-03,  3.0754e-02, -3.6999e-02, -2.4300e-02, -4.1336e-02],\n",
      "        [-1.2758e-02,  1.7945e-02,  3.4445e-02, -5.5309e-02, -1.2240e-02,\n",
      "          1.0454e-02,  8.1263e-04,  3.9437e-02,  1.3794e-02, -3.5621e-02,\n",
      "         -2.2814e-02, -5.7551e-02, -4.7751e-03, -3.0235e-02,  6.5086e-03,\n",
      "         -2.0298e-02, -1.6484e-02, -9.6718e-04, -1.2264e-02,  5.0445e-02,\n",
      "          4.0376e-02,  2.5674e-02, -2.4894e-03, -4.5580e-02, -1.6274e-02,\n",
      "         -3.5463e-02, -3.1548e-02,  7.3016e-04, -1.7527e-03,  3.0150e-02,\n",
      "         -5.3622e-03, -2.4702e-03, -2.0930e-03,  5.4189e-02,  1.1394e-02,\n",
      "         -3.5045e-02,  1.0044e-02,  7.4034e-02,  2.8261e-02,  1.0712e-02],\n",
      "        [ 1.0343e-02,  1.6023e-02, -3.1706e-02,  1.4357e-02, -1.7951e-02,\n",
      "          4.8937e-02, -3.4031e-02,  2.2382e-02,  1.6596e-02, -6.4913e-02,\n",
      "          2.9112e-02, -1.0458e-02,  6.4052e-03,  3.3815e-02, -6.5097e-03,\n",
      "          2.4006e-02,  2.2912e-02, -3.3527e-02,  1.2637e-02,  4.3686e-02,\n",
      "          1.2888e-02,  4.0081e-02,  6.9698e-03,  1.6100e-02, -1.8881e-02,\n",
      "         -3.9915e-02,  3.5539e-02, -3.7158e-02, -5.1173e-02,  2.3922e-03,\n",
      "         -4.7244e-04, -3.5155e-02,  2.0324e-02, -8.0963e-03, -4.9787e-03,\n",
      "         -6.6250e-03,  1.7193e-02,  2.4758e-02, -1.5321e-02, -2.3189e-02],\n",
      "        [ 1.5739e-02, -4.1146e-04, -1.3539e-02,  2.7448e-03, -2.1671e-02,\n",
      "          3.9270e-02, -2.4491e-02,  3.3706e-02,  2.0964e-02, -4.5417e-02,\n",
      "          1.6144e-02, -9.3052e-03,  7.9371e-03,  2.2400e-02, -2.8414e-03,\n",
      "          1.2782e-02,  4.8895e-03, -2.5817e-02,  1.9302e-02,  3.5111e-02,\n",
      "          1.6291e-02,  2.0968e-02,  1.0048e-03,  9.5134e-03, -1.5341e-02,\n",
      "         -3.2383e-02,  2.9416e-02, -4.0516e-02, -3.2370e-02,  6.5078e-03,\n",
      "         -1.2632e-02, -3.4096e-02,  1.4270e-02,  1.5124e-03,  3.8168e-03,\n",
      "         -7.2868e-03,  1.6550e-02,  2.5216e-02, -9.9753e-03, -2.2081e-02],\n",
      "        [-1.1737e-01,  1.4197e-01,  1.3334e-01, -2.4638e-01,  1.9697e-02,\n",
      "         -5.5433e-03,  2.6698e-02,  1.3656e-01,  2.5157e-03, -1.6771e-01,\n",
      "         -1.0218e-01, -2.8491e-01, -7.7183e-02, -1.5315e-01,  6.0383e-02,\n",
      "         -8.0952e-02, -1.7264e-02,  1.6441e-02, -8.7066e-02,  1.5798e-01,\n",
      "          1.4205e-01,  1.5445e-01,  2.4272e-02, -2.2122e-01, -1.1518e-01,\n",
      "         -9.5785e-02, -1.9662e-01,  1.0773e-01, -1.7834e-02,  1.5450e-01,\n",
      "          2.2106e-02,  8.6596e-02, -3.6587e-02,  2.0550e-01,  5.9640e-02,\n",
      "         -1.3923e-01,  5.8056e-03,  2.6814e-01,  1.5399e-01,  1.2117e-01],\n",
      "        [-1.3164e-02,  3.2092e-03,  1.9847e-03, -8.7170e-03,  1.5425e-02,\n",
      "         -8.1663e-03,  1.3985e-02, -6.6704e-04, -2.1460e-02,  3.3801e-03,\n",
      "         -5.1353e-03, -1.0042e-02, -1.5133e-02, -1.6675e-03,  1.3087e-02,\n",
      "         -4.2115e-03,  1.2530e-02, -6.7398e-03,  1.1875e-03, -7.5507e-03,\n",
      "          7.2256e-03,  3.0781e-03, -1.3835e-02, -1.1459e-02, -1.0187e-02,\n",
      "          1.6767e-02, -9.1368e-03,  1.5880e-02, -9.3820e-03, -9.1378e-03,\n",
      "          9.6118e-03,  1.2957e-02, -1.0363e-02,  6.1139e-03, -3.4437e-03,\n",
      "          6.3179e-04, -1.7482e-02, -2.4583e-03,  2.7258e-03,  1.5567e-02],\n",
      "        [ 9.8083e-03, -2.0348e-02, -1.4816e-02,  2.9268e-02,  1.5750e-02,\n",
      "         -2.5078e-03,  5.2958e-03, -1.0237e-02, -2.4330e-02,  2.9717e-02,\n",
      "         -5.6423e-05,  2.9231e-02,  6.6682e-03,  1.8732e-02,  3.3796e-03,\n",
      "          2.3133e-03,  5.1606e-03, -6.4558e-03,  3.1566e-03, -3.1662e-02,\n",
      "         -1.6698e-02, -2.8518e-02, -9.4607e-03,  1.8822e-02,  3.6221e-03,\n",
      "          1.9853e-02,  9.9848e-03,  2.0352e-03, -1.3110e-03, -2.0775e-02,\n",
      "          7.7810e-03, -1.8797e-03,  5.6232e-03, -1.0403e-02, -1.5253e-02,\n",
      "          2.1473e-02, -1.1683e-02, -3.4362e-02, -1.0736e-02,  3.7657e-03],\n",
      "        [-1.6928e-02,  7.8476e-03,  8.0256e-03, -2.0164e-02,  1.1406e-02,\n",
      "         -6.5137e-03,  1.0776e-02,  9.3428e-03, -1.6690e-02, -9.2281e-03,\n",
      "         -6.1425e-03, -2.3838e-02, -1.3341e-02, -8.8027e-03,  1.6653e-02,\n",
      "         -5.5749e-03,  7.9676e-03, -1.0351e-02, -4.9318e-04,  9.1543e-03,\n",
      "          1.6128e-02,  1.3385e-02, -1.6452e-02, -2.0900e-02, -1.5588e-02,\n",
      "          4.9253e-03, -1.6320e-02,  1.3264e-02, -1.3092e-02, -3.1631e-04,\n",
      "          9.9752e-03,  1.0867e-02, -1.0973e-02,  1.5881e-02, -5.1376e-03,\n",
      "         -8.5083e-03, -1.2169e-02,  1.7237e-02,  6.5488e-03,  1.4255e-02],\n",
      "        [-4.6526e-03, -2.5888e-02,  1.1641e-02,  1.2810e-02,  3.4059e-02,\n",
      "         -3.9051e-02,  3.0602e-02, -2.7810e-02, -4.1072e-02,  6.7385e-02,\n",
      "         -1.0204e-02,  2.6810e-02,  3.7621e-03, -2.3216e-02,  1.4391e-02,\n",
      "         -1.6371e-02, -8.1382e-03,  1.1055e-02, -1.4581e-02, -4.9412e-02,\n",
      "         -2.4482e-02, -4.4781e-02, -3.4351e-02,  2.4899e-03,  2.3537e-02,\n",
      "          3.7408e-02, -2.0955e-02,  2.4906e-02,  3.6104e-02, -1.6369e-02,\n",
      "          1.5727e-02,  2.8393e-02, -7.9227e-03, -6.8459e-03, -3.0874e-02,\n",
      "          2.6275e-02, -1.6701e-02, -3.6426e-02,  2.9377e-03,  1.3609e-02],\n",
      "        [ 2.1070e-03, -1.1190e-02, -5.4620e-03,  4.1453e-03,  9.5763e-03,\n",
      "         -9.1363e-03,  1.4180e-02, -8.0805e-03, -1.6844e-02,  1.3638e-02,\n",
      "         -5.0251e-04,  8.5009e-03,  2.4538e-03,  4.4686e-03,  9.2217e-03,\n",
      "          2.4117e-03,  1.9276e-03, -7.0487e-03,  9.5160e-03, -9.4569e-03,\n",
      "          5.0224e-03, -1.1842e-02, -1.7780e-02,  4.5635e-03,  3.1012e-03,\n",
      "          1.7953e-02,  6.5333e-03,  7.4322e-04, -5.5840e-03, -2.6831e-02,\n",
      "          7.0731e-03, -8.8618e-04, -9.8390e-03, -1.6528e-03, -1.1307e-02,\n",
      "          1.0224e-02, -1.4983e-02, -9.9433e-03, -1.1355e-02,  2.6001e-03],\n",
      "        [ 1.9285e-02, -1.9767e-02, -3.0490e-02,  3.6928e-02, -2.0166e-02,\n",
      "          1.2202e-02, -2.6428e-02, -3.7823e-03,  1.7624e-02, -1.2357e-02,\n",
      "          3.8543e-02,  2.6619e-02,  3.5409e-02,  2.2419e-02, -2.6680e-04,\n",
      "          2.9789e-02, -3.6448e-03, -2.2225e-02,  1.6326e-02,  1.7108e-02,\n",
      "         -1.3595e-02,  1.3707e-03, -8.3059e-03,  3.5944e-02,  2.2352e-03,\n",
      "         -1.7959e-02,  3.2927e-02, -3.7391e-02, -1.7929e-02, -7.7809e-03,\n",
      "          2.9533e-03, -3.4737e-02,  1.2170e-02, -2.8572e-02, -2.6781e-02,\n",
      "          1.0748e-02,  2.7683e-02, -1.0455e-02, -2.6323e-02, -4.1027e-02],\n",
      "        [ 6.6651e-02, -5.1369e-02, -7.0710e-02,  1.2543e-01, -3.6276e-02,\n",
      "          2.6537e-02, -3.9463e-02, -6.4710e-02,  3.3227e-02,  5.4107e-02,\n",
      "          6.2561e-02,  1.3320e-01,  4.0008e-02,  8.1831e-02, -5.9018e-02,\n",
      "          5.0033e-02,  1.5400e-02,  1.5090e-02,  3.5284e-02, -6.3281e-02,\n",
      "         -8.5923e-02, -6.1406e-02,  3.5616e-02,  1.1106e-01,  5.5059e-02,\n",
      "          2.2016e-02,  1.0315e-01, -5.8657e-02,  1.6998e-02, -4.2871e-02,\n",
      "         -2.1176e-02, -5.0887e-02,  3.7321e-02, -1.0620e-01,  2.5962e-04,\n",
      "          5.7324e-02,  2.0969e-02, -1.3412e-01, -7.0063e-02, -6.8321e-02],\n",
      "        [-1.0995e-02, -2.9157e-03,  1.6294e-02, -8.8462e-03,  6.6524e-03,\n",
      "         -2.5843e-02,  1.6817e-02, -1.8489e-02, -1.5115e-02,  3.2193e-02,\n",
      "         -1.8386e-02, -5.3192e-03,  2.6271e-03, -1.9065e-02,  2.6288e-03,\n",
      "         -3.6695e-03, -7.5996e-03,  1.8485e-02, -1.1022e-02, -1.8382e-02,\n",
      "         -1.1300e-02, -2.1425e-02,  7.4181e-03, -1.6086e-02, -1.3124e-03,\n",
      "          2.4801e-02, -2.9223e-02,  3.3792e-02,  2.9173e-02, -5.4479e-03,\n",
      "          8.8266e-03,  1.5403e-02, -9.7021e-03,  9.8214e-03,  7.4777e-03,\n",
      "         -3.9202e-03, -9.0928e-03, -1.5031e-02,  1.1298e-02,  2.0699e-02],\n",
      "        [ 3.9665e-02, -4.4955e-02, -1.0845e-01,  1.5606e-01,  1.4544e-02,\n",
      "         -1.1477e-02, -1.7713e-02, -1.0338e-01, -2.1103e-02,  6.4863e-02,\n",
      "          8.9293e-02,  1.5796e-01,  5.5283e-03,  1.0393e-01, -1.6395e-02,\n",
      "          6.7294e-02,  5.7566e-02, -1.2972e-02,  5.0264e-02, -1.1647e-01,\n",
      "         -9.7714e-02, -4.3787e-02, -1.1049e-03,  1.3382e-01,  4.5844e-02,\n",
      "          7.8894e-02,  1.1507e-01, -2.9702e-02, -2.3929e-02, -8.5775e-02,\n",
      "          1.0108e-02, -1.1093e-02, -8.0745e-04, -1.5939e-01, -3.0630e-02,\n",
      "          8.9280e-02, -1.8802e-02, -1.8931e-01, -9.0131e-02, -5.1854e-02],\n",
      "        [ 5.6042e-02, -1.7224e-02, -5.5448e-03,  2.9111e-02, -5.9559e-02,\n",
      "          2.6081e-02, -4.5240e-02,  1.1625e-02,  9.7585e-02, -2.2044e-03,\n",
      "          8.2462e-03,  4.6952e-02,  5.2475e-02,  3.9867e-03, -5.3143e-02,\n",
      "          1.0240e-03, -6.0631e-02,  3.9388e-02, -9.0027e-03,  1.3811e-02,\n",
      "         -1.9520e-02, -2.4260e-02,  6.1026e-02,  4.7233e-02,  5.6280e-02,\n",
      "         -6.6588e-02,  4.1845e-02, -6.3293e-02,  4.4338e-02,  3.6219e-02,\n",
      "         -5.0659e-02, -4.1461e-02,  3.3909e-02, -1.8481e-02,  2.2899e-02,\n",
      "          7.1929e-03,  6.7373e-02,  6.9574e-03, -5.1995e-03, -5.8562e-02],\n",
      "        [-6.7184e-02,  4.0260e-02,  6.4540e-02, -1.0132e-01,  5.8875e-02,\n",
      "         -4.4355e-02,  7.2805e-02,  2.8878e-02, -6.4184e-02,  1.1259e-02,\n",
      "         -7.6132e-02, -8.9298e-02, -7.7311e-02, -5.8156e-02,  4.2011e-02,\n",
      "         -6.2807e-02,  1.3913e-02,  1.5330e-02, -2.8846e-02, -1.3028e-02,\n",
      "          5.7208e-02,  1.6063e-02, -2.7971e-02, -9.9003e-02, -3.1325e-02,\n",
      "          4.3393e-02, -8.8769e-02,  9.1845e-02,  2.0944e-03,  8.4348e-03,\n",
      "          2.0211e-02,  8.0384e-02, -4.7919e-02,  8.7343e-02,  2.1988e-02,\n",
      "         -2.9110e-02, -7.0010e-02,  5.2217e-02,  5.8638e-02,  1.0019e-01],\n",
      "        [ 2.7209e-02, -4.3566e-02,  9.0285e-04,  1.9081e-02, -1.1559e-02,\n",
      "          5.4657e-03,  5.4335e-03,  5.4441e-03,  1.5944e-03,  4.4760e-02,\n",
      "         -4.7035e-03,  4.4326e-02,  1.5805e-02,  1.8323e-02, -4.8034e-03,\n",
      "         -9.9021e-03, -2.0963e-02, -1.8336e-02,  2.2520e-02, -6.1400e-03,\n",
      "          4.9744e-03, -4.1569e-02, -3.7688e-02,  2.0790e-02,  2.7903e-02,\n",
      "          9.0268e-04,  3.3242e-02, -4.1239e-02,  4.7667e-03, -3.1260e-02,\n",
      "         -1.9403e-02, -3.1819e-02,  5.9353e-03, -1.5046e-03, -1.9550e-02,\n",
      "          1.6345e-02, -5.8971e-03, -7.3128e-03, -2.5742e-02, -2.7166e-02],\n",
      "        [-3.5653e-03,  3.9690e-03,  2.9062e-02, -2.2154e-02,  5.3140e-03,\n",
      "         -2.4440e-02,  1.8983e-02, -2.9622e-03,  8.7659e-03,  2.4483e-02,\n",
      "         -2.3600e-02,  3.1635e-03, -1.2591e-02, -2.0320e-02, -5.1879e-03,\n",
      "         -3.0003e-02, -2.5018e-02,  3.1820e-02, -1.2358e-02, -9.7663e-03,\n",
      "          9.0738e-03, -1.0375e-02,  4.2516e-03, -1.2264e-02,  2.5161e-02,\n",
      "          3.1993e-03, -2.0456e-02,  1.1289e-02,  2.3405e-02,  1.0492e-02,\n",
      "         -1.0501e-02,  2.4901e-02, -1.2476e-02,  1.5630e-02,  1.0603e-02,\n",
      "         -8.2074e-03, -1.0326e-02,  1.4607e-02,  1.1787e-02,  1.5505e-02],\n",
      "        [-1.6159e-02,  6.4815e-03,  8.1964e-03, -1.4249e-02,  6.6720e-03,\n",
      "         -2.2959e-02,  1.2257e-02, -1.5002e-02, -5.4683e-03,  1.3366e-02,\n",
      "         -1.0752e-02, -9.8299e-03, -1.3944e-02, -1.5361e-02,  5.0716e-03,\n",
      "         -1.6657e-03,  4.2930e-03,  1.2379e-02, -2.8521e-03, -1.5774e-02,\n",
      "         -2.0321e-03,  1.0160e-03,  5.4871e-03, -1.6348e-02, -7.1952e-03,\n",
      "          2.4631e-02, -1.9547e-02,  2.9851e-02,  1.5937e-02,  1.6668e-03,\n",
      "          4.6757e-03,  2.1159e-02, -1.6371e-02,  1.2881e-03,  1.2669e-02,\n",
      "         -1.5947e-03, -1.0490e-02, -1.4114e-02,  9.4663e-03,  1.2814e-02]]) \n",
      " tensor([[ 1.2581e-01, -1.1420e-01, -1.3009e-01,  1.2758e-01,  1.0775e-01,\n",
      "         -2.0887e-01,  2.0351e-01, -6.4258e-02, -2.2173e-01,  4.0644e-01,\n",
      "          4.0194e-02,  2.1125e-01, -3.4199e-01,  2.4401e-02,  2.5200e-01,\n",
      "         -1.6903e-01,  1.3295e-01, -8.8227e-02,  4.3151e-01, -4.0613e-01,\n",
      "          2.6336e-01, -3.7418e-01, -1.4884e-01,  7.1876e-02,  2.7111e-01,\n",
      "          3.2120e-01,  2.4093e-01,  1.0298e-01, -7.1127e-02, -3.8625e-01,\n",
      "         -6.1536e-02,  1.7505e-01, -3.2307e-01, -5.6547e-02, -3.7873e-02,\n",
      "          1.7407e-01, -2.1971e-01, -2.3428e-01, -9.5939e-02,  8.9774e-02],\n",
      "        [ 3.2045e-01, -2.0344e-01,  1.7086e-01, -2.1722e-01, -1.1380e-01,\n",
      "         -3.7610e-01,  1.5140e-01,  3.9468e-01, -3.9156e-02,  4.8836e-01,\n",
      "         -2.5437e-01,  4.5946e-02,  1.2452e-01, -4.8789e-02,  1.5113e-01,\n",
      "         -1.4423e-01, -5.0822e-01,  2.5282e-01,  4.2635e-02, -1.2150e-01,\n",
      "          6.8055e-02, -5.8020e-01, -1.3783e-01, -2.0068e-01,  3.4290e-01,\n",
      "         -2.1222e-01, -7.7322e-02, -1.2752e-01,  3.1250e-01,  1.4940e-01,\n",
      "          1.8683e-02,  4.1696e-02, -1.9929e-01,  3.0941e-01, -1.2913e-01,\n",
      "          1.6706e-01, -1.3321e-01,  1.2005e-01,  8.5666e-02,  1.2421e-01],\n",
      "        [-2.9553e-04, -4.8367e-01,  4.2692e-01, -2.2997e-01, -5.5795e-01,\n",
      "          8.2562e-02,  1.7233e-01,  2.1104e-01,  4.9448e-01,  2.0133e-01,\n",
      "         -6.6138e-02, -1.1006e-01,  2.7957e-01, -3.6968e-01, -3.5515e-02,\n",
      "         -1.8258e-01, -3.1572e-01,  3.6002e-01,  2.2636e-01,  4.3040e-01,\n",
      "          1.4931e-01, -3.7520e-01, -1.7984e-01, -1.2794e-01,  3.8263e-01,\n",
      "         -2.7117e-01, -1.0283e-01, -2.8098e-01,  3.6222e-01,  7.4228e-02,\n",
      "         -4.9329e-01, -1.3445e-01, -1.0183e-01,  2.2051e-01,  1.6053e-01,\n",
      "         -4.0950e-01,  5.8183e-02,  2.5912e-01, -2.1869e-01, -1.9473e-01],\n",
      "        [ 2.4084e-02,  1.1348e-02, -1.7487e-01,  2.7359e-01, -4.5308e-02,\n",
      "          1.2965e-01, -1.8190e-01,  1.1995e-01,  1.7771e-01, -1.8530e-01,\n",
      "          2.5750e-01,  3.4058e-01, -3.0580e-01,  2.7663e-01, -3.0462e-01,\n",
      "         -8.8689e-02,  2.8392e-01, -1.7559e-01,  3.8621e-02, -4.0371e-02,\n",
      "         -1.3056e-01,  2.4504e-01,  5.9182e-02,  1.8334e-01, -1.2640e-01,\n",
      "          6.6368e-02,  2.4865e-01, -1.8876e-01, -3.0985e-01,  3.3399e-01,\n",
      "         -1.1343e-01,  3.9196e-02,  1.6753e-01, -4.5534e-01,  5.8825e-02,\n",
      "          1.2454e-01,  2.3737e-01, -1.7128e-01, -5.4057e-02, -3.2621e-01],\n",
      "        [-1.5111e-01, -1.1167e-01, -3.6762e-02,  2.6616e-01, -3.7477e-01,\n",
      "          1.2266e-01, -2.4775e-01,  9.5583e-02,  5.1109e-01, -2.9540e-04,\n",
      "          2.5242e-01,  4.6900e-01, -2.6284e-01,  2.2578e-01, -2.8978e-01,\n",
      "          2.2211e-01,  4.4248e-03, -2.6986e-01,  5.5461e-01,  1.2465e-01,\n",
      "         -3.3730e-02,  3.5711e-01,  1.2045e-01,  8.7287e-02,  8.3169e-02,\n",
      "         -1.8486e-01,  2.0521e-01, -9.5152e-02,  2.4408e-04,  3.5173e-01,\n",
      "         -3.5392e-01, -1.7749e-01, -2.3939e-02, -3.5565e-01,  2.1184e-01,\n",
      "         -1.4913e-01,  1.5179e-01, -9.6558e-02, -1.9824e-01, -3.2757e-01],\n",
      "        [-1.9983e-01,  4.8129e-01,  2.4013e-01, -2.4409e-01,  1.2332e-01,\n",
      "         -2.3967e-01,  9.1289e-02,  1.0360e-02,  1.6678e-01, -5.8617e-02,\n",
      "         -2.1068e-01, -2.7796e-01, -2.2205e-01, -2.3237e-01, -2.5305e-01,\n",
      "         -1.3182e-01,  1.7435e-01,  3.8892e-01,  2.1633e-02,  1.2431e-01,\n",
      "         -1.8586e-01,  3.0679e-01,  4.3351e-01, -1.7099e-01,  5.9907e-02,\n",
      "          5.5612e-02, -3.0795e-01,  4.1896e-01,  9.7856e-02,  4.1369e-01,\n",
      "         -1.2049e-01,  1.8464e-01,  1.5296e-01,  1.4548e-01,  1.9745e-01,\n",
      "         -2.0593e-01, -1.8831e-01,  5.5805e-02, -2.2222e-03,  3.1647e-01],\n",
      "        [-1.1742e-01,  4.7843e-01, -2.2434e-01,  2.9618e-02,  1.3612e-01,\n",
      "         -1.4911e-01,  1.9884e-01, -4.9877e-01, -1.8743e-01,  1.0394e-03,\n",
      "          4.2907e-02,  5.2704e-02, -4.6291e-01, -1.1963e-02, -1.3589e-01,\n",
      "          1.3538e-01,  2.9602e-01,  2.3400e-01, -8.7201e-02, -3.8936e-01,\n",
      "         -2.5653e-01,  5.0559e-01,  3.1318e-01, -2.9047e-02,  2.2324e-01,\n",
      "          9.9592e-02,  4.4967e-02,  2.4846e-01, -2.4356e-02, -9.4062e-03,\n",
      "         -4.5329e-02,  3.3748e-01, -2.7430e-01, -4.1909e-01,  2.5428e-01,\n",
      "          3.4772e-01, -3.2892e-01, -4.9924e-01, -3.0715e-03,  2.7482e-01],\n",
      "        [-7.0515e-02,  1.5738e-01, -6.1167e-02, -4.0444e-02,  1.9272e-01,\n",
      "         -3.1326e-03, -2.6961e-03,  7.9385e-02, -2.2777e-01, -9.4739e-02,\n",
      "          9.4207e-02, -8.2975e-02,  9.5834e-02, -8.8156e-03,  3.0431e-01,\n",
      "          1.5995e-01, -8.4039e-02, -3.6536e-01,  1.3245e-03,  4.6562e-02,\n",
      "         -5.5297e-02,  3.2242e-01, -3.0155e-02,  8.6457e-02, -2.2038e-01,\n",
      "         -1.4554e-01, -8.5398e-02,  1.2679e-02, -1.1734e-01, -1.0295e-01,\n",
      "          1.2149e-01,  1.5294e-01, -9.2786e-02, -1.2692e-01, -1.0882e-01,\n",
      "         -5.2723e-02,  7.8026e-02,  2.5332e-01,  1.5317e-01,  2.5536e-01],\n",
      "        [-3.0091e-01, -3.4064e-01,  9.9379e-02, -1.0697e-01,  6.6951e-02,\n",
      "         -4.0712e-01,  2.0363e-02,  4.7560e-02,  1.2474e-01, -7.4333e-02,\n",
      "          3.9787e-01, -1.0058e-01, -2.1601e-01, -3.2997e-01,  3.2024e-01,\n",
      "         -1.3807e-02, -7.6210e-02, -2.2556e-01,  6.1379e-02,  3.5523e-02,\n",
      "          2.4620e-01,  1.7600e-01, -2.3003e-01, -2.0480e-01, -2.2508e-01,\n",
      "          6.5403e-02, -1.2079e-01, -2.3336e-02, -1.1322e-01, -7.2447e-02,\n",
      "          1.6482e-01,  2.6963e-01, -4.2742e-01, -2.9590e-01, -2.7282e-01,\n",
      "          8.9359e-02,  1.8195e-01, -3.2970e-03, -9.4510e-02, -4.3096e-01],\n",
      "        [ 2.0766e-01,  1.9269e-01, -1.4785e-01,  8.0678e-02, -1.6916e-01,\n",
      "          2.2048e-01, -2.2804e-01,  9.8617e-02,  3.1200e-01, -2.5185e-01,\n",
      "         -9.3321e-02, -5.1620e-02,  6.9522e-02,  1.5035e-01, -8.1383e-02,\n",
      "          1.8751e-01, -1.3336e-01, -9.8417e-02,  2.5537e-01,  3.0025e-01,\n",
      "          1.5000e-01,  6.8150e-02,  3.4601e-01,  8.9222e-02, -8.1872e-02,\n",
      "         -1.2277e-01,  2.1844e-01, -1.5876e-01, -1.4789e-01, -2.1876e-01,\n",
      "         -2.1492e-01, -2.7636e-01,  4.3584e-02, -9.0787e-02,  7.6418e-02,\n",
      "         -1.3378e-01, -7.7693e-02,  1.5819e-01, -2.0330e-01, -6.7836e-02],\n",
      "        [ 3.1589e-01,  4.4852e-01,  3.8889e-01, -1.0050e-01, -2.5103e-01,\n",
      "          2.8071e-01, -2.7088e-01,  9.2423e-03,  2.9191e-01, -1.2092e-01,\n",
      "         -1.7205e-01, -4.9012e-02, -2.8994e-02, -1.9360e-01, -2.3539e-01,\n",
      "         -1.7535e-01, -3.2055e-02,  6.2315e-02, -3.6841e-02,  1.0616e-01,\n",
      "          1.3476e-01,  1.8554e-01,  1.8639e-01, -1.5710e-01,  2.4180e-01,\n",
      "         -3.2031e-01, -6.4596e-02, -2.2364e-01,  3.0702e-01,  2.9898e-01,\n",
      "         -1.5640e-01,  2.3682e-02,  1.7846e-01,  1.5844e-01,  1.0084e-01,\n",
      "         -2.4056e-01,  9.2936e-02,  2.4254e-01,  3.0259e-01, -1.6796e-01],\n",
      "        [ 2.5185e-01,  4.4391e-03,  2.2553e-01, -4.6967e-03,  3.3151e-01,\n",
      "          2.3007e-02,  2.1128e-01, -3.9957e-01, -2.4884e-01,  2.6006e-01,\n",
      "         -3.2324e-01, -9.0720e-02,  6.1122e-02, -1.0288e-01, -3.5219e-02,\n",
      "         -5.5278e-02, -7.1579e-02, -9.7551e-03, -1.8526e-01, -3.6441e-01,\n",
      "         -8.1074e-02, -2.1695e-01, -1.3791e-02,  7.1048e-02,  7.3811e-02,\n",
      "          3.5330e-01, -2.9615e-01,  1.4120e-01,  1.9137e-01, -2.7816e-01,\n",
      "          2.6480e-01,  3.3039e-02,  3.9052e-02, -1.4431e-02, -2.3821e-02,\n",
      "          2.8420e-02, -2.5941e-01, -4.8222e-02,  2.1931e-01,  1.2323e-01],\n",
      "        [-2.7500e-01, -1.0931e-01, -9.4739e-02,  2.6646e-01,  1.7632e-01,\n",
      "         -2.9951e-01,  9.0803e-02, -4.5830e-02, -3.0363e-01,  1.6968e-01,\n",
      "         -1.8360e-01,  1.7102e-01,  5.7347e-01, -6.0682e-02, -4.2770e-03,\n",
      "          4.2170e-01, -1.2781e-01, -2.3908e-01, -2.0470e-01,  3.3625e-01,\n",
      "         -9.6815e-02, -2.1656e-01, -1.4545e-01,  1.9452e-02, -1.9210e-01,\n",
      "          5.5304e-02, -2.5584e-01,  2.5808e-01, -1.6632e-01, -2.7194e-01,\n",
      "          3.0113e-01, -3.6146e-01, -6.3718e-02,  1.0270e-01, -3.2882e-01,\n",
      "         -1.1455e-01, -1.7453e-02, -1.5875e-01, -4.1027e-01,  6.2494e-02],\n",
      "        [ 7.9372e-02, -1.8451e-01, -3.0998e-01,  2.4169e-01,  4.0727e-01,\n",
      "          1.0630e-01,  7.5684e-02, -3.2294e-02,  1.4031e-02, -5.4294e-02,\n",
      "          2.0457e-01,  3.0928e-01,  2.7247e-01,  9.8834e-02,  1.8610e-01,\n",
      "          9.6029e-02, -8.1525e-02,  1.4253e-02, -1.9292e-01,  7.6354e-02,\n",
      "          1.7023e-02, -1.7180e-01, -1.1555e-01,  3.2113e-01,  2.4116e-01,\n",
      "         -1.4162e-01,  2.7166e-01, -2.1185e-01, -2.3361e-01, -5.2156e-01,\n",
      "          8.2733e-02, -8.1991e-02,  2.2906e-01, -3.3191e-01, -2.8731e-01,\n",
      "          4.4359e-01,  1.2844e-01, -1.9906e-02, -3.5076e-01, -1.2967e-01],\n",
      "        [ 2.0654e-01, -1.7708e-01, -1.9127e-03,  4.0208e-02,  1.1784e-01,\n",
      "          3.8808e-03, -4.3568e-04, -5.3925e-02, -1.8426e-01,  1.3542e-01,\n",
      "         -1.6282e-01, -2.5291e-01,  4.2623e-01, -3.3618e-01,  1.1596e-01,\n",
      "         -8.5420e-02, -4.0461e-01,  4.2149e-02, -9.8137e-02,  6.0630e-03,\n",
      "          1.1675e-01, -2.6956e-01,  2.6595e-03, -1.9689e-01, -1.1559e-01,\n",
      "         -1.9496e-01, -1.5949e-01,  1.3765e-01,  4.1644e-01,  1.2321e-01,\n",
      "         -1.2588e-02,  6.8523e-02,  3.1965e-01,  1.3997e-02, -1.6420e-01,\n",
      "          1.6429e-01,  3.6156e-01,  2.3996e-01,  2.5326e-01,  8.1098e-02],\n",
      "        [-3.3149e-01, -2.4991e-01, -2.1126e-01,  4.6171e-01, -1.8964e-01,\n",
      "         -2.2709e-01, -1.2359e-01, -3.3882e-01, -1.3186e-01,  3.3985e-01,\n",
      "          2.7363e-01,  2.1868e-01, -1.9546e-01,  4.6718e-01, -3.7380e-02,\n",
      "          4.0179e-01,  4.4152e-01, -1.4133e-01,  3.7388e-01, -9.0941e-02,\n",
      "         -5.3650e-01, -2.0022e-01, -1.2549e-01, -4.6331e-02, -3.3492e-01,\n",
      "          2.2554e-01, -4.3463e-02,  2.4910e-01,  1.2044e-01, -2.2320e-01,\n",
      "         -1.1228e-02, -7.1181e-02, -8.8695e-02, -3.7057e-01, -2.0386e-01,\n",
      "         -3.5503e-01, -2.3532e-01, -4.5386e-01, -3.0982e-01,  7.0433e-02],\n",
      "        [-3.1720e-01,  4.4015e-03,  3.1845e-01, -1.9353e-01,  2.7715e-01,\n",
      "          4.1039e-03,  7.5746e-02, -9.7278e-03,  1.4479e-02,  1.8265e-01,\n",
      "         -4.3081e-01,  6.9762e-02, -3.5636e-01, -1.9183e-01, -2.7350e-01,\n",
      "         -2.8285e-01,  1.7303e-01,  3.1403e-01, -3.0278e-01, -1.7991e-01,\n",
      "          7.3071e-02, -3.4852e-01,  3.4538e-01, -2.5843e-01, -1.2223e-01,\n",
      "          3.8807e-01, -1.6146e-01,  2.8154e-01,  3.6976e-01, -4.9610e-02,\n",
      "          4.1117e-02,  2.8769e-01,  1.2489e-01,  2.0437e-01,  3.8207e-01,\n",
      "         -2.7287e-02, -1.2955e-01, -1.8761e-01,  1.3649e-02,  1.8570e-01],\n",
      "        [-4.8740e-02,  2.6002e-01,  6.2119e-02, -1.4509e-01, -2.9367e-01,\n",
      "          4.3322e-02,  1.9190e-02,  7.7187e-02,  4.0709e-01, -2.8691e-01,\n",
      "          2.2954e-01, -3.0813e-01, -8.6280e-02, -2.2014e-01, -1.5072e-01,\n",
      "          5.3598e-02,  4.1855e-01,  4.9335e-01, -2.5567e-01,  2.7179e-02,\n",
      "          9.1600e-02,  2.0466e-01,  4.6544e-01, -6.8860e-02, -3.8101e-05,\n",
      "         -9.1437e-02, -1.1496e-01,  1.5209e-01,  1.3650e-01,  3.6210e-01,\n",
      "          8.7981e-02,  2.2559e-01,  1.3140e-02,  4.6853e-02,  3.8214e-01,\n",
      "         -4.3354e-01,  2.6049e-01,  1.1453e-01,  1.1324e-01, -8.1655e-02],\n",
      "        [ 1.9619e-01,  1.4801e-02, -2.7403e-01,  3.9702e-01, -4.2939e-01,\n",
      "          2.9974e-01, -3.3742e-01, -5.6801e-01, -3.2876e-01, -1.2052e-01,\n",
      "          2.4203e-01, -9.2625e-02,  2.5161e-01, -9.4294e-03, -2.6890e-01,\n",
      "          4.8986e-01,  4.4962e-01, -2.4145e-01,  2.3229e-01, -5.9670e-02,\n",
      "         -4.2245e-01,  8.1803e-02,  3.6964e-01, -4.3508e-02, -4.5051e-01,\n",
      "          1.1781e-01,  1.5135e-01,  7.7690e-02,  1.2335e-01, -2.0319e-01,\n",
      "          3.7442e-01, -2.5067e-01,  1.7481e-01,  7.0091e-02,  1.4625e-01,\n",
      "          2.0369e-01,  2.2681e-01, -3.8637e-01,  1.4580e-01, -1.0979e-01],\n",
      "        [ 1.8806e-01, -3.2990e-01,  1.7625e-02,  2.7681e-01, -3.3172e-01,\n",
      "          1.2636e-01, -1.4203e-01,  3.5509e-01,  3.5187e-01,  1.5016e-01,\n",
      "         -2.2210e-01,  4.8914e-01,  3.7417e-01,  1.6982e-01, -3.8948e-01,\n",
      "         -1.0830e-01, -5.2326e-01, -1.9863e-01,  4.4165e-02,  1.7636e-01,\n",
      "         -1.4741e-02, -1.3696e-01,  1.8774e-01,  1.9672e-01,  2.2695e-01,\n",
      "         -2.3659e-01,  9.7955e-02, -1.9014e-01, -1.6855e-01, -2.1749e-01,\n",
      "         -1.6968e-01, -2.4619e-01,  2.1790e-01,  2.7875e-01, -2.0949e-01,\n",
      "         -3.1199e-01,  1.8701e-01,  2.1968e-01, -5.2203e-01, -1.5093e-01]])\n",
      "classifier.0.bias \n",
      " tensor([ 0.0047, -0.0361,  0.0351,  0.0349,  0.0252,  0.1494,  0.0168, -0.0071,\n",
      "         0.0256, -0.0254,  0.0058, -0.0127, -0.0822, -0.0154, -0.0815, -0.0793,\n",
      "         0.0714, -0.0042, -0.0156, -0.0093]) \n",
      " tensor([-0.1452,  0.0197, -0.0159, -0.1799,  0.0685, -0.0006, -0.0678, -0.0697,\n",
      "         0.0818,  0.0209,  0.0930, -0.1171,  0.0637, -0.1741, -0.0160, -0.0113,\n",
      "        -0.0692, -0.0341,  0.0848, -0.0442])\n",
      "decoder.latent_to_hidden.weight \n",
      " tensor([[-4.4276e-05,  5.8959e-05, -6.8896e-05,  ...,  1.0571e-04,\n",
      "         -2.5207e-05, -3.5311e-05],\n",
      "        [-5.9943e-05, -8.6229e-05, -1.0651e-04,  ..., -2.4269e-04,\n",
      "         -9.8475e-05,  1.9031e-05],\n",
      "        [ 2.9700e-05,  1.1388e-05,  9.5075e-05,  ...,  4.2849e-05,\n",
      "          5.7563e-05,  3.1827e-05],\n",
      "        ...,\n",
      "        [ 1.1400e-04, -1.0445e-05,  2.3559e-04,  ...,  5.5494e-05,\n",
      "          1.4246e-04,  6.1732e-05],\n",
      "        [-6.9112e-05,  3.7647e-06, -1.9618e-04,  ..., -3.7998e-05,\n",
      "         -1.1457e-04, -7.1336e-05],\n",
      "        [-1.0589e-04, -9.5781e-05, -2.6507e-04,  ..., -2.9607e-04,\n",
      "         -1.9395e-04, -4.1075e-05]]) \n",
      " tensor([[-0.1603, -0.0896,  0.0888,  ...,  0.1511,  0.0838,  0.0129],\n",
      "        [ 0.2036, -0.2382,  0.0369,  ...,  0.1034,  0.2288, -0.0801],\n",
      "        [ 0.1922,  0.2800, -0.0468,  ..., -0.2040, -0.2197, -0.1530],\n",
      "        ...,\n",
      "        [ 0.1790,  0.1024,  0.0404,  ...,  0.1643, -0.0789,  0.0641],\n",
      "        [-0.1878,  0.0904, -0.0206,  ..., -0.2727, -0.0927, -0.0598],\n",
      "        [-0.1414, -0.1427, -0.1771,  ...,  0.0707,  0.1376,  0.0460]])\n",
      "decoder.latent_to_hidden.bias \n",
      " tensor([ 2.0411e-04,  5.5517e-05, -1.2285e-04, -5.7311e-04, -2.9225e-05,\n",
      "        -2.9409e-06, -6.0876e-04,  1.8806e-05, -7.8981e-05, -8.9324e-05,\n",
      "        -1.7738e-04, -3.1876e-04,  5.5769e-04, -8.0551e-05,  4.4600e-05,\n",
      "        -4.7600e-05, -3.7597e-05, -3.2514e-04,  1.5485e-04,  4.7604e-04,\n",
      "         3.3829e-04,  2.3480e-04, -4.0006e-04, -5.0434e-05, -3.2123e-04,\n",
      "        -1.1248e-04,  5.5080e-04, -2.7847e-04,  1.2682e-04,  1.9660e-04,\n",
      "        -2.2924e-05,  2.0388e-06,  1.1763e-04,  5.3513e-05,  1.3830e-04,\n",
      "        -1.5011e-04,  4.1583e-05,  3.9369e-04,  2.4273e-04,  8.0904e-05,\n",
      "        -6.3920e-05,  1.7374e-04,  3.6240e-04, -1.6036e-04,  3.3214e-05,\n",
      "         2.6297e-04, -2.6300e-04, -2.5534e-04, -3.7407e-05,  6.4629e-04,\n",
      "         5.9196e-05, -5.1593e-04,  8.0495e-05,  5.1878e-04,  4.1690e-04,\n",
      "         2.4156e-04,  6.7047e-06, -4.1521e-04,  2.4074e-04, -7.1775e-06,\n",
      "        -2.1253e-04,  9.8053e-05, -7.5180e-05, -3.1938e-04, -1.8431e-04,\n",
      "        -5.1113e-04, -8.1093e-05, -9.1914e-05, -5.4240e-04,  5.6488e-05,\n",
      "         1.2361e-04,  6.3732e-04, -1.4098e-04, -3.2752e-04, -6.3117e-04,\n",
      "        -5.7862e-04,  8.1921e-05,  1.3935e-04,  1.5222e-04,  8.4249e-05,\n",
      "         2.2062e-04,  5.7759e-04, -2.1143e-04, -5.0653e-04,  4.6897e-04,\n",
      "         2.2169e-04,  1.5589e-04, -3.9044e-04,  2.9249e-04,  2.6369e-04]) \n",
      " tensor([ 0.0193, -0.0058, -0.0816,  0.1005, -0.1483,  0.0240,  0.1886, -0.2547,\n",
      "        -0.1823,  0.1459,  0.1033,  0.1093, -0.2343,  0.2002, -0.1158,  0.0107,\n",
      "         0.2789,  0.2061, -0.2556,  0.0521, -0.1575, -0.1298,  0.2167, -0.1776,\n",
      "         0.2387,  0.1137, -0.2242,  0.1120, -0.2164, -0.2206, -0.2195, -0.1886,\n",
      "         0.0045, -0.0481, -0.1165,  0.1386,  0.2447,  0.0164, -0.1936, -0.2203,\n",
      "         0.2649, -0.1131, -0.0154,  0.2056,  0.2700, -0.1714,  0.1143, -0.0053,\n",
      "         0.1623, -0.2295,  0.2484,  0.2055,  0.2092, -0.0095, -0.1285,  0.0206,\n",
      "        -0.1031,  0.1006, -0.1481,  0.0273, -0.0193,  0.0952, -0.1880,  0.1765,\n",
      "         0.1504,  0.0813, -0.1426, -0.1670,  0.0622,  0.1426,  0.2370, -0.0142,\n",
      "        -0.1189,  0.0969,  0.0251,  0.0753, -0.1625, -0.2267, -0.1929, -0.1939,\n",
      "        -0.2195, -0.1593, -0.0121,  0.2660, -0.2317, -0.2200,  0.1408,  0.1972,\n",
      "        -0.0447,  0.1900])\n",
      "decoder.model.lstm.weight_ih \n",
      " tensor([[ 7.8018e-07, -1.4521e-07,  7.2218e-08,  ..., -8.2430e-07,\n",
      "          2.0042e-06,  3.9033e-08],\n",
      "        [ 2.5039e-07, -5.8827e-08,  2.7247e-08,  ..., -2.4546e-07,\n",
      "          5.9787e-07, -1.1473e-08],\n",
      "        [ 3.4710e-06, -7.3266e-07,  3.4423e-07,  ..., -4.2336e-06,\n",
      "          1.0120e-05,  4.8626e-07],\n",
      "        ...,\n",
      "        [ 3.0582e-07, -4.9777e-08, -7.5109e-09,  ..., -3.0348e-07,\n",
      "          7.7457e-07,  2.0137e-08],\n",
      "        [ 2.7959e-06, -4.4140e-07, -5.5428e-08,  ..., -2.9279e-06,\n",
      "          7.2795e-06,  2.4314e-07],\n",
      "        [ 4.7575e-06, -7.0738e-07,  4.1908e-07,  ..., -5.7882e-06,\n",
      "          1.3770e-05,  8.9044e-07]]) \n",
      " tensor([[-0.1192,  0.1165, -0.1039,  ...,  0.0538, -0.1587,  0.1226],\n",
      "        [-0.1366,  0.1247, -0.0076,  ...,  0.0035,  0.0067, -0.0157],\n",
      "        [-0.2353,  0.1171,  0.0336,  ...,  0.1628, -0.1198,  0.1196],\n",
      "        ...,\n",
      "        [-0.0352,  0.1752, -0.0965,  ...,  0.0495, -0.0696,  0.0476],\n",
      "        [-0.1961,  0.2216, -0.0192,  ...,  0.0557, -0.0989,  0.0867],\n",
      "        [-0.1912,  0.2092, -0.0894,  ...,  0.1033, -0.0389,  0.2391]])\n",
      "decoder.model.lstm.weight_hh \n",
      " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) \n",
      " tensor([[ 0.1127, -0.0255,  0.0859,  ..., -0.0071, -0.0114,  0.0529],\n",
      "        [-0.0259, -0.0900,  0.0310,  ..., -0.0674, -0.0099,  0.0826],\n",
      "        [ 0.1306, -0.1727,  0.0570,  ..., -0.0519, -0.0817,  0.1391],\n",
      "        ...,\n",
      "        [ 0.1031,  0.0178,  0.0231,  ...,  0.0227, -0.1505,  0.1647],\n",
      "        [ 0.0108,  0.0044,  0.0560,  ..., -0.0607, -0.1626,  0.0929],\n",
      "        [ 0.1776, -0.1533,  0.1683,  ..., -0.0244, -0.2136,  0.2903]])\n",
      "decoder.model.lstm.bias_ih \n",
      " tensor([-9.6996e-07, -2.9766e-07, -4.7343e-06, -1.7856e-07, -1.4019e-05,\n",
      "        -3.9541e-07, -5.8898e-07, -1.1740e-05, -7.6475e-06, -5.5330e-07,\n",
      "        -5.0721e-07, -1.7520e-05, -4.9602e-07, -1.3851e-05, -7.6915e-07,\n",
      "        -2.1676e-07, -4.5587e-07, -8.7920e-07, -5.8548e-07, -1.8648e-05,\n",
      "        -4.8964e-07, -2.5218e-07, -6.7516e-07, -3.3495e-07, -6.7302e-07,\n",
      "        -1.8752e-07, -5.8751e-07, -1.6167e-07, -9.6694e-07, -5.4524e-07,\n",
      "        -1.0621e-05, -3.1954e-07, -1.0899e-06, -1.5336e-05, -1.4331e-05,\n",
      "        -9.0940e-07, -9.8945e-06, -6.1251e-07, -3.6919e-07, -1.6649e-05,\n",
      "        -3.3743e-07, -1.7376e-07, -7.0736e-07, -1.5652e-07, -2.3855e-07,\n",
      "        -1.1661e-07, -3.9885e-07, -5.2301e-07, -8.5420e-06, -5.7573e-07,\n",
      "        -1.6876e-05, -6.2413e-07, -5.7456e-07, -1.6538e-05, -6.6685e-06,\n",
      "        -1.1202e-06, -1.3478e-05, -1.7123e-07, -2.8211e-07, -8.4369e-07,\n",
      "        -1.3141e-05, -8.0380e-07, -5.7622e-07, -5.0540e-07, -2.2733e-07,\n",
      "        -1.3444e-05, -9.9965e-07, -5.2603e-07, -1.1873e-05, -7.9850e-07,\n",
      "        -5.5320e-07, -1.1439e-06, -5.6634e-07, -6.8423e-07, -7.9543e-06,\n",
      "        -1.1535e-06, -8.1020e-06, -1.5814e-07, -2.3536e-06, -5.8145e-07,\n",
      "        -5.3858e-07, -2.7271e-05, -8.4929e-07, -1.9068e-06, -2.0362e-07,\n",
      "        -3.0754e-07, -5.7759e-06, -3.1358e-07, -1.0525e-05, -8.4579e-06,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -2.8513e-05,  6.4084e-06, -1.2885e-05, -2.6393e-07,  8.1231e-06,\n",
      "        -2.0254e-07, -8.9789e-05,  6.9307e-06,  7.8778e-06, -1.5390e-07,\n",
      "        -7.8029e-05,  1.1247e-05,  2.7645e-07,  6.8727e-06,  8.1350e-05,\n",
      "         7.7494e-08, -5.1320e-07, -4.3392e-05, -2.6889e-07, -1.3186e-05,\n",
      "         2.1628e-07,  1.6974e-05, -1.4322e-06, -1.7078e-07, -1.1917e-04,\n",
      "         1.8596e-06,  2.2759e-04, -9.8139e-08,  1.0061e-05, -1.5241e-07,\n",
      "         6.8542e-06,  2.1773e-06, -7.0104e-06,  5.8716e-06,  7.2630e-06,\n",
      "        -3.5850e-06, -5.6633e-06, -1.6710e-07, -6.0142e-07, -8.8473e-06,\n",
      "         2.6688e-05,  2.0205e-07,  2.4684e-07,  2.0589e-07, -2.0415e-07,\n",
      "         5.8467e-06,  2.4390e-07, -9.5013e-07,  1.6826e-05,  2.5049e-05,\n",
      "         7.7810e-06, -4.8422e-07, -1.5305e-04, -1.7272e-03,  4.1969e-06,\n",
      "        -1.2071e-05,  3.7739e-06, -1.3481e-07,  3.1447e-07, -1.6353e-05,\n",
      "        -5.6855e-06, -3.5410e-05, -6.7614e-05,  2.4780e-06, -6.0373e-07,\n",
      "        -1.9335e-05, -7.0122e-05,  1.2453e-07,  3.4650e-06,  5.6946e-07,\n",
      "        -2.0980e-04, -5.8157e-07, -5.9807e-07,  2.0223e-04,  8.2804e-06,\n",
      "         4.6859e-07, -2.7792e-06,  1.7742e-07, -5.8794e-06,  1.0727e-06,\n",
      "        -1.3188e-04, -8.3993e-06,  8.3928e-05,  4.3680e-05,  3.0834e-07,\n",
      "        -1.6229e-07,  2.1979e-05,  3.1273e-05,  7.2359e-06, -6.6975e-06,\n",
      "        -1.0734e-06, -7.2446e-07, -4.3192e-06, -2.4951e-07, -4.0397e-06,\n",
      "        -5.2771e-07, -7.1241e-07, -3.3843e-06, -9.0650e-06, -3.4143e-07,\n",
      "        -8.8320e-07, -2.2910e-06, -8.3977e-07, -7.3090e-06, -5.1549e-07,\n",
      "        -2.2112e-07, -6.0222e-07, -8.0574e-07, -4.9907e-07, -3.0314e-06,\n",
      "        -5.7162e-07, -4.9395e-07, -6.3186e-07, -4.0400e-07, -1.3270e-06,\n",
      "        -3.0081e-07, -8.9587e-07, -3.6175e-07, -1.3376e-06, -5.1526e-07,\n",
      "        -5.5577e-06, -3.8748e-07, -1.2656e-06, -5.0626e-06, -3.5432e-06,\n",
      "        -9.2541e-07, -6.0971e-06, -7.0295e-07, -5.1508e-07, -3.5729e-06,\n",
      "        -4.2112e-07, -3.4255e-07, -1.2066e-06, -2.3998e-07, -4.8234e-07,\n",
      "        -6.1962e-07, -5.6139e-07, -7.6121e-07, -6.4309e-06, -8.2662e-07,\n",
      "        -6.7592e-06, -4.5715e-07, -9.9025e-07, -1.6296e-05, -2.3625e-06,\n",
      "        -9.1085e-07, -6.6048e-06, -3.7360e-07, -9.1494e-07, -1.5312e-06,\n",
      "        -3.5371e-06, -1.7235e-06, -1.0845e-06, -1.3121e-06, -3.9604e-07,\n",
      "        -5.5555e-06, -6.7758e-07, -6.2196e-07, -3.5195e-06, -9.5296e-07,\n",
      "        -1.4339e-06, -1.4586e-06, -8.3255e-07, -1.1520e-06, -5.7841e-06,\n",
      "        -1.0359e-06, -3.4391e-06, -6.3956e-07, -1.9854e-06, -1.0277e-06,\n",
      "        -1.5709e-06, -8.9496e-06, -8.1886e-07, -1.9453e-06, -4.1410e-07,\n",
      "        -6.5047e-07, -4.4369e-06, -3.6619e-07, -3.4903e-06, -6.4862e-06]) \n",
      " tensor([ 0.0737,  0.1088,  0.0968,  0.0210,  0.1342,  0.1366,  0.1904,  0.0580,\n",
      "         0.1655,  0.0149,  0.1646,  0.1914,  0.1361,  0.0895,  0.1413, -0.0190,\n",
      "         0.0588,  0.1488,  0.0446,  0.1534,  0.1886,  0.0085,  0.1547,  0.1590,\n",
      "         0.0579,  0.1922,  0.0578,  0.0708, -0.0071,  0.0363,  0.2096,  0.0034,\n",
      "         0.0218,  0.2230,  0.1942,  0.1844,  0.0651,  0.1302,  0.1529,  0.0985,\n",
      "         0.1177,  0.0114, -0.0100,  0.1307,  0.1910,  0.1587,  0.1457,  0.1689,\n",
      "         0.2105,  0.1572,  0.0603,  0.0066,  0.1508,  0.1230,  0.0548,  0.0929,\n",
      "         0.0785,  0.1466,  0.1651,  0.1637,  0.1464,  0.1370,  0.1474,  0.0354,\n",
      "         0.1641,  0.1122, -0.0033,  0.1833,  0.1071,  0.0756,  0.1318,  0.1533,\n",
      "         0.1832,  0.0566,  0.1393,  0.1394,  0.0805,  0.1408,  0.1027,  0.1445,\n",
      "         0.2019,  0.1338,  0.1553,  0.0711,  0.1473,  0.1414,  0.2192,  0.0349,\n",
      "         0.1401,  0.1075,  0.1847,  0.0215,  0.1110,  0.0557,  0.0618,  0.0203,\n",
      "        -0.0237,  0.1132,  0.1370, -0.0371,  0.1430,  0.1608,  0.0807,  0.0747,\n",
      "         0.1448,  0.1687,  0.0794,  0.0788, -0.0740,  0.0848,  0.0023,  0.0858,\n",
      "         0.1257,  0.1068,  0.0614,  0.1590,  0.1083,  0.0774,  0.1475,  0.0448,\n",
      "         0.0932,  0.0891,  0.0327,  0.0264,  0.1705,  0.0532,  0.0520,  0.1047,\n",
      "         0.0049,  0.0387,  0.0445,  0.0226, -0.0223,  0.0444, -0.0369,  0.1232,\n",
      "        -0.0732,  0.1109,  0.2080,  0.0018,  0.0181, -0.0617,  0.0848,  0.0374,\n",
      "         0.1857,  0.1408,  0.1193,  0.0251,  0.0788, -0.0933,  0.1705,  0.0487,\n",
      "        -0.0042,  0.0649,  0.0189,  0.2138,  0.1213, -0.0311,  0.1972, -0.1019,\n",
      "         0.1017,  0.1919,  0.0150, -0.0338,  0.0004,  0.0040,  0.1489,  0.0549,\n",
      "         0.0661,  0.0704,  0.0834,  0.1479,  0.1782,  0.0006,  0.0691, -0.0407,\n",
      "         0.1327,  0.1804,  0.2073,  0.1151, -0.0917,  0.0166,  0.2084,  0.0661,\n",
      "        -0.1880,  0.0197, -0.1002, -0.0289, -0.0424, -0.0253,  0.0353, -0.1849,\n",
      "         0.0310, -0.1697, -0.0636, -0.1061, -0.0651,  0.0902, -0.0016,  0.1036,\n",
      "         0.0162, -0.0162, -0.0025,  0.0078, -0.0324,  0.0134,  0.0958,  0.0413,\n",
      "        -0.0378,  0.1034, -0.0555, -0.1023,  0.0723, -0.1402, -0.2068, -0.0011,\n",
      "         0.1107,  0.0958, -0.0686,  0.1474, -0.0269, -0.1209,  0.0093, -0.0796,\n",
      "         0.0793,  0.0496, -0.0855,  0.0942, -0.2156, -0.1022, -0.1221, -0.0179,\n",
      "         0.0438, -0.0083, -0.1476, -0.0610, -0.1604,  0.1064, -0.0747, -0.0997,\n",
      "         0.1302, -0.0887, -0.1109,  0.0783,  0.0190,  0.0439,  0.0396, -0.0953,\n",
      "        -0.1991,  0.0457, -0.0310, -0.0076,  0.1145,  0.0542, -0.1035, -0.0735,\n",
      "         0.0447, -0.0012, -0.0668,  0.0362, -0.0700,  0.0557,  0.0777, -0.0350,\n",
      "        -0.0178, -0.0358, -0.1421,  0.0914, -0.1232,  0.1353,  0.1745,  0.1294,\n",
      "         0.2182,  0.0826,  0.1837, -0.0016,  0.1656,  0.1828,  0.1311,  0.1624,\n",
      "         0.0814,  0.2232,  0.0522,  0.0733,  0.1239,  0.1935,  0.1151,  0.2150,\n",
      "         0.0277,  0.0877,  0.0841,  0.1558,  0.1994,  0.1011,  0.1491,  0.0647,\n",
      "         0.1858,  0.1603,  0.0643,  0.1745,  0.1198,  0.0388,  0.0264,  0.0963,\n",
      "         0.1464,  0.1531,  0.1854, -0.0102,  0.1625,  0.0887,  0.0958,  0.0527,\n",
      "        -0.0048,  0.0834,  0.0563,  0.0947,  0.2034,  0.1449,  0.1357,  0.0973,\n",
      "         0.0468,  0.0568,  0.1538,  0.1648,  0.2333,  0.1025,  0.1769,  0.0755,\n",
      "        -0.0112, -0.0191,  0.2418,  0.1787,  0.0528,  0.0659,  0.1061,  0.2349,\n",
      "         0.0583,  0.0558,  0.1158,  0.0742,  0.0416,  0.0565,  0.0937, -0.0159,\n",
      "         0.0375,  0.1605,  0.1635,  0.0027,  0.1689,  0.1460,  0.0430,  0.2283,\n",
      "         0.1744,  0.0738,  0.0061,  0.0833,  0.2223,  0.1237,  0.1777,  0.1953])\n",
      "decoder.model.lstm.bias_hh \n",
      " tensor([-9.6996e-07, -2.9766e-07, -4.7343e-06, -1.7856e-07, -1.4019e-05,\n",
      "        -3.9541e-07, -5.8898e-07, -1.1740e-05, -7.6475e-06, -5.5330e-07,\n",
      "        -5.0721e-07, -1.7520e-05, -4.9602e-07, -1.3851e-05, -7.6915e-07,\n",
      "        -2.1676e-07, -4.5587e-07, -8.7920e-07, -5.8548e-07, -1.8648e-05,\n",
      "        -4.8964e-07, -2.5218e-07, -6.7516e-07, -3.3495e-07, -6.7302e-07,\n",
      "        -1.8752e-07, -5.8751e-07, -1.6167e-07, -9.6694e-07, -5.4524e-07,\n",
      "        -1.0621e-05, -3.1954e-07, -1.0899e-06, -1.5336e-05, -1.4331e-05,\n",
      "        -9.0940e-07, -9.8945e-06, -6.1251e-07, -3.6919e-07, -1.6649e-05,\n",
      "        -3.3743e-07, -1.7376e-07, -7.0736e-07, -1.5652e-07, -2.3855e-07,\n",
      "        -1.1661e-07, -3.9885e-07, -5.2301e-07, -8.5420e-06, -5.7573e-07,\n",
      "        -1.6876e-05, -6.2413e-07, -5.7456e-07, -1.6538e-05, -6.6685e-06,\n",
      "        -1.1202e-06, -1.3478e-05, -1.7123e-07, -2.8211e-07, -8.4369e-07,\n",
      "        -1.3141e-05, -8.0380e-07, -5.7622e-07, -5.0540e-07, -2.2733e-07,\n",
      "        -1.3444e-05, -9.9965e-07, -5.2603e-07, -1.1873e-05, -7.9850e-07,\n",
      "        -5.5320e-07, -1.1439e-06, -5.6634e-07, -6.8423e-07, -7.9543e-06,\n",
      "        -1.1535e-06, -8.1020e-06, -1.5814e-07, -2.3536e-06, -5.8145e-07,\n",
      "        -5.3858e-07, -2.7271e-05, -8.4929e-07, -1.9068e-06, -2.0362e-07,\n",
      "        -3.0754e-07, -5.7759e-06, -3.1358e-07, -1.0525e-05, -8.4579e-06,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -2.8513e-05,  6.4084e-06, -1.2885e-05, -2.6393e-07,  8.1231e-06,\n",
      "        -2.0254e-07, -8.9789e-05,  6.9307e-06,  7.8778e-06, -1.5390e-07,\n",
      "        -7.8029e-05,  1.1247e-05,  2.7645e-07,  6.8727e-06,  8.1350e-05,\n",
      "         7.7494e-08, -5.1320e-07, -4.3392e-05, -2.6889e-07, -1.3186e-05,\n",
      "         2.1628e-07,  1.6974e-05, -1.4322e-06, -1.7078e-07, -1.1917e-04,\n",
      "         1.8596e-06,  2.2759e-04, -9.8139e-08,  1.0061e-05, -1.5241e-07,\n",
      "         6.8542e-06,  2.1773e-06, -7.0104e-06,  5.8716e-06,  7.2630e-06,\n",
      "        -3.5850e-06, -5.6633e-06, -1.6710e-07, -6.0142e-07, -8.8473e-06,\n",
      "         2.6688e-05,  2.0205e-07,  2.4684e-07,  2.0589e-07, -2.0415e-07,\n",
      "         5.8467e-06,  2.4390e-07, -9.5013e-07,  1.6826e-05,  2.5049e-05,\n",
      "         7.7810e-06, -4.8422e-07, -1.5305e-04, -1.7272e-03,  4.1969e-06,\n",
      "        -1.2071e-05,  3.7739e-06, -1.3481e-07,  3.1447e-07, -1.6353e-05,\n",
      "        -5.6855e-06, -3.5410e-05, -6.7614e-05,  2.4780e-06, -6.0373e-07,\n",
      "        -1.9335e-05, -7.0122e-05,  1.2453e-07,  3.4650e-06,  5.6946e-07,\n",
      "        -2.0980e-04, -5.8157e-07, -5.9807e-07,  2.0223e-04,  8.2804e-06,\n",
      "         4.6859e-07, -2.7792e-06,  1.7742e-07, -5.8794e-06,  1.0727e-06,\n",
      "        -1.3188e-04, -8.3993e-06,  8.3928e-05,  4.3680e-05,  3.0834e-07,\n",
      "        -1.6229e-07,  2.1979e-05,  3.1273e-05,  7.2359e-06, -6.6975e-06,\n",
      "        -1.0734e-06, -7.2446e-07, -4.3192e-06, -2.4951e-07, -4.0397e-06,\n",
      "        -5.2771e-07, -7.1241e-07, -3.3843e-06, -9.0650e-06, -3.4143e-07,\n",
      "        -8.8320e-07, -2.2910e-06, -8.3977e-07, -7.3090e-06, -5.1549e-07,\n",
      "        -2.2112e-07, -6.0222e-07, -8.0574e-07, -4.9907e-07, -3.0314e-06,\n",
      "        -5.7162e-07, -4.9395e-07, -6.3186e-07, -4.0400e-07, -1.3270e-06,\n",
      "        -3.0081e-07, -8.9587e-07, -3.6175e-07, -1.3376e-06, -5.1526e-07,\n",
      "        -5.5577e-06, -3.8748e-07, -1.2656e-06, -5.0626e-06, -3.5432e-06,\n",
      "        -9.2541e-07, -6.0971e-06, -7.0295e-07, -5.1508e-07, -3.5729e-06,\n",
      "        -4.2112e-07, -3.4255e-07, -1.2066e-06, -2.3998e-07, -4.8234e-07,\n",
      "        -6.1962e-07, -5.6139e-07, -7.6121e-07, -6.4309e-06, -8.2662e-07,\n",
      "        -6.7592e-06, -4.5715e-07, -9.9025e-07, -1.6296e-05, -2.3625e-06,\n",
      "        -9.1085e-07, -6.6048e-06, -3.7360e-07, -9.1494e-07, -1.5312e-06,\n",
      "        -3.5371e-06, -1.7235e-06, -1.0845e-06, -1.3121e-06, -3.9604e-07,\n",
      "        -5.5555e-06, -6.7758e-07, -6.2196e-07, -3.5195e-06, -9.5296e-07,\n",
      "        -1.4339e-06, -1.4586e-06, -8.3255e-07, -1.1520e-06, -5.7841e-06,\n",
      "        -1.0359e-06, -3.4391e-06, -6.3956e-07, -1.9854e-06, -1.0277e-06,\n",
      "        -1.5709e-06, -8.9496e-06, -8.1886e-07, -1.9453e-06, -4.1410e-07,\n",
      "        -6.5047e-07, -4.4369e-06, -3.6619e-07, -3.4903e-06, -6.4862e-06]) \n",
      " tensor([ 7.1738e-02,  1.5689e-02,  1.6619e-01,  1.7335e-02,  1.2364e-01,\n",
      "         4.6007e-02,  1.7903e-01,  6.5190e-02,  3.2770e-02,  1.9452e-01,\n",
      "         4.2740e-02,  1.2359e-01,  1.2162e-01,  1.5170e-01,  9.3575e-02,\n",
      "         2.5288e-02, -9.1733e-03,  1.6453e-01,  1.5501e-01,  1.7635e-01,\n",
      "         1.7360e-01,  7.0631e-02,  1.7970e-01,  1.5510e-01,  4.0461e-02,\n",
      "         5.8836e-02,  1.8430e-01, -1.6849e-02,  1.4829e-01,  1.2473e-01,\n",
      "         9.8816e-02,  1.0629e-01,  4.7577e-02,  6.7114e-02,  1.9524e-01,\n",
      "         9.1117e-02,  2.0889e-01,  1.1075e-01, -1.7188e-02,  7.0841e-02,\n",
      "         1.1203e-01,  1.5215e-01,  9.6401e-02,  8.7324e-02,  1.4213e-01,\n",
      "         6.0528e-02,  1.4977e-01,  1.7658e-01,  2.3820e-01,  3.7005e-03,\n",
      "         2.0316e-01, -1.2081e-03,  3.1367e-02,  9.8768e-03,  7.5960e-02,\n",
      "         1.1216e-01,  1.3485e-01, -1.7467e-02,  1.4640e-01,  1.0275e-01,\n",
      "         1.3658e-01,  2.2055e-02,  9.6268e-02,  1.8875e-01,  1.5926e-02,\n",
      "         1.7116e-01,  3.4503e-03, -1.6332e-02,  2.0061e-01,  1.8636e-01,\n",
      "         2.2652e-02,  4.5953e-02,  1.5150e-01, -5.0935e-03,  7.3626e-02,\n",
      "         1.7039e-02,  1.1060e-01,  5.6998e-02,  1.3424e-01,  8.5412e-02,\n",
      "         1.6073e-01,  6.3613e-02,  1.4324e-01,  2.5307e-02,  3.7553e-02,\n",
      "         2.5962e-02,  6.6844e-02,  6.1674e-02,  9.0024e-02,  1.8275e-01,\n",
      "         1.4994e-01, -8.0597e-02,  1.6820e-01, -9.9036e-03,  4.8207e-02,\n",
      "        -8.2390e-02,  9.5108e-02,  1.8209e-01,  1.5527e-02,  5.7055e-02,\n",
      "         5.3577e-02,  1.4078e-01,  4.9802e-02,  1.9887e-01,  5.3361e-02,\n",
      "         7.3839e-03, -2.1853e-02,  2.8113e-02,  1.9979e-02,  1.3607e-01,\n",
      "         1.1627e-01,  1.3988e-01,  1.5475e-02,  8.2553e-03,  1.9248e-01,\n",
      "         1.7206e-01, -3.0608e-02,  7.6871e-02,  6.2203e-02,  1.1363e-01,\n",
      "         8.0395e-02,  7.0970e-02,  1.2015e-01,  1.0677e-01,  1.1926e-01,\n",
      "         8.9690e-02,  1.2352e-01,  3.1708e-02,  8.2924e-02,  1.0773e-02,\n",
      "         8.2890e-02,  7.7790e-02,  3.1175e-02,  1.6518e-01,  4.0533e-02,\n",
      "         4.5421e-02,  4.5928e-02, -2.3247e-02,  3.5881e-02,  9.7389e-02,\n",
      "         2.9759e-02, -6.6486e-02,  2.3359e-02,  1.7501e-01,  1.1669e-01,\n",
      "        -1.1753e-02,  1.2430e-01, -2.6245e-02, -2.7542e-02,  3.1978e-02,\n",
      "         9.1245e-02,  4.7531e-02, -3.0651e-03, -7.7520e-02,  5.2347e-02,\n",
      "         1.8274e-01,  6.0498e-02, -5.1001e-02,  1.4711e-02,  1.0624e-01,\n",
      "         1.2317e-01,  5.3302e-02, -3.0531e-02,  7.5017e-02,  4.3240e-02,\n",
      "         4.2912e-02,  1.1436e-01,  6.9157e-02,  1.5289e-01, -1.4847e-02,\n",
      "         8.6644e-02,  1.8164e-01,  1.3502e-01,  1.7126e-01,  1.6288e-01,\n",
      "         2.5799e-02,  2.0353e-01,  6.8247e-04,  1.2412e-01,  2.0082e-01,\n",
      "         4.5361e-02,  1.2051e-02,  1.7076e-01, -1.5934e-03, -1.0111e-01,\n",
      "         9.5673e-02,  2.3340e-02, -4.5567e-02, -1.4171e-01,  6.9691e-02,\n",
      "         9.1484e-02, -2.0534e-01,  8.1418e-02, -4.6780e-02, -6.5409e-03,\n",
      "        -1.2115e-01, -6.5616e-02,  3.4767e-02,  8.6172e-02,  1.2923e-01,\n",
      "        -9.9039e-02, -1.1140e-03, -6.1942e-02, -3.1558e-02,  6.2419e-02,\n",
      "        -7.8444e-02, -6.4319e-03, -4.8586e-02,  1.1475e-01,  9.7815e-02,\n",
      "        -1.3048e-01,  4.1958e-02,  9.1230e-02, -1.9875e-01, -5.8051e-02,\n",
      "         8.3911e-02,  1.2230e-01,  1.2809e-01, -3.0915e-02,  3.7230e-02,\n",
      "         3.5006e-02, -8.7289e-02, -7.2024e-02,  1.8232e-02,  1.1021e-02,\n",
      "         3.8060e-02, -8.9800e-03,  5.1584e-02, -9.7046e-02,  5.1255e-02,\n",
      "        -2.7824e-03,  4.0526e-02,  9.0858e-02,  8.8757e-02, -1.4935e-01,\n",
      "         2.0323e-03, -1.5857e-01,  1.2036e-01, -2.0632e-02, -4.4511e-02,\n",
      "         1.8774e-01, -9.4219e-03,  7.2161e-02, -4.4181e-02, -4.7768e-04,\n",
      "         1.6028e-01, -5.6853e-02, -7.5457e-02, -1.0375e-02, -7.8231e-03,\n",
      "         7.2981e-02, -5.4416e-02,  1.1703e-01,  8.8744e-02, -1.3416e-01,\n",
      "         2.2686e-02,  2.7072e-02, -8.4831e-02, -2.5826e-02, -1.2682e-01,\n",
      "         7.5267e-02,  3.7326e-02,  2.0671e-02,  9.9062e-02, -5.3022e-02,\n",
      "        -1.0062e-02, -1.7984e-01, -3.2643e-02, -9.5990e-02,  3.4145e-02,\n",
      "         2.4832e-02,  1.1439e-01,  1.6665e-01,  3.4131e-02,  1.5574e-01,\n",
      "        -2.3913e-02,  1.0434e-01,  9.2517e-02,  1.1555e-01,  1.7649e-01,\n",
      "         4.7584e-02,  1.6155e-01,  3.0967e-02,  1.6316e-01,  1.8572e-01,\n",
      "         1.2555e-01,  2.5761e-02,  1.0796e-02,  5.8047e-02,  1.1089e-01,\n",
      "         5.5353e-02,  8.3145e-02,  6.4570e-02,  5.2488e-02,  1.3814e-02,\n",
      "         1.6736e-01,  6.5231e-02,  1.4571e-01,  6.3314e-02,  2.4203e-02,\n",
      "         1.6638e-01, -3.7811e-03, -1.5845e-04,  1.0233e-02,  1.9855e-01,\n",
      "         1.2345e-01,  5.5181e-02,  1.6354e-03, -1.8265e-02,  1.4589e-01,\n",
      "         1.8994e-01,  1.0800e-01,  3.6976e-02,  6.1405e-02,  8.1235e-02,\n",
      "         6.0867e-02,  4.9735e-02,  8.5847e-02,  5.2165e-02,  8.6167e-02,\n",
      "         1.2276e-01,  1.9569e-01,  2.0990e-02,  6.0897e-02,  2.1413e-01,\n",
      "         1.3515e-01,  9.2342e-02,  5.7583e-02, -2.1180e-02,  1.5424e-01,\n",
      "         1.7149e-01,  7.4188e-02,  1.3263e-01,  7.6402e-02,  3.6432e-02,\n",
      "         2.1058e-01,  1.6422e-01,  5.7183e-02,  2.1523e-01,  5.6144e-02,\n",
      "         8.2958e-02,  2.1021e-01,  6.5443e-02,  7.9097e-02,  1.7983e-01,\n",
      "         1.3519e-01,  1.4151e-01,  7.1356e-02, -7.8145e-03,  1.2434e-01,\n",
      "         2.6807e-02,  1.1730e-01,  8.4432e-02,  3.4304e-02,  9.1586e-02,\n",
      "         3.4862e-02,  1.2551e-01,  1.1759e-01,  3.7572e-02,  2.2518e-01])\n",
      "decoder.model.phased_cell.tau \n",
      " tensor([ 9.7537e-04,  7.1429e-04,  6.7463e-04,  4.7202e-04,  2.0519e-04,\n",
      "         1.0597e-02,  5.0677e-04,  1.1837e-03,  7.4368e-04,  1.8615e-03,\n",
      "         4.2309e-04,  3.7111e-04,  4.3623e-04,  2.3171e-03,  2.9239e-04,\n",
      "         1.9436e-04,  6.6397e-04,  9.5202e-03,  1.8501e-03,  1.8177e-04,\n",
      "         9.1663e-04,  7.5302e-04,  5.3704e-04,  1.6992e-03,  6.0898e-04,\n",
      "         2.0172e-04,  7.3860e-04,  1.2493e-04,  1.4062e-03,  4.2991e-01,\n",
      "         7.2660e-04,  7.6878e-04,  2.4965e-03,  5.8716e-04,  6.5364e-04,\n",
      "         7.0515e-04,  7.2018e-04,  1.2132e-03,  6.9425e-04,  3.1072e-04,\n",
      "         1.0523e-03,  2.0095e-04,  2.6966e-03,  1.8039e-04,  2.4181e-04,\n",
      "         2.1109e-04,  7.1712e-04,  8.0178e-03,  6.5172e-04,  9.7016e-04,\n",
      "         5.2155e-04,  8.7854e-04,  1.1648e-03, -2.7869e-04,  1.0969e-03,\n",
      "         2.4232e-03,  4.9395e-04,  1.0748e-03,  1.5261e-03,  1.8881e-03,\n",
      "         6.6738e-04,  9.4272e-04,  1.0221e-03,  1.1321e-03,  2.9817e-04,\n",
      "         7.3990e-04,  7.9867e-04,  9.3172e-04,  1.3008e-04,  2.0571e-03,\n",
      "         6.5132e-04,  2.5377e-03,  1.1228e-03,  1.8751e-03,  2.1992e-03,\n",
      "         7.4756e-04,  2.1668e-03,  1.5063e-03,  5.1881e-03,  2.6153e-03,\n",
      "         2.8062e-03,  6.4531e-04,  3.8466e-03,  1.1777e-03,  2.5270e-03,\n",
      "         1.2192e-03,  1.0201e-03,  5.0163e-04,  6.1080e-05,  1.1177e-03]) \n",
      " tensor([3.0581, 1.8429, 0.5546, 1.8744, 1.8955, 0.2738, 4.6641, 0.9583, 0.7131,\n",
      "        1.6870, 3.5927, 1.1166, 4.6630, 0.1218, 5.1114, 3.5526, 3.3282, 0.3768,\n",
      "        1.9448, 2.4812, 2.5235, 1.5876, 4.8467, 1.2740, 4.1883, 3.7496, 3.0165,\n",
      "        4.7664, 1.4909, 0.0057, 0.6236, 1.8836, 1.3019, 1.3678, 0.9150, 5.0960,\n",
      "        0.7351, 2.3168, 3.0772, 1.3929, 1.2331, 4.0257, 0.6853, 3.2842, 3.3632,\n",
      "        2.7045, 2.4433, 0.2275, 0.8012, 1.9584, 0.8416, 2.2165, 2.2105, 3.7738,\n",
      "        0.3005, 1.2944, 0.8923, 1.2150, 1.3975, 1.4160, 0.9745, 2.8916, 1.5184,\n",
      "        3.0095, 3.0532, 0.6747, 2.4775, 2.9203, 4.2637, 1.8988, 4.4512, 1.5523,\n",
      "        2.6914, 1.2519, 0.1439, 5.0785, 0.1609, 1.6477, 0.7814, 1.4216, 1.1359,\n",
      "        0.6139, 0.6128, 3.0871, 0.8076, 1.3044, 0.4585, 1.9888, 3.6796, 0.4655])\n",
      "decoder.model.phased_cell.phase \n",
      " None \n",
      " tensor([2.9722e+00, 1.4952e+00, 4.3151e-01, 1.3323e+00, 9.5042e-01, 1.1584e+00,\n",
      "        3.5039e+00, 9.9245e-01, 2.6885e-01, 2.1787e+00, 1.9748e+00, 9.7208e-01,\n",
      "        3.2885e+00, 8.7497e-01, 2.8154e+00, 1.4362e+00, 2.3017e+00, 6.9492e-01,\n",
      "        2.5133e+00, 9.3797e-01, 2.0606e+00, 1.4142e+00, 3.6454e+00, 4.2727e-01,\n",
      "        3.3169e+00, 1.5670e+00, 2.2999e+00, 1.5763e+00, 1.6611e+00, 8.0306e-01,\n",
      "        3.6072e-01, 4.6903e-01, 2.7243e-02, 9.8126e-01, 6.1807e-02, 4.9098e+00,\n",
      "        2.4631e-01, 2.2442e+00, 2.4230e+00, 1.0348e+00, 1.3254e+00, 3.5737e-01,\n",
      "        7.6266e-01, 6.4987e-01, 1.5303e+00, 6.2770e-01, 9.6173e-02, 9.1902e-01,\n",
      "        1.7868e-01, 2.7329e-01, 1.0207e+00, 1.7705e+00, 2.1327e+00, 1.2947e+00,\n",
      "        6.9200e-01, 1.9998e+00, 8.5119e-02, 1.3098e+00, 3.4480e-01, 2.0578e-01,\n",
      "        1.0511e-03, 2.8789e+00, 1.5010e+00, 3.2563e+00, 4.1510e-01, 3.0832e-01,\n",
      "        2.3533e-02, 2.8876e+00, 1.0804e+00, 2.5352e+00, 4.1539e+00, 2.2068e+00,\n",
      "        2.7254e+00, 2.7388e-01, 8.5227e-01, 5.2321e+00, 8.3553e-01, 1.7146e-01,\n",
      "        3.1720e-01, 2.0922e+00, 1.8893e-01, 1.0154e+00, 1.3180e+00, 3.4335e+00,\n",
      "        1.3665e+00, 1.3647e+00, 5.2979e-01, 5.8532e-01, 8.4507e-01, 5.2289e-01])\n",
      "decoder.hidden_to_output.weight \n",
      " tensor([[-1.6261e-03,  7.0903e-04, -1.8298e-02,  ...,  5.4072e-04,\n",
      "          1.8357e-02, -1.8295e-02],\n",
      "        [-6.3820e-05,  2.8049e-05, -8.1521e-04,  ...,  2.0086e-05,\n",
      "          8.3066e-04, -8.1443e-04],\n",
      "        [ 5.5113e-05, -2.4071e-05,  2.1392e-04,  ..., -1.9878e-05,\n",
      "         -1.9551e-04,  2.1473e-04],\n",
      "        ...,\n",
      "        [-1.9213e-04,  8.0120e-05, -2.2567e-03,  ...,  6.7227e-05,\n",
      "          2.2587e-03, -2.2563e-03],\n",
      "        [-2.3312e-05,  8.8059e-06, -9.9446e-04,  ...,  7.2101e-06,\n",
      "          1.0130e-03, -9.9363e-04],\n",
      "        [-5.7244e-06,  2.7502e-07, -1.2370e-03,  ...,  1.2518e-06,\n",
      "          1.2597e-03, -1.2359e-03]]) \n",
      " tensor([[ 0.6259, -0.5396,  0.9354,  ..., -0.8473, -0.5184,  1.0360],\n",
      "        [-0.1425,  0.1087,  0.0795,  ...,  0.1350, -0.1956, -0.1392],\n",
      "        [ 0.0891,  0.0272,  0.1180,  ...,  0.0392, -0.4152,  0.2800],\n",
      "        ...,\n",
      "        [ 0.3000, -0.2534,  0.2158,  ..., -0.3747, -0.1585,  0.1060],\n",
      "        [-0.1139, -0.1325,  0.1316,  ...,  0.0894, -0.4499,  0.3124],\n",
      "        [-0.0097, -0.0341,  0.3889,  ..., -0.2299, -0.2247,  0.3435]])\n",
      "decoder.hidden_to_output.bias \n",
      " tensor([-0.0241, -0.0011,  0.0003, -0.0021, -0.0021, -0.0011, -0.0002, -0.0015,\n",
      "        -0.0018, -0.0022, -0.0366, -0.0010, -0.0012, -0.0023, -0.0016, -0.0015,\n",
      "        -0.0030, -0.0013, -0.0017]) \n",
      " tensor([0.5629, 0.1004, 0.2639, 0.3771, 0.2678, 0.2247, 0.0774, 0.2473, 0.2454,\n",
      "        0.2250, 0.6343, 0.2218, 0.1665, 0.4636, 0.2939, 0.3242, 0.2317, 0.2398,\n",
      "        0.2397])\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(n, '\\n', p.grad, '\\n', p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2298,  0.9993,  1.1570,  ...,  0.5158,  1.2001,  2.0645],\n",
       "        [-0.7551, -0.0446,  0.3882,  ..., -0.5571,  0.4411,  1.6011],\n",
       "        [ 0.8026,  0.5022, -1.3479,  ...,  0.9216, -0.4514, -1.2693],\n",
       "        ...,\n",
       "        [-0.8640,  0.2775,  0.8303,  ...,  2.0807,  0.2537,  0.0259],\n",
       "        [-1.5042,  0.7906,  0.2099,  ..., -0.9292,  0.4017,  1.6255],\n",
       "        [ 0.3486,  0.3889, -0.6923,  ...,  1.2951, -0.4356, -1.1198]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1667)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([[0,0,0],[0.5,0,0],[0,0,0]])\n",
    "a = a  # + 1e-5\n",
    "b = torch.LongTensor([1,0,1])\n",
    "cl_loss_fn = torch.nn.NLLLoss()\n",
    "cl_loss = cl_loss_fn(a,b)\n",
    "cl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e-05, 1.0000e-05, 1.0000e-05],\n",
       "        [5.0000e-01, 1.0000e-05, 1.0000e-05],\n",
       "        [1.0000e-05, 1.0000e-05, 1.0000e-05]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a==0] += 1e-5\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.9112e-02,  1.2708e-01, -9.4901e-02,  ..., -3.1674e-01,\n",
       "          2.2952e-02,  1.9503e-01],\n",
       "        [-1.7971e-01,  6.1885e-01, -1.5249e-01,  ..., -1.0224e-01,\n",
       "          2.4602e-03,  1.7789e-01],\n",
       "        [ 2.2590e-02, -1.6115e-01,  1.9003e-01,  ..., -1.0203e-02,\n",
       "          3.5609e-03,  1.4919e-03],\n",
       "        ...,\n",
       "        [ 1.6272e-01,  6.4871e-01,  1.9312e-01,  ...,  7.9410e-03,\n",
       "          5.8570e-02,  2.8513e-02],\n",
       "        [-1.7712e-01,  6.4660e-01, -1.8619e-01,  ...,  7.6034e-01,\n",
       "          1.6913e-03,  2.0129e-01],\n",
       "        [ 4.5560e-02, -2.7322e-01,  1.9220e-01,  ..., -6.0587e-04,\n",
       "          3.1728e-03,  1.0741e-03]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cell_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0938e-01,  1.6640e-01, -1.1733e-01,  ..., -3.3476e-01,\n",
       "           2.5190e-02,  2.9042e-01],\n",
       "         [-2.3589e-01,  8.0084e-01, -1.9713e-01,  ..., -9.9090e-02,\n",
       "           2.4997e-03,  2.3425e-01],\n",
       "         [ 2.3158e-01, -1.7619e-01,  2.4627e-01,  ..., -1.0205e-02,\n",
       "           7.8403e-02,  2.7764e-01],\n",
       "         ...,\n",
       "         [ 2.1043e-01,  8.2889e-01,  2.5290e-01,  ...,  8.4661e-03,\n",
       "           7.6851e-02,  2.2670e-01],\n",
       "         [-2.3196e-01,  8.4477e-01, -2.4314e-01,  ...,  9.9703e-01,\n",
       "           1.7901e-03,  2.5981e-01],\n",
       "         [ 2.3226e-01, -3.0311e-01,  2.5084e-01,  ..., -6.0587e-04,\n",
       "           7.8402e-02,  2.3603e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.model.phased_cell.c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.fc.weight \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]]) \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "encoder.fc.bias \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]) \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "encoder.model.lstm.weight_ih \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]]) \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "encoder.model.lstm.weight_hh \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]]) \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "encoder.model.lstm.bias_ih \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]) \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "encoder.model.lstm.bias_hh \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]) \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "encoder.model.phased_cell.tau \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]) \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "encoder.model.phased_cell.phase \n",
      " None \n",
      " tensor([4.3579, 1.0059, 3.9284, 2.2026, 1.6147, 1.8327, 5.5256, 0.7834, 2.0463,\n",
      "        0.6080, 0.2822, 0.6000, 0.0391, 0.1464, 0.5916, 3.9748, 0.4988, 0.5253,\n",
      "        1.6675, 0.7863, 0.2739, 2.8919, 0.2172, 0.8328, 0.4840, 3.0377, 1.4090,\n",
      "        0.1706, 1.4264, 0.2346, 2.3351, 1.2407, 1.8235, 1.7531, 1.8264, 4.8685,\n",
      "        1.5669, 0.8452, 1.6010, 3.4169, 0.7335, 0.3655, 0.2062, 0.7669, 0.8432,\n",
      "        1.0561, 1.4645, 2.1673, 0.0534, 1.0374, 1.3027, 0.9339, 4.0773, 0.7087,\n",
      "        1.3104, 0.9358, 0.3599, 0.6380, 2.5711, 0.7495, 0.6246, 0.4843, 0.7930,\n",
      "        0.1423, 1.0167, 0.2031, 1.7839, 0.7762, 0.8769, 0.5366, 0.7869, 2.4897,\n",
      "        0.8787, 1.7472, 1.3667, 0.3119, 2.3740, 2.2957, 0.6830, 0.2749, 0.7797,\n",
      "        1.1204, 0.9850, 0.3341, 0.7986, 0.7178, 1.1555, 0.8661, 1.5925, 0.2110])\n",
      "lmbd.hidden_to_mean.weight \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]]) \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "lmbd.hidden_to_mean.bias \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]) \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "classifier.0.weight \n",
      " tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]) \n",
      " tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
      "classifier.0.bias \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]) \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "decoder.latent_to_hidden.weight \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]]) \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "decoder.latent_to_hidden.bias \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]) \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "decoder.model.lstm.weight_ih \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]]) \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "decoder.model.lstm.weight_hh \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]]) \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "decoder.model.lstm.bias_ih \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]) \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "decoder.model.lstm.bias_hh \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]) \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "decoder.model.phased_cell.tau \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]) \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "decoder.model.phased_cell.phase \n",
      " None \n",
      " tensor([2.9722e+00, 1.4952e+00, 4.3151e-01, 1.3323e+00, 9.5042e-01, 1.1584e+00,\n",
      "        3.5039e+00, 9.9245e-01, 2.6885e-01, 2.1787e+00, 1.9748e+00, 9.7208e-01,\n",
      "        3.2885e+00, 8.7497e-01, 2.8154e+00, 1.4362e+00, 2.3017e+00, 6.9492e-01,\n",
      "        2.5133e+00, 9.3797e-01, 2.0606e+00, 1.4142e+00, 3.6454e+00, 4.2727e-01,\n",
      "        3.3169e+00, 1.5670e+00, 2.2999e+00, 1.5763e+00, 1.6611e+00, 8.0306e-01,\n",
      "        3.6072e-01, 4.6903e-01, 2.7243e-02, 9.8126e-01, 6.1807e-02, 4.9098e+00,\n",
      "        2.4631e-01, 2.2442e+00, 2.4230e+00, 1.0348e+00, 1.3254e+00, 3.5737e-01,\n",
      "        7.6266e-01, 6.4987e-01, 1.5303e+00, 6.2770e-01, 9.6173e-02, 9.1902e-01,\n",
      "        1.7868e-01, 2.7329e-01, 1.0207e+00, 1.7705e+00, 2.1327e+00, 1.2947e+00,\n",
      "        6.9200e-01, 1.9998e+00, 8.5119e-02, 1.3098e+00, 3.4480e-01, 2.0578e-01,\n",
      "        1.0511e-03, 2.8789e+00, 1.5010e+00, 3.2563e+00, 4.1510e-01, 3.0832e-01,\n",
      "        2.3533e-02, 2.8876e+00, 1.0804e+00, 2.5352e+00, 4.1539e+00, 2.2068e+00,\n",
      "        2.7254e+00, 2.7388e-01, 8.5227e-01, 5.2321e+00, 8.3553e-01, 1.7146e-01,\n",
      "        3.1720e-01, 2.0922e+00, 1.8893e-01, 1.0154e+00, 1.3180e+00, 3.4335e+00,\n",
      "        1.3665e+00, 1.3647e+00, 5.2979e-01, 5.8532e-01, 8.4507e-01, 5.2289e-01])\n",
      "decoder.hidden_to_output.weight \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]]) \n",
      " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "decoder.hidden_to_output.bias \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]) \n",
      " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(n, '\\n', p.grad, '\\n', p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "the derivative for 'fmod: other' is not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-1da8264fc211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/AE/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/AE/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: the derivative for 'fmod: other' is not implemented"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([5.0])\n",
    "b = torch.tensor([2.0], requires_grad=True)\n",
    "c = torch.fmod(a, b)\n",
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# B30 before grad vanishing\n",
    "k = k.squeeze() # [400, 32, 90]\n",
    "k = k.detach().numpy()\n",
    "plt = sns.heatmap(k[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# B32 before grad vanishing\n",
    "k = k.squeeze() # [400, 32, 90]\n",
    "k = k.detach().numpy()\n",
    "plt = sns.heatmap(k[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# B33 acc 0.8+\n",
    "# k = k.squeeze() # [400, 32, 90]\n",
    "# k = k.detach().numpy()\n",
    "plt = sns.heatmap(k[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # choose model\n",
    "#     # if max_val_acc <= val_accuracy:\n",
    "#     #     model_dir = logDir + model_name + str(epoch) + '.pt'\n",
    "#     #     print('Saving model at {} epoch to {}'.format(epoch, model_dir))\n",
    "#     #     max_val_acc = val_accuracy\n",
    "#     #     torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, model_dir)\n",
    "#     model_dir = logDir + model_name + str(epoch) + '.pt'\n",
    "#     print('Saving model at {} epoch to {}'.format(epoch, model_dir))\n",
    "#     max_val_acc = val_accuracy\n",
    "#     torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, model_dir)\n",
    "\n",
    "# training_end =  datetime.now()\n",
    "# training_time = training_end -training_start \n",
    "# print(\"training takes time {}\".format(training_time))\n",
    "\n",
    "# model.is_fitted = True\n",
    "model.eval()\n",
    "\n",
    "# TEST\n",
    "correct = 0\n",
    "test_num = 0\n",
    "for i, (XB,  y) in enumerate(test_loader):\n",
    "    if model.header == 'CNN':\n",
    "        x = XI\n",
    "    else:\n",
    "        x = XB\n",
    "    x, y = x.to(device), y.long().to(device)\n",
    "    \n",
    "    if x.size(0) != batch_size:\n",
    "        print(\" test batch {} size {} < {}, skip\".format(i, x.size()[0], batch_size))\n",
    "        break\n",
    "    test_num += x.size(0)\n",
    "    if block == \"phased_LSTM\":\n",
    "        x_decoded, latent, output = model(x, times)\n",
    "    else:\n",
    "        x_decoded, latent, output = model(x)\n",
    "\n",
    "    # compute classification acc\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    correct += pred.eq(y.data.view_as(pred)).long().cpu().sum().item()\n",
    "    \n",
    "test_acc = correct / test_num #len(test_loader.dataset)\n",
    "print('Test accuracy for', str(kfold_number), ' fold : ', test_acc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stats\n",
    "results_dict = {\"epoch_train_loss\": epoch_train_loss,\n",
    "                \"epoch_train_acc\": epoch_train_acc,\n",
    "                \"epoch_val_loss\": epoch_val_loss,\n",
    "                \"epoch_val_acc\": epoch_val_acc,\n",
    "                \"test_acc\": test_acc}\n",
    "\n",
    "dict_name = model_name + '_stats_fold{}_{}.pkl'.format(str(kfold_number), args.rep)\n",
    "pickle.dump(results_dict, open(logDir + dict_name, 'wb'))\n",
    "print(\"dump results dict to {}\".format(dict_name))\n",
    "\n",
    "assert n_epochs == len(epoch_train_acc), \"different epoch length {} {}\".format(n_epochs, len(epoch_train_acc))\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "ax.plot(np.arange(n_epochs), epoch_train_acc, label=\"train acc\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('acc')\n",
    "ax.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "figname = logDir + model_name +\"_train_acc.png\"\n",
    "if if_plot:\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AE]",
   "language": "python",
   "name": "conda-env-AE-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
